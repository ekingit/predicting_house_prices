{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "890b740b-5678-45d2-9b74-08b34be973a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main libs\n",
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e3499f-5199-4089-82d4-ee890a84b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6113a0f-611f-45a9-a94c-b4e2dbf5c889",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98cc57-c117-4296-ada7-b64ec5488ba1",
   "metadata": {},
   "source": [
    "__Input:__ Two datasets of sizes 1460 and 1459, respectively. Each data point in these datasets consists of 79 parameters: 39 numerical values and 40 non-numerical discrete parameters. The first dataset also includes SalePrice.\n",
    "\n",
    "__Output:__ Predictions of SalePrice for the second dataset.\n",
    "\n",
    "__Goal:__ Create a regression model to predict SalePrice for the second dataset with minimal error. The error is calculated using the formula:\n",
    "\n",
    "log_rmse = $(\\frac{\\sum_{i=1}^N(log(y_i)-log(\\hat{y}_i))^2}{N})^{1/2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b7713-6436-466e-80f2-21f6e6fd7e6b",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b686c-a89d-482a-842d-1af0de41adee",
   "metadata": {},
   "source": [
    "In this section, we will preprocess data. This includes cleaning, vectorizing, normalizing, and splitting it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63489867-daed-4973-a4bb-9b26d304b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81) (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('Kaggle_house_prices/train.csv') #Data to be splitted train-test. Includes SalePrice\n",
    "df_pred_raw= pd.read_csv('Kaggle_house_prices/test.csv') #Data for the results. SalePrice to be predicted.\n",
    "print(df_raw.shape, df_pred_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29bc3fe0-5b97-45d7-8015-ee97b6cecb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d944fa-9412-44fd-9a73-59d3bd38171e",
   "metadata": {},
   "source": [
    "Since SalePrice is the target value we want to predict, we save it separately and drop it along with Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d8ed00-68d3-4783-b9a6-944ecd633650",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw = df_raw['SalePrice'] #split the target values\n",
    "df_clean = df_raw.drop(columns=['Id','SalePrice']) #drop unnecessary columns\n",
    "pred_id = df_pred_raw['Id'] #save prediction Id for later use\n",
    "df_pred_clean = df_pred_raw.drop(columns=['Id']) #drop unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa87fb-074d-49c2-a716-38ac58b0ca6d",
   "metadata": {},
   "source": [
    "Next, we concatenate the training and test data to normalize them together. Normalization is a standard technique in both data science and theoretical physics, used to represent data with smaller numbers for convenience and computational efficiency. We subtract mu, a constant representing the mean, and divide by sigma, another constant representing the standard deviation. It is crucial to save mu and sigma, as they must remain constant. Whenever we apply our model to new data, we must use these same constants, not the mean and standard deviation of the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249bf503-cffc-48d6-b408-bed4ffdbc9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conc = pd.concat([df_clean,df_pred_clean]) #concatenate all data for preprocessing\n",
    "num_columns = df_conc.select_dtypes(include = ['float64', 'int64']).columns #take numerical columns\n",
    "mu_df = df_conc[num_columns].mean() #save mean for further use\n",
    "sigma_df = df_conc[num_columns].std() #save std dev for further use\n",
    "df_conc[num_columns] = df_conc[num_columns].apply(lambda x: (x-x.mean())/(x.std())) #normalize around the mean\n",
    "df_conc[num_columns] = df_conc[num_columns].fillna(0) #set NaN to 0 which is the mean of the normalized data\n",
    "mu_target = target_raw.mean() #save mean for further use\n",
    "sigma_target = target_raw.std() #save std dev for further use\n",
    "target = (target_raw - target_raw.mean())/target_raw.std() #normalize target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f25e7d4-1af3-41ca-b258-c832eb68b390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7726f-ab06-4065-b18d-ad2c8e16de4a",
   "metadata": {},
   "source": [
    "Next, we address non-numerical values by creating a new numerical dimension for each unique non-numerical value in a column. For example, the SaleCondition column has two non-numerical values: Normal and Abnorml. These are mapped to $\\mathbb{R}^2$, where Normal is represented by (1,0) and Abnorml by (0,1). This process, known as one-hot encoding, can become complex and cumbersome, especially when dealing with many non-numerical values. However, in our case, we only have a few unique values across 43 non-numerical columns, and the PyTorch library simplifies this task by automating the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af04f785-6aab-4329-a2c6-9db817566404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 330)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conc = pd.get_dummies(df_conc,dummy_na=True) ### vectorize non-numerical discrete values: 43 columns -> 294 columns\n",
    "df_conc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede74a7-13c3-4e19-8df9-80253edcb4d4",
   "metadata": {},
   "source": [
    "Preprocessing is almost done! We just need to un-concatenate the data and split it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a193d24b-c49c-4587-a3cf-18432165518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate df and pred\n",
    "df = df_conc[:1460]\n",
    "df_pred = df_conc[1460:]\n",
    "\n",
    "#split df train=1200, test=260\n",
    "var = np.random.choice(range(1459), 260, replace=False) #replace = no rep.\n",
    "df_train = df.drop(var)\n",
    "train_target = target_raw.drop(var)\n",
    "df_test = df.loc[var]\n",
    "test_target = target_raw.loc[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298c9e8a-c910-4ac3-a416-943f6c63941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform everything to appropriate tensors for the dataloader\n",
    "df_tens = torch.tensor(df.values.astype(np.float32))\n",
    "df_train_tens = torch.tensor(df_train.values.astype(np.float32))\n",
    "target_train_tens = torch.tensor(train_target.values.astype(np.float32)).reshape(-1,1)\n",
    "df_test_tens = torch.tensor(df_test.values.astype(np.float32))\n",
    "target_test_tens = torch.tensor(test_target.values.astype(np.float32)).reshape(-1,1)\n",
    "df_pred_tens = torch.tensor(df_pred.values.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe958d2-ae29-4dd4-b179-8f76a6505a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 330]) torch.Size([1200, 1])\n",
      "torch.Size([260, 330]) torch.Size([260, 1])\n",
      "torch.Size([1459, 330])\n"
     ]
    }
   ],
   "source": [
    "print(df_train_tens.shape, target_train_tens.shape)\n",
    "print(df_test_tens.shape, target_test_tens.shape)\n",
    "print(df_pred_tens.shape)\n",
    "#looks good\n",
    "#DATA IS READY FOR TRAINING!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d461862-b64b-4bd1-b453-c8b99fead1cf",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab86bb-1bdf-4b5e-90e6-ed56e5ea147c",
   "metadata": {},
   "source": [
    "We use a regression model and define model as a subclass of nn.Module of Pytorch library so that we can use Pytorch functionalities for training. The choices we have to make at this point are\n",
    "\n",
    "__depth:__ number of hidden layers (features) = 4\n",
    "\n",
    "__hidden layer size:__ number of nodes in each hidden layer: 500, 2000, 200, 32\n",
    "\n",
    "__activation functions:__ non-linear function to be applied at each layer: ReLU\n",
    "\n",
    "__loss function:__ log_rmse "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae49fc4-68dc-418d-b7e3-ecd10cb69d31",
   "metadata": {},
   "source": [
    "Here is a summary of layers we defined:\n",
    "\n",
    "$$\\mathbb{R}^{330\\times n}\\xrightarrow[ReLU\\circ (Linear)]{max(0,- .W_1+b_1)} \\mathbb{R}^{500\\times n} \\rightarrow \\mathbb{R}^{2000\\times n}\\rightarrow \\mathbb{R}^{200\\times n}\\rightarrow \\mathbb{R}^{32\\times n}\\rightarrow \\mathbb{R}^{1\\times n}$$\n",
    "\n",
    "$$X\\mapsto H_1 = max(0,W_1.X+b_1)\\mapsto H_2 = max(0,W_2.H_1+b_2) \\mapsto H_3\\mapsto H_4 \\mapsto \\hat{Y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09fef2a5-448d-4fe2-8815-5eae792ab9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class predicter(nn.Module): #(batch_size=n, 330) --> 1\n",
    "    def __init__(self, in_size=330, H1=500,H2=2000,H3=200,H4=32,out_size=1,d_o=0):\n",
    "        super(predicter, self).__init__()\n",
    "        self.lp1 = nn.Linear(in_size,H1)\n",
    "        self.lp2 = nn.Linear(H1,H2)\n",
    "        self.lp3 = nn.Linear(H2,H3)\n",
    "        self.lp4 = nn.Linear(H3,H4)\n",
    "        self.lp5 = nn.Linear(H4,out_size)\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(d_o)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.drop(self.act(self.lp1(X)))\n",
    "        X = self.drop(self.act(self.lp2(X)))\n",
    "        X = self.drop(self.act(self.lp3(X)))\n",
    "        X = self.drop(self.act(self.lp4(X)))\n",
    "        X = self.drop(self.act(self.lp5(X)))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50a40f71-caf1-4b2e-8279-278cbd05cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss(reduction='sum')\n",
    "mse_loss_mean = nn.MSELoss(reduction='mean')\n",
    "def rmse(y,y_hat):\n",
    "    log_rmse = torch.sqrt(mse_loss_mean(torch.log(y),torch.log(y_hat)))\n",
    "    return log_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda8d20-8f25-4ef7-82a5-b57b78de86ea",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d8ef9-7e9f-4a31-a2e6-37e79edf3085",
   "metadata": {},
   "source": [
    "Next, we have to define the training loop. The model we defined assigns random $W$ and $b$ parameters to find a number by the map $\\mathbb{R}^{330\\times n}\\rightarrow\\mathbb{R}^{1\\times n}$. Then, we calculate loss, take partial derivative $g_i = \\nabla_{i} (loss)$ for each layer and update parameters $(W_i,b_i)\\rightarrow (W,b)-\\eta. g_i$ where $\\eta$ is the learning rate. Pytorch does these automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3cec5d8-d987-4fc8-a85b-597870e27e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, target, optimizer, epoch, device='cpu'):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    data = data\n",
    "    target = target\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model(data)\n",
    "    train_loss = rmse(output, target) #mean squared error: #TODO: PROBLEM HERE!!!\n",
    "    train_loss.backward() #calculates gradients\n",
    "    optimizer.step() #updates weights\n",
    "    trainloss.append(train_loss.detach())\n",
    "    print('\\nEpoch: {} Train Loss: {:.4f}\\n'.format(epoch,\n",
    "    train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1556b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data, target, device='cpu'):\n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    data = data\n",
    "    target = target\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model(data)\n",
    "    test_loss = rmse(output, target)\n",
    "    testloss.append(test_loss.detach())\n",
    "    print(f\" Test Loss {test_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe9454-da8c-47a0-9695-4dade17d4612",
   "metadata": {},
   "source": [
    "__Hyperparameters:__\n",
    "\n",
    "optimizer: updates parameters. = SGD.\n",
    "\n",
    "learnin rate = 0.1\n",
    "\n",
    "weight decay = None\n",
    "\n",
    "drop out = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae0c249-9472-43c1-8f82-fadd61f277aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Train Loss: 14.1226\n",
      "\n",
      " Test Loss 11.909591674804688\n",
      "\n",
      "Epoch: 2 Train Loss: 11.8673\n",
      "\n",
      " Test Loss 11.763948440551758\n",
      "\n",
      "Epoch: 3 Train Loss: 11.7218\n",
      "\n",
      " Test Loss 11.623167991638184\n",
      "\n",
      "Epoch: 4 Train Loss: 11.5812\n",
      "\n",
      " Test Loss 11.478662490844727\n",
      "\n",
      "Epoch: 5 Train Loss: 11.4369\n",
      "\n",
      " Test Loss 11.322469711303711\n",
      "\n",
      "Epoch: 6 Train Loss: 11.2809\n",
      "\n",
      " Test Loss 11.147958755493164\n",
      "\n",
      "Epoch: 7 Train Loss: 11.1068\n",
      "\n",
      " Test Loss 10.951188087463379\n",
      "\n",
      "Epoch: 8 Train Loss: 10.9104\n",
      "\n",
      " Test Loss 10.731232643127441\n",
      "\n",
      "Epoch: 9 Train Loss: 10.6908\n",
      "\n",
      " Test Loss 10.489948272705078\n",
      "\n",
      "Epoch: 10 Train Loss: 10.4498\n",
      "\n",
      " Test Loss 10.232319831848145\n",
      "\n",
      "Epoch: 11 Train Loss: 10.1925\n",
      "\n",
      " Test Loss 9.96566104888916\n",
      "\n",
      "Epoch: 12 Train Loss: 9.9260\n",
      "\n",
      " Test Loss 9.697720527648926\n",
      "\n",
      "Epoch: 13 Train Loss: 9.6582\n",
      "\n",
      " Test Loss 9.434917449951172\n",
      "\n",
      "Epoch: 14 Train Loss: 9.3955\n",
      "\n",
      " Test Loss 9.181634902954102\n",
      "\n",
      "Epoch: 15 Train Loss: 9.1423\n",
      "\n",
      " Test Loss 8.940415382385254\n",
      "\n",
      "Epoch: 16 Train Loss: 8.9010\n",
      "\n",
      " Test Loss 8.712271690368652\n",
      "\n",
      "Epoch: 17 Train Loss: 8.6729\n",
      "\n",
      " Test Loss 8.497347831726074\n",
      "\n",
      "Epoch: 18 Train Loss: 8.4579\n",
      "\n",
      " Test Loss 8.295202255249023\n",
      "\n",
      "Epoch: 19 Train Loss: 8.2558\n",
      "\n",
      " Test Loss 8.105072975158691\n",
      "\n",
      "Epoch: 20 Train Loss: 8.0656\n",
      "\n",
      " Test Loss 7.92608642578125\n",
      "\n",
      "Epoch: 21 Train Loss: 7.8866\n",
      "\n",
      " Test Loss 7.7573161125183105\n",
      "\n",
      "Epoch: 22 Train Loss: 7.7178\n",
      "\n",
      " Test Loss 7.597896575927734\n",
      "\n",
      "Epoch: 23 Train Loss: 7.5584\n",
      "\n",
      " Test Loss 7.447012424468994\n",
      "\n",
      "Epoch: 24 Train Loss: 7.4075\n",
      "\n",
      " Test Loss 7.303924560546875\n",
      "\n",
      "Epoch: 25 Train Loss: 7.2644\n",
      "\n",
      " Test Loss 7.167953968048096\n",
      "\n",
      "Epoch: 26 Train Loss: 7.1284\n",
      "\n",
      " Test Loss 7.0384955406188965\n",
      "\n",
      "Epoch: 27 Train Loss: 6.9989\n",
      "\n",
      " Test Loss 6.91500997543335\n",
      "\n",
      "Epoch: 28 Train Loss: 6.8754\n",
      "\n",
      " Test Loss 6.797013759613037\n",
      "\n",
      "Epoch: 29 Train Loss: 6.7574\n",
      "\n",
      " Test Loss 6.684075355529785\n",
      "\n",
      "Epoch: 30 Train Loss: 6.6445\n",
      "\n",
      " Test Loss 6.5757975578308105\n",
      "\n",
      "Epoch: 31 Train Loss: 6.5362\n",
      "\n",
      " Test Loss 6.471837043762207\n",
      "\n",
      "Epoch: 32 Train Loss: 6.4322\n",
      "\n",
      " Test Loss 6.371881484985352\n",
      "\n",
      "Epoch: 33 Train Loss: 6.3323\n",
      "\n",
      " Test Loss 6.275644302368164\n",
      "\n",
      "Epoch: 34 Train Loss: 6.2360\n",
      "\n",
      " Test Loss 6.182870864868164\n",
      "\n",
      "Epoch: 35 Train Loss: 6.1432\n",
      "\n",
      " Test Loss 6.09332799911499\n",
      "\n",
      "Epoch: 36 Train Loss: 6.0537\n",
      "\n",
      " Test Loss 6.006807327270508\n",
      "\n",
      "Epoch: 37 Train Loss: 5.9672\n",
      "\n",
      " Test Loss 5.92311954498291\n",
      "\n",
      "Epoch: 38 Train Loss: 5.8835\n",
      "\n",
      " Test Loss 5.842089653015137\n",
      "\n",
      "Epoch: 39 Train Loss: 5.8024\n",
      "\n",
      " Test Loss 5.763558864593506\n",
      "\n",
      "Epoch: 40 Train Loss: 5.7239\n",
      "\n",
      " Test Loss 5.687384128570557\n",
      "\n",
      "Epoch: 41 Train Loss: 5.6477\n",
      "\n",
      " Test Loss 5.613430023193359\n",
      "\n",
      "Epoch: 42 Train Loss: 5.5737\n",
      "\n",
      " Test Loss 5.541574478149414\n",
      "\n",
      "Epoch: 43 Train Loss: 5.5019\n",
      "\n",
      " Test Loss 5.471704483032227\n",
      "\n",
      "Epoch: 44 Train Loss: 5.4320\n",
      "\n",
      " Test Loss 5.403714656829834\n",
      "\n",
      "Epoch: 45 Train Loss: 5.3640\n",
      "\n",
      " Test Loss 5.337509632110596\n",
      "\n",
      "Epoch: 46 Train Loss: 5.2978\n",
      "\n",
      " Test Loss 5.272998809814453\n",
      "\n",
      "Epoch: 47 Train Loss: 5.2333\n",
      "\n",
      " Test Loss 5.210099697113037\n",
      "\n",
      "Epoch: 48 Train Loss: 5.1704\n",
      "\n",
      " Test Loss 5.148736476898193\n",
      "\n",
      "Epoch: 49 Train Loss: 5.1090\n",
      "\n",
      " Test Loss 5.088835716247559\n",
      "\n",
      "Epoch: 50 Train Loss: 5.0491\n",
      "\n",
      " Test Loss 5.030331611633301\n",
      "\n",
      "Epoch: 51 Train Loss: 4.9906\n",
      "\n",
      " Test Loss 4.9731621742248535\n",
      "\n",
      "Epoch: 52 Train Loss: 4.9334\n",
      "\n",
      " Test Loss 4.917267799377441\n",
      "\n",
      "Epoch: 53 Train Loss: 4.8775\n",
      "\n",
      " Test Loss 4.862595081329346\n",
      "\n",
      "Epoch: 54 Train Loss: 4.8228\n",
      "\n",
      " Test Loss 4.8090901374816895\n",
      "\n",
      "Epoch: 55 Train Loss: 4.7693\n",
      "\n",
      " Test Loss 4.756707191467285\n",
      "\n",
      "Epoch: 56 Train Loss: 4.7169\n",
      "\n",
      " Test Loss 4.705400466918945\n",
      "\n",
      "Epoch: 57 Train Loss: 4.6656\n",
      "\n",
      " Test Loss 4.655128479003906\n",
      "\n",
      "Epoch: 58 Train Loss: 4.6153\n",
      "\n",
      " Test Loss 4.605848789215088\n",
      "\n",
      "Epoch: 59 Train Loss: 4.5660\n",
      "\n",
      " Test Loss 4.557525634765625\n",
      "\n",
      "Epoch: 60 Train Loss: 4.5177\n",
      "\n",
      " Test Loss 4.5101213455200195\n",
      "\n",
      "Epoch: 61 Train Loss: 4.4702\n",
      "\n",
      " Test Loss 4.463603496551514\n",
      "\n",
      "Epoch: 62 Train Loss: 4.4237\n",
      "\n",
      " Test Loss 4.417938709259033\n",
      "\n",
      "Epoch: 63 Train Loss: 4.3780\n",
      "\n",
      " Test Loss 4.3730974197387695\n",
      "\n",
      "Epoch: 64 Train Loss: 4.3332\n",
      "\n",
      " Test Loss 4.3290510177612305\n",
      "\n",
      "Epoch: 65 Train Loss: 4.2891\n",
      "\n",
      " Test Loss 4.28577184677124\n",
      "\n",
      "Epoch: 66 Train Loss: 4.2458\n",
      "\n",
      " Test Loss 4.243234634399414\n",
      "\n",
      "Epoch: 67 Train Loss: 4.2033\n",
      "\n",
      " Test Loss 4.201414108276367\n",
      "\n",
      "Epoch: 68 Train Loss: 4.1615\n",
      "\n",
      " Test Loss 4.160287380218506\n",
      "\n",
      "Epoch: 69 Train Loss: 4.1203\n",
      "\n",
      " Test Loss 4.119832515716553\n",
      "\n",
      "Epoch: 70 Train Loss: 4.0798\n",
      "\n",
      " Test Loss 4.080027103424072\n",
      "\n",
      "Epoch: 71 Train Loss: 4.0400\n",
      "\n",
      " Test Loss 4.040852069854736\n",
      "\n",
      "Epoch: 72 Train Loss: 4.0008\n",
      "\n",
      " Test Loss 4.002287864685059\n",
      "\n",
      "Epoch: 73 Train Loss: 3.9623\n",
      "\n",
      " Test Loss 3.964315891265869\n",
      "\n",
      "Epoch: 74 Train Loss: 3.9243\n",
      "\n",
      " Test Loss 3.9269185066223145\n",
      "\n",
      "Epoch: 75 Train Loss: 3.8869\n",
      "\n",
      " Test Loss 3.8900790214538574\n",
      "\n",
      "Epoch: 76 Train Loss: 3.8500\n",
      "\n",
      " Test Loss 3.85378098487854\n",
      "\n",
      "Epoch: 77 Train Loss: 3.8137\n",
      "\n",
      " Test Loss 3.818009614944458\n",
      "\n",
      "Epoch: 78 Train Loss: 3.7779\n",
      "\n",
      " Test Loss 3.782749652862549\n",
      "\n",
      "Epoch: 79 Train Loss: 3.7427\n",
      "\n",
      " Test Loss 3.7479872703552246\n",
      "\n",
      "Epoch: 80 Train Loss: 3.7079\n",
      "\n",
      " Test Loss 3.7137084007263184\n",
      "\n",
      "Epoch: 81 Train Loss: 3.6736\n",
      "\n",
      " Test Loss 3.6799004077911377\n",
      "\n",
      "Epoch: 82 Train Loss: 3.6398\n",
      "\n",
      " Test Loss 3.6465506553649902\n",
      "\n",
      "Epoch: 83 Train Loss: 3.6064\n",
      "\n",
      " Test Loss 3.613646984100342\n",
      "\n",
      "Epoch: 84 Train Loss: 3.5735\n",
      "\n",
      " Test Loss 3.5811781883239746\n",
      "\n",
      "Epoch: 85 Train Loss: 3.5410\n",
      "\n",
      " Test Loss 3.549133062362671\n",
      "\n",
      "Epoch: 86 Train Loss: 3.5090\n",
      "\n",
      " Test Loss 3.5175013542175293\n",
      "\n",
      "Epoch: 87 Train Loss: 3.4773\n",
      "\n",
      " Test Loss 3.4862723350524902\n",
      "\n",
      "Epoch: 88 Train Loss: 3.4461\n",
      "\n",
      " Test Loss 3.4554359912872314\n",
      "\n",
      "Epoch: 89 Train Loss: 3.4152\n",
      "\n",
      " Test Loss 3.424983501434326\n",
      "\n",
      "Epoch: 90 Train Loss: 3.3848\n",
      "\n",
      " Test Loss 3.3949050903320312\n",
      "\n",
      "Epoch: 91 Train Loss: 3.3547\n",
      "\n",
      " Test Loss 3.3651926517486572\n",
      "\n",
      "Epoch: 92 Train Loss: 3.3249\n",
      "\n",
      " Test Loss 3.33583664894104\n",
      "\n",
      "Epoch: 93 Train Loss: 3.2956\n",
      "\n",
      " Test Loss 3.3068294525146484\n",
      "\n",
      "Epoch: 94 Train Loss: 3.2666\n",
      "\n",
      " Test Loss 3.278162717819214\n",
      "\n",
      "Epoch: 95 Train Loss: 3.2379\n",
      "\n",
      " Test Loss 3.249829053878784\n",
      "\n",
      "Epoch: 96 Train Loss: 3.2095\n",
      "\n",
      " Test Loss 3.2218210697174072\n",
      "\n",
      "Epoch: 97 Train Loss: 3.1815\n",
      "\n",
      " Test Loss 3.194131374359131\n",
      "\n",
      "Epoch: 98 Train Loss: 3.1538\n",
      "\n",
      " Test Loss 3.1667535305023193\n",
      "\n",
      "Epoch: 99 Train Loss: 3.1264\n",
      "\n",
      " Test Loss 3.1396803855895996\n",
      "\n",
      "Epoch: 100 Train Loss: 3.0993\n",
      "\n",
      " Test Loss 3.112905263900757\n",
      "\n",
      "Epoch: 101 Train Loss: 3.0725\n",
      "\n",
      " Test Loss 3.0864224433898926\n",
      "\n",
      "Epoch: 102 Train Loss: 3.0460\n",
      "\n",
      " Test Loss 3.060225248336792\n",
      "\n",
      "Epoch: 103 Train Loss: 3.0198\n",
      "\n",
      " Test Loss 3.034308433532715\n",
      "\n",
      "Epoch: 104 Train Loss: 2.9939\n",
      "\n",
      " Test Loss 3.0086660385131836\n",
      "\n",
      "Epoch: 105 Train Loss: 2.9682\n",
      "\n",
      " Test Loss 2.9832923412323\n",
      "\n",
      "Epoch: 106 Train Loss: 2.9429\n",
      "\n",
      " Test Loss 2.958181858062744\n",
      "\n",
      "Epoch: 107 Train Loss: 2.9177\n",
      "\n",
      " Test Loss 2.9333298206329346\n",
      "\n",
      "Epoch: 108 Train Loss: 2.8929\n",
      "\n",
      " Test Loss 2.9087307453155518\n",
      "\n",
      "Epoch: 109 Train Loss: 2.8683\n",
      "\n",
      " Test Loss 2.884380340576172\n",
      "\n",
      "Epoch: 110 Train Loss: 2.8439\n",
      "\n",
      " Test Loss 2.8602733612060547\n",
      "\n",
      "Epoch: 111 Train Loss: 2.8198\n",
      "\n",
      " Test Loss 2.8364055156707764\n",
      "\n",
      "Epoch: 112 Train Loss: 2.7959\n",
      "\n",
      " Test Loss 2.812772512435913\n",
      "\n",
      "Epoch: 113 Train Loss: 2.7722\n",
      "\n",
      " Test Loss 2.7893693447113037\n",
      "\n",
      "Epoch: 114 Train Loss: 2.7488\n",
      "\n",
      " Test Loss 2.7661921977996826\n",
      "\n",
      "Epoch: 115 Train Loss: 2.7256\n",
      "\n",
      " Test Loss 2.743236780166626\n",
      "\n",
      "Epoch: 116 Train Loss: 2.7027\n",
      "\n",
      " Test Loss 2.7204997539520264\n",
      "\n",
      "Epoch: 117 Train Loss: 2.6799\n",
      "\n",
      " Test Loss 2.697976589202881\n",
      "\n",
      "Epoch: 118 Train Loss: 2.6574\n",
      "\n",
      " Test Loss 2.675663471221924\n",
      "\n",
      "Epoch: 119 Train Loss: 2.6351\n",
      "\n",
      " Test Loss 2.653557062149048\n",
      "\n",
      "Epoch: 120 Train Loss: 2.6129\n",
      "\n",
      " Test Loss 2.6316537857055664\n",
      "\n",
      "Epoch: 121 Train Loss: 2.5910\n",
      "\n",
      " Test Loss 2.609949827194214\n",
      "\n",
      "Epoch: 122 Train Loss: 2.5693\n",
      "\n",
      " Test Loss 2.588442325592041\n",
      "\n",
      "Epoch: 123 Train Loss: 2.5478\n",
      "\n",
      " Test Loss 2.5671277046203613\n",
      "\n",
      "Epoch: 124 Train Loss: 2.5264\n",
      "\n",
      " Test Loss 2.5460023880004883\n",
      "\n",
      "Epoch: 125 Train Loss: 2.5053\n",
      "\n",
      " Test Loss 2.5250637531280518\n",
      "\n",
      "Epoch: 126 Train Loss: 2.4843\n",
      "\n",
      " Test Loss 2.5043089389801025\n",
      "\n",
      "Epoch: 127 Train Loss: 2.4636\n",
      "\n",
      " Test Loss 2.483734369277954\n",
      "\n",
      "Epoch: 128 Train Loss: 2.4430\n",
      "\n",
      " Test Loss 2.4633376598358154\n",
      "\n",
      "Epoch: 129 Train Loss: 2.4226\n",
      "\n",
      " Test Loss 2.443115711212158\n",
      "\n",
      "Epoch: 130 Train Loss: 2.4023\n",
      "\n",
      " Test Loss 2.4230659008026123\n",
      "\n",
      "Epoch: 131 Train Loss: 2.3823\n",
      "\n",
      " Test Loss 2.4031856060028076\n",
      "\n",
      "Epoch: 132 Train Loss: 2.3624\n",
      "\n",
      " Test Loss 2.383472204208374\n",
      "\n",
      "Epoch: 133 Train Loss: 2.3427\n",
      "\n",
      " Test Loss 2.3639233112335205\n",
      "\n",
      "Epoch: 134 Train Loss: 2.3231\n",
      "\n",
      " Test Loss 2.344536304473877\n",
      "\n",
      "Epoch: 135 Train Loss: 2.3037\n",
      "\n",
      " Test Loss 2.3253085613250732\n",
      "\n",
      "Epoch: 136 Train Loss: 2.2844\n",
      "\n",
      " Test Loss 2.3062376976013184\n",
      "\n",
      "Epoch: 137 Train Loss: 2.2654\n",
      "\n",
      " Test Loss 2.2873218059539795\n",
      "\n",
      "Epoch: 138 Train Loss: 2.2464\n",
      "\n",
      " Test Loss 2.2685587406158447\n",
      "\n",
      "Epoch: 139 Train Loss: 2.2276\n",
      "\n",
      " Test Loss 2.249945640563965\n",
      "\n",
      "Epoch: 140 Train Loss: 2.2090\n",
      "\n",
      " Test Loss 2.2314810752868652\n",
      "\n",
      "Epoch: 141 Train Loss: 2.1905\n",
      "\n",
      " Test Loss 2.2131621837615967\n",
      "\n",
      "Epoch: 142 Train Loss: 2.1722\n",
      "\n",
      " Test Loss 2.1949877738952637\n",
      "\n",
      "Epoch: 143 Train Loss: 2.1540\n",
      "\n",
      " Test Loss 2.176955461502075\n",
      "\n",
      "Epoch: 144 Train Loss: 2.1360\n",
      "\n",
      " Test Loss 2.1590633392333984\n",
      "\n",
      "Epoch: 145 Train Loss: 2.1181\n",
      "\n",
      " Test Loss 2.1413092613220215\n",
      "\n",
      "Epoch: 146 Train Loss: 2.1003\n",
      "\n",
      " Test Loss 2.1236915588378906\n",
      "\n",
      "Epoch: 147 Train Loss: 2.0826\n",
      "\n",
      " Test Loss 2.106208562850952\n",
      "\n",
      "Epoch: 148 Train Loss: 2.0652\n",
      "\n",
      " Test Loss 2.0888583660125732\n",
      "\n",
      "Epoch: 149 Train Loss: 2.0478\n",
      "\n",
      " Test Loss 2.071639060974121\n",
      "\n",
      "Epoch: 150 Train Loss: 2.0305\n",
      "\n",
      " Test Loss 2.054549217224121\n",
      "\n",
      "Epoch: 151 Train Loss: 2.0134\n",
      "\n",
      " Test Loss 2.0375871658325195\n",
      "\n",
      "Epoch: 152 Train Loss: 1.9965\n",
      "\n",
      " Test Loss 2.0207509994506836\n",
      "\n",
      "Epoch: 153 Train Loss: 1.9796\n",
      "\n",
      " Test Loss 2.004039764404297\n",
      "\n",
      "Epoch: 154 Train Loss: 1.9629\n",
      "\n",
      " Test Loss 1.9874513149261475\n",
      "\n",
      "Epoch: 155 Train Loss: 1.9463\n",
      "\n",
      " Test Loss 1.9709844589233398\n",
      "\n",
      "Epoch: 156 Train Loss: 1.9298\n",
      "\n",
      " Test Loss 1.9546376466751099\n",
      "\n",
      "Epoch: 157 Train Loss: 1.9134\n",
      "\n",
      " Test Loss 1.9384093284606934\n",
      "\n",
      "Epoch: 158 Train Loss: 1.8972\n",
      "\n",
      " Test Loss 1.9222981929779053\n",
      "\n",
      "Epoch: 159 Train Loss: 1.8811\n",
      "\n",
      " Test Loss 1.906302809715271\n",
      "\n",
      "Epoch: 160 Train Loss: 1.8650\n",
      "\n",
      " Test Loss 1.890421986579895\n",
      "\n",
      "Epoch: 161 Train Loss: 1.8491\n",
      "\n",
      " Test Loss 1.8746542930603027\n",
      "\n",
      "Epoch: 162 Train Loss: 1.8334\n",
      "\n",
      " Test Loss 1.85899817943573\n",
      "\n",
      "Epoch: 163 Train Loss: 1.8177\n",
      "\n",
      " Test Loss 1.84345281124115\n",
      "\n",
      "Epoch: 164 Train Loss: 1.8021\n",
      "\n",
      " Test Loss 1.828016757965088\n",
      "\n",
      "Epoch: 165 Train Loss: 1.7867\n",
      "\n",
      " Test Loss 1.8126888275146484\n",
      "\n",
      "Epoch: 166 Train Loss: 1.7713\n",
      "\n",
      " Test Loss 1.7974677085876465\n",
      "\n",
      "Epoch: 167 Train Loss: 1.7561\n",
      "\n",
      " Test Loss 1.7823525667190552\n",
      "\n",
      "Epoch: 168 Train Loss: 1.7409\n",
      "\n",
      " Test Loss 1.7673418521881104\n",
      "\n",
      "Epoch: 169 Train Loss: 1.7259\n",
      "\n",
      " Test Loss 1.7524348497390747\n",
      "\n",
      "Epoch: 170 Train Loss: 1.7110\n",
      "\n",
      " Test Loss 1.7376304864883423\n",
      "\n",
      "Epoch: 171 Train Loss: 1.6962\n",
      "\n",
      " Test Loss 1.7229273319244385\n",
      "\n",
      "Epoch: 172 Train Loss: 1.6814\n",
      "\n",
      " Test Loss 1.7083247900009155\n",
      "\n",
      "Epoch: 173 Train Loss: 1.6668\n",
      "\n",
      " Test Loss 1.693821668624878\n",
      "\n",
      "Epoch: 174 Train Loss: 1.6523\n",
      "\n",
      " Test Loss 1.6794168949127197\n",
      "\n",
      "Epoch: 175 Train Loss: 1.6379\n",
      "\n",
      " Test Loss 1.6651097536087036\n",
      "\n",
      "Epoch: 176 Train Loss: 1.6235\n",
      "\n",
      " Test Loss 1.6508994102478027\n",
      "\n",
      "Epoch: 177 Train Loss: 1.6093\n",
      "\n",
      " Test Loss 1.636784315109253\n",
      "\n",
      "Epoch: 178 Train Loss: 1.5952\n",
      "\n",
      " Test Loss 1.622764229774475\n",
      "\n",
      "Epoch: 179 Train Loss: 1.5811\n",
      "\n",
      " Test Loss 1.6088378429412842\n",
      "\n",
      "Epoch: 180 Train Loss: 1.5672\n",
      "\n",
      " Test Loss 1.5950045585632324\n",
      "\n",
      "Epoch: 181 Train Loss: 1.5533\n",
      "\n",
      " Test Loss 1.581263542175293\n",
      "\n",
      "Epoch: 182 Train Loss: 1.5396\n",
      "\n",
      " Test Loss 1.5676138401031494\n",
      "\n",
      "Epoch: 183 Train Loss: 1.5259\n",
      "\n",
      " Test Loss 1.554054856300354\n",
      "\n",
      "Epoch: 184 Train Loss: 1.5123\n",
      "\n",
      " Test Loss 1.5405855178833008\n",
      "\n",
      "Epoch: 185 Train Loss: 1.4988\n",
      "\n",
      " Test Loss 1.527205228805542\n",
      "\n",
      "Epoch: 186 Train Loss: 1.4854\n",
      "\n",
      " Test Loss 1.5139135122299194\n",
      "\n",
      "Epoch: 187 Train Loss: 1.4721\n",
      "\n",
      " Test Loss 1.5007094144821167\n",
      "\n",
      "Epoch: 188 Train Loss: 1.4589\n",
      "\n",
      " Test Loss 1.4875919818878174\n",
      "\n",
      "Epoch: 189 Train Loss: 1.4458\n",
      "\n",
      " Test Loss 1.4745609760284424\n",
      "\n",
      "Epoch: 190 Train Loss: 1.4327\n",
      "\n",
      " Test Loss 1.4616155624389648\n",
      "\n",
      "Epoch: 191 Train Loss: 1.4198\n",
      "\n",
      " Test Loss 1.4487550258636475\n",
      "\n",
      "Epoch: 192 Train Loss: 1.4069\n",
      "\n",
      " Test Loss 1.4359787702560425\n",
      "\n",
      "Epoch: 193 Train Loss: 1.3941\n",
      "\n",
      " Test Loss 1.4232863187789917\n",
      "\n",
      "Epoch: 194 Train Loss: 1.3814\n",
      "\n",
      " Test Loss 1.4106769561767578\n",
      "\n",
      "Epoch: 195 Train Loss: 1.3687\n",
      "\n",
      " Test Loss 1.3981499671936035\n",
      "\n",
      "Epoch: 196 Train Loss: 1.3562\n",
      "\n",
      " Test Loss 1.3857049942016602\n",
      "\n",
      "Epoch: 197 Train Loss: 1.3437\n",
      "\n",
      " Test Loss 1.3733415603637695\n",
      "\n",
      "Epoch: 198 Train Loss: 1.3313\n",
      "\n",
      " Test Loss 1.3610588312149048\n",
      "\n",
      "Epoch: 199 Train Loss: 1.3190\n",
      "\n",
      " Test Loss 1.3488565683364868\n",
      "\n",
      "Epoch: 200 Train Loss: 1.3068\n",
      "\n",
      " Test Loss 1.3367342948913574\n",
      "\n",
      "Epoch: 201 Train Loss: 1.2947\n",
      "\n",
      " Test Loss 1.3246912956237793\n",
      "\n",
      "Epoch: 202 Train Loss: 1.2826\n",
      "\n",
      " Test Loss 1.3127273321151733\n",
      "\n",
      "Epoch: 203 Train Loss: 1.2706\n",
      "\n",
      " Test Loss 1.3008418083190918\n",
      "\n",
      "Epoch: 204 Train Loss: 1.2587\n",
      "\n",
      " Test Loss 1.289034366607666\n",
      "\n",
      "Epoch: 205 Train Loss: 1.2469\n",
      "\n",
      " Test Loss 1.2773045301437378\n",
      "\n",
      "Epoch: 206 Train Loss: 1.2351\n",
      "\n",
      " Test Loss 1.2656519412994385\n",
      "\n",
      "Epoch: 207 Train Loss: 1.2235\n",
      "\n",
      " Test Loss 1.2540761232376099\n",
      "\n",
      "Epoch: 208 Train Loss: 1.2119\n",
      "\n",
      " Test Loss 1.2425768375396729\n",
      "\n",
      "Epoch: 209 Train Loss: 1.2003\n",
      "\n",
      " Test Loss 1.2311538457870483\n",
      "\n",
      "Epoch: 210 Train Loss: 1.1889\n",
      "\n",
      " Test Loss 1.2198063135147095\n",
      "\n",
      "Epoch: 211 Train Loss: 1.1775\n",
      "\n",
      " Test Loss 1.2085344791412354\n",
      "\n",
      "Epoch: 212 Train Loss: 1.1662\n",
      "\n",
      " Test Loss 1.1973376274108887\n",
      "\n",
      "Epoch: 213 Train Loss: 1.1550\n",
      "\n",
      " Test Loss 1.1862157583236694\n",
      "\n",
      "Epoch: 214 Train Loss: 1.1439\n",
      "\n",
      " Test Loss 1.175168514251709\n",
      "\n",
      "Epoch: 215 Train Loss: 1.1328\n",
      "\n",
      " Test Loss 1.1641954183578491\n",
      "\n",
      "Epoch: 216 Train Loss: 1.1218\n",
      "\n",
      " Test Loss 1.1532964706420898\n",
      "\n",
      "Epoch: 217 Train Loss: 1.1109\n",
      "\n",
      " Test Loss 1.142471432685852\n",
      "\n",
      "Epoch: 218 Train Loss: 1.1000\n",
      "\n",
      " Test Loss 1.1317198276519775\n",
      "\n",
      "Epoch: 219 Train Loss: 1.0893\n",
      "\n",
      " Test Loss 1.1210416555404663\n",
      "\n",
      "Epoch: 220 Train Loss: 1.0786\n",
      "\n",
      " Test Loss 1.1104367971420288\n",
      "\n",
      "Epoch: 221 Train Loss: 1.0680\n",
      "\n",
      " Test Loss 1.0999048948287964\n",
      "\n",
      "Epoch: 222 Train Loss: 1.0574\n",
      "\n",
      " Test Loss 1.0894458293914795\n",
      "\n",
      "Epoch: 223 Train Loss: 1.0469\n",
      "\n",
      " Test Loss 1.079059362411499\n",
      "\n",
      "Epoch: 224 Train Loss: 1.0365\n",
      "\n",
      " Test Loss 1.068745493888855\n",
      "\n",
      "Epoch: 225 Train Loss: 1.0262\n",
      "\n",
      " Test Loss 1.0585042238235474\n",
      "\n",
      "Epoch: 226 Train Loss: 1.0159\n",
      "\n",
      " Test Loss 1.048335313796997\n",
      "\n",
      "Epoch: 227 Train Loss: 1.0057\n",
      "\n",
      " Test Loss 1.0382386445999146\n",
      "\n",
      "Epoch: 228 Train Loss: 0.9956\n",
      "\n",
      " Test Loss 1.0282140970230103\n",
      "\n",
      "Epoch: 229 Train Loss: 0.9856\n",
      "\n",
      " Test Loss 1.0182616710662842\n",
      "\n",
      "Epoch: 230 Train Loss: 0.9756\n",
      "\n",
      " Test Loss 1.0083813667297363\n",
      "\n",
      "Epoch: 231 Train Loss: 0.9657\n",
      "\n",
      " Test Loss 0.9985732436180115\n",
      "\n",
      "Epoch: 232 Train Loss: 0.9559\n",
      "\n",
      " Test Loss 0.9888370633125305\n",
      "\n",
      "Epoch: 233 Train Loss: 0.9461\n",
      "\n",
      " Test Loss 0.9791728854179382\n",
      "\n",
      "Epoch: 234 Train Loss: 0.9364\n",
      "\n",
      " Test Loss 0.9695808291435242\n",
      "\n",
      "Epoch: 235 Train Loss: 0.9268\n",
      "\n",
      " Test Loss 0.9600608348846436\n",
      "\n",
      "Epoch: 236 Train Loss: 0.9173\n",
      "\n",
      " Test Loss 0.9506128430366516\n",
      "\n",
      "Epoch: 237 Train Loss: 0.9078\n",
      "\n",
      " Test Loss 0.9412370920181274\n",
      "\n",
      "Epoch: 238 Train Loss: 0.8984\n",
      "\n",
      " Test Loss 0.9319336414337158\n",
      "\n",
      "Epoch: 239 Train Loss: 0.8891\n",
      "\n",
      " Test Loss 0.922702431678772\n",
      "\n",
      "Epoch: 240 Train Loss: 0.8799\n",
      "\n",
      " Test Loss 0.9135438203811646\n",
      "\n",
      "Epoch: 241 Train Loss: 0.8707\n",
      "\n",
      " Test Loss 0.904457688331604\n",
      "\n",
      "Epoch: 242 Train Loss: 0.8616\n",
      "\n",
      " Test Loss 0.8954442739486694\n",
      "\n",
      "Epoch: 243 Train Loss: 0.8526\n",
      "\n",
      " Test Loss 0.8865036368370056\n",
      "\n",
      "Epoch: 244 Train Loss: 0.8436\n",
      "\n",
      " Test Loss 0.8776360154151917\n",
      "\n",
      "Epoch: 245 Train Loss: 0.8348\n",
      "\n",
      " Test Loss 0.8688415288925171\n",
      "\n",
      "Epoch: 246 Train Loss: 0.8260\n",
      "\n",
      " Test Loss 0.8601205348968506\n",
      "\n",
      "Epoch: 247 Train Loss: 0.8172\n",
      "\n",
      " Test Loss 0.8514730334281921\n",
      "\n",
      "Epoch: 248 Train Loss: 0.8086\n",
      "\n",
      " Test Loss 0.8428992629051208\n",
      "\n",
      "Epoch: 249 Train Loss: 0.8000\n",
      "\n",
      " Test Loss 0.8343995213508606\n",
      "\n",
      "Epoch: 250 Train Loss: 0.7915\n",
      "\n",
      " Test Loss 0.8259741067886353\n",
      "\n",
      "Epoch: 251 Train Loss: 0.7831\n",
      "\n",
      " Test Loss 0.8176230788230896\n",
      "\n",
      "Epoch: 252 Train Loss: 0.7747\n",
      "\n",
      " Test Loss 0.8093467950820923\n",
      "\n",
      "Epoch: 253 Train Loss: 0.7664\n",
      "\n",
      " Test Loss 0.8011455535888672\n",
      "\n",
      "Epoch: 254 Train Loss: 0.7582\n",
      "\n",
      " Test Loss 0.7930196523666382\n",
      "\n",
      "Epoch: 255 Train Loss: 0.7501\n",
      "\n",
      " Test Loss 0.7849694490432739\n",
      "\n",
      "Epoch: 256 Train Loss: 0.7420\n",
      "\n",
      " Test Loss 0.7769951820373535\n",
      "\n",
      "Epoch: 257 Train Loss: 0.7341\n",
      "\n",
      " Test Loss 0.7690972685813904\n",
      "\n",
      "Epoch: 258 Train Loss: 0.7262\n",
      "\n",
      " Test Loss 0.7612757682800293\n",
      "\n",
      "Epoch: 259 Train Loss: 0.7183\n",
      "\n",
      " Test Loss 0.7535313963890076\n",
      "\n",
      "Epoch: 260 Train Loss: 0.7106\n",
      "\n",
      " Test Loss 0.7458642721176147\n",
      "\n",
      "Epoch: 261 Train Loss: 0.7029\n",
      "\n",
      " Test Loss 0.7382748126983643\n",
      "\n",
      "Epoch: 262 Train Loss: 0.6954\n",
      "\n",
      " Test Loss 0.7307634353637695\n",
      "\n",
      "Epoch: 263 Train Loss: 0.6879\n",
      "\n",
      " Test Loss 0.7233304381370544\n",
      "\n",
      "Epoch: 264 Train Loss: 0.6804\n",
      "\n",
      " Test Loss 0.715976357460022\n",
      "\n",
      "Epoch: 265 Train Loss: 0.6731\n",
      "\n",
      " Test Loss 0.7087014317512512\n",
      "\n",
      "Epoch: 266 Train Loss: 0.6658\n",
      "\n",
      " Test Loss 0.7015060782432556\n",
      "\n",
      "Epoch: 267 Train Loss: 0.6587\n",
      "\n",
      " Test Loss 0.6943907737731934\n",
      "\n",
      "Epoch: 268 Train Loss: 0.6516\n",
      "\n",
      " Test Loss 0.6873558163642883\n",
      "\n",
      "Epoch: 269 Train Loss: 0.6445\n",
      "\n",
      " Test Loss 0.6804018020629883\n",
      "\n",
      "Epoch: 270 Train Loss: 0.6376\n",
      "\n",
      " Test Loss 0.673528790473938\n",
      "\n",
      "Epoch: 271 Train Loss: 0.6308\n",
      "\n",
      " Test Loss 0.6667376160621643\n",
      "\n",
      "Epoch: 272 Train Loss: 0.6240\n",
      "\n",
      " Test Loss 0.6600281596183777\n",
      "\n",
      "Epoch: 273 Train Loss: 0.6173\n",
      "\n",
      " Test Loss 0.6534013748168945\n",
      "\n",
      "Epoch: 274 Train Loss: 0.6107\n",
      "\n",
      " Test Loss 0.6468573212623596\n",
      "\n",
      "Epoch: 275 Train Loss: 0.6042\n",
      "\n",
      " Test Loss 0.6403964757919312\n",
      "\n",
      "Epoch: 276 Train Loss: 0.5978\n",
      "\n",
      " Test Loss 0.6340191960334778\n",
      "\n",
      "Epoch: 277 Train Loss: 0.5914\n",
      "\n",
      " Test Loss 0.6277258992195129\n",
      "\n",
      "Epoch: 278 Train Loss: 0.5852\n",
      "\n",
      " Test Loss 0.6215168833732605\n",
      "\n",
      "Epoch: 279 Train Loss: 0.5790\n",
      "\n",
      " Test Loss 0.6153926849365234\n",
      "\n",
      "Epoch: 280 Train Loss: 0.5729\n",
      "\n",
      " Test Loss 0.6093533635139465\n",
      "\n",
      "Epoch: 281 Train Loss: 0.5669\n",
      "\n",
      " Test Loss 0.603399395942688\n",
      "\n",
      "Epoch: 282 Train Loss: 0.5610\n",
      "\n",
      " Test Loss 0.5975309610366821\n",
      "\n",
      "Epoch: 283 Train Loss: 0.5552\n",
      "\n",
      " Test Loss 0.5917485952377319\n",
      "\n",
      "Epoch: 284 Train Loss: 0.5495\n",
      "\n",
      " Test Loss 0.5860522985458374\n",
      "\n",
      "Epoch: 285 Train Loss: 0.5439\n",
      "\n",
      " Test Loss 0.580442488193512\n",
      "\n",
      "Epoch: 286 Train Loss: 0.5383\n",
      "\n",
      " Test Loss 0.5749194622039795\n",
      "\n",
      "Epoch: 287 Train Loss: 0.5329\n",
      "\n",
      " Test Loss 0.5694830417633057\n",
      "\n",
      "Epoch: 288 Train Loss: 0.5275\n",
      "\n",
      " Test Loss 0.5641337633132935\n",
      "\n",
      "Epoch: 289 Train Loss: 0.5222\n",
      "\n",
      " Test Loss 0.5588715672492981\n",
      "\n",
      "Epoch: 290 Train Loss: 0.5170\n",
      "\n",
      " Test Loss 0.5536965727806091\n",
      "\n",
      "Epoch: 291 Train Loss: 0.5119\n",
      "\n",
      " Test Loss 0.5486089587211609\n",
      "\n",
      "Epoch: 292 Train Loss: 0.5069\n",
      "\n",
      " Test Loss 0.5436086058616638\n",
      "\n",
      "Epoch: 293 Train Loss: 0.5020\n",
      "\n",
      " Test Loss 0.5386956334114075\n",
      "\n",
      "Epoch: 294 Train Loss: 0.4972\n",
      "\n",
      " Test Loss 0.5338699221611023\n",
      "\n",
      "Epoch: 295 Train Loss: 0.4925\n",
      "\n",
      " Test Loss 0.5291314125061035\n",
      "\n",
      "Epoch: 296 Train Loss: 0.4878\n",
      "\n",
      " Test Loss 0.5244799256324768\n",
      "\n",
      "Epoch: 297 Train Loss: 0.4833\n",
      "\n",
      " Test Loss 0.5199154019355774\n",
      "\n",
      "Epoch: 298 Train Loss: 0.4788\n",
      "\n",
      " Test Loss 0.5154374837875366\n",
      "\n",
      "Epoch: 299 Train Loss: 0.4744\n",
      "\n",
      " Test Loss 0.5110461115837097\n",
      "\n",
      "Epoch: 300 Train Loss: 0.4701\n",
      "\n",
      " Test Loss 0.5067408084869385\n",
      "\n",
      "Epoch: 301 Train Loss: 0.4659\n",
      "\n",
      " Test Loss 0.5025212168693542\n",
      "\n",
      "Epoch: 302 Train Loss: 0.4618\n",
      "\n",
      " Test Loss 0.49838703870773315\n",
      "\n",
      "Epoch: 303 Train Loss: 0.4578\n",
      "\n",
      " Test Loss 0.4943377673625946\n",
      "\n",
      "Epoch: 304 Train Loss: 0.4539\n",
      "\n",
      " Test Loss 0.4903728663921356\n",
      "\n",
      "Epoch: 305 Train Loss: 0.4501\n",
      "\n",
      " Test Loss 0.486491858959198\n",
      "\n",
      "Epoch: 306 Train Loss: 0.4463\n",
      "\n",
      " Test Loss 0.4826941192150116\n",
      "\n",
      "Epoch: 307 Train Loss: 0.4426\n",
      "\n",
      " Test Loss 0.4789789319038391\n",
      "\n",
      "Epoch: 308 Train Loss: 0.4391\n",
      "\n",
      " Test Loss 0.4753457009792328\n",
      "\n",
      "Epoch: 309 Train Loss: 0.4356\n",
      "\n",
      " Test Loss 0.47179368138313293\n",
      "\n",
      "Epoch: 310 Train Loss: 0.4322\n",
      "\n",
      " Test Loss 0.4683220088481903\n",
      "\n",
      "Epoch: 311 Train Loss: 0.4288\n",
      "\n",
      " Test Loss 0.4649299383163452\n",
      "\n",
      "Epoch: 312 Train Loss: 0.4256\n",
      "\n",
      " Test Loss 0.461616575717926\n",
      "\n",
      "Epoch: 313 Train Loss: 0.4224\n",
      "\n",
      " Test Loss 0.45838087797164917\n",
      "\n",
      "Epoch: 314 Train Loss: 0.4193\n",
      "\n",
      " Test Loss 0.455221951007843\n",
      "\n",
      "Epoch: 315 Train Loss: 0.4163\n",
      "\n",
      " Test Loss 0.4521388113498688\n",
      "\n",
      "Epoch: 316 Train Loss: 0.4134\n",
      "\n",
      " Test Loss 0.44913041591644287\n",
      "\n",
      "Epoch: 317 Train Loss: 0.4106\n",
      "\n",
      " Test Loss 0.4461956024169922\n",
      "\n",
      "Epoch: 318 Train Loss: 0.4078\n",
      "\n",
      " Test Loss 0.44333329796791077\n",
      "\n",
      "Epoch: 319 Train Loss: 0.4051\n",
      "\n",
      " Test Loss 0.44054239988327026\n",
      "\n",
      "Epoch: 320 Train Loss: 0.4025\n",
      "\n",
      " Test Loss 0.4378215968608856\n",
      "\n",
      "Epoch: 321 Train Loss: 0.3999\n",
      "\n",
      " Test Loss 0.4351697862148285\n",
      "\n",
      "Epoch: 322 Train Loss: 0.3974\n",
      "\n",
      " Test Loss 0.4325857162475586\n",
      "\n",
      "Epoch: 323 Train Loss: 0.3950\n",
      "\n",
      " Test Loss 0.43006807565689087\n",
      "\n",
      "Epoch: 324 Train Loss: 0.3927\n",
      "\n",
      " Test Loss 0.42761561274528503\n",
      "\n",
      "Epoch: 325 Train Loss: 0.3904\n",
      "\n",
      " Test Loss 0.42522698640823364\n",
      "\n",
      "Epoch: 326 Train Loss: 0.3882\n",
      "\n",
      " Test Loss 0.42290088534355164\n",
      "\n",
      "Epoch: 327 Train Loss: 0.3860\n",
      "\n",
      " Test Loss 0.42063599824905396\n",
      "\n",
      "Epoch: 328 Train Loss: 0.3839\n",
      "\n",
      " Test Loss 0.41843101382255554\n",
      "\n",
      "Epoch: 329 Train Loss: 0.3819\n",
      "\n",
      " Test Loss 0.41628456115722656\n",
      "\n",
      "Epoch: 330 Train Loss: 0.3799\n",
      "\n",
      " Test Loss 0.4141952693462372\n",
      "\n",
      "Epoch: 331 Train Loss: 0.3780\n",
      "\n",
      " Test Loss 0.4121617376804352\n",
      "\n",
      "Epoch: 332 Train Loss: 0.3762\n",
      "\n",
      " Test Loss 0.4101826846599579\n",
      "\n",
      "Epoch: 333 Train Loss: 0.3744\n",
      "\n",
      " Test Loss 0.4082566797733307\n",
      "\n",
      "Epoch: 334 Train Loss: 0.3726\n",
      "\n",
      " Test Loss 0.4063824415206909\n",
      "\n",
      "Epoch: 335 Train Loss: 0.3709\n",
      "\n",
      " Test Loss 0.40455853939056396\n",
      "\n",
      "Epoch: 336 Train Loss: 0.3693\n",
      "\n",
      " Test Loss 0.40278375148773193\n",
      "\n",
      "Epoch: 337 Train Loss: 0.3677\n",
      "\n",
      " Test Loss 0.40105652809143066\n",
      "\n",
      "Epoch: 338 Train Loss: 0.3662\n",
      "\n",
      " Test Loss 0.3993757367134094\n",
      "\n",
      "Epoch: 339 Train Loss: 0.3646\n",
      "\n",
      " Test Loss 0.3977400064468384\n",
      "\n",
      "Epoch: 340 Train Loss: 0.3632\n",
      "\n",
      " Test Loss 0.39614808559417725\n",
      "\n",
      "Epoch: 341 Train Loss: 0.3618\n",
      "\n",
      " Test Loss 0.3945985734462738\n",
      "\n",
      "Epoch: 342 Train Loss: 0.3604\n",
      "\n",
      " Test Loss 0.3930903971195221\n",
      "\n",
      "Epoch: 343 Train Loss: 0.3591\n",
      "\n",
      " Test Loss 0.3916221261024475\n",
      "\n",
      "Epoch: 344 Train Loss: 0.3578\n",
      "\n",
      " Test Loss 0.3901926577091217\n",
      "\n",
      "Epoch: 345 Train Loss: 0.3565\n",
      "\n",
      " Test Loss 0.3888007402420044\n",
      "\n",
      "Epoch: 346 Train Loss: 0.3553\n",
      "\n",
      " Test Loss 0.38744524121284485\n",
      "\n",
      "Epoch: 347 Train Loss: 0.3541\n",
      "\n",
      " Test Loss 0.38612496852874756\n",
      "\n",
      "Epoch: 348 Train Loss: 0.3530\n",
      "\n",
      " Test Loss 0.3848388195037842\n",
      "\n",
      "Epoch: 349 Train Loss: 0.3518\n",
      "\n",
      " Test Loss 0.3835856318473816\n",
      "\n",
      "Epoch: 350 Train Loss: 0.3508\n",
      "\n",
      " Test Loss 0.3823643922805786\n",
      "\n",
      "Epoch: 351 Train Loss: 0.3497\n",
      "\n",
      " Test Loss 0.3811739981174469\n",
      "\n",
      "Epoch: 352 Train Loss: 0.3487\n",
      "\n",
      " Test Loss 0.3800134062767029\n",
      "\n",
      "Epoch: 353 Train Loss: 0.3477\n",
      "\n",
      " Test Loss 0.3788817226886749\n",
      "\n",
      "Epoch: 354 Train Loss: 0.3467\n",
      "\n",
      " Test Loss 0.37777775526046753\n",
      "\n",
      "Epoch: 355 Train Loss: 0.3458\n",
      "\n",
      " Test Loss 0.3767007291316986\n",
      "\n",
      "Epoch: 356 Train Loss: 0.3448\n",
      "\n",
      " Test Loss 0.37564966082572937\n",
      "\n",
      "Epoch: 357 Train Loss: 0.3439\n",
      "\n",
      " Test Loss 0.3746235966682434\n",
      "\n",
      "Epoch: 358 Train Loss: 0.3431\n",
      "\n",
      " Test Loss 0.37362170219421387\n",
      "\n",
      "Epoch: 359 Train Loss: 0.3422\n",
      "\n",
      " Test Loss 0.3726431727409363\n",
      "\n",
      "Epoch: 360 Train Loss: 0.3414\n",
      "\n",
      " Test Loss 0.37168705463409424\n",
      "\n",
      "Epoch: 361 Train Loss: 0.3406\n",
      "\n",
      " Test Loss 0.37075263261795044\n",
      "\n",
      "Epoch: 362 Train Loss: 0.3398\n",
      "\n",
      " Test Loss 0.3698391914367676\n",
      "\n",
      "Epoch: 363 Train Loss: 0.3390\n",
      "\n",
      " Test Loss 0.36894580721855164\n",
      "\n",
      "Epoch: 364 Train Loss: 0.3383\n",
      "\n",
      " Test Loss 0.36807188391685486\n",
      "\n",
      "Epoch: 365 Train Loss: 0.3375\n",
      "\n",
      " Test Loss 0.36721667647361755\n",
      "\n",
      "Epoch: 366 Train Loss: 0.3368\n",
      "\n",
      " Test Loss 0.3663794696331024\n",
      "\n",
      "Epoch: 367 Train Loss: 0.3361\n",
      "\n",
      " Test Loss 0.3655596077442169\n",
      "\n",
      "Epoch: 368 Train Loss: 0.3354\n",
      "\n",
      " Test Loss 0.36475643515586853\n",
      "\n",
      "Epoch: 369 Train Loss: 0.3348\n",
      "\n",
      " Test Loss 0.36396944522857666\n",
      "\n",
      "Epoch: 370 Train Loss: 0.3341\n",
      "\n",
      " Test Loss 0.3631979823112488\n",
      "\n",
      "Epoch: 371 Train Loss: 0.3335\n",
      "\n",
      " Test Loss 0.36244136095046997\n",
      "\n",
      "Epoch: 372 Train Loss: 0.3328\n",
      "\n",
      " Test Loss 0.3616991937160492\n",
      "\n",
      "Epoch: 373 Train Loss: 0.3322\n",
      "\n",
      " Test Loss 0.3609708547592163\n",
      "\n",
      "Epoch: 374 Train Loss: 0.3316\n",
      "\n",
      " Test Loss 0.36025577783584595\n",
      "\n",
      "Epoch: 375 Train Loss: 0.3310\n",
      "\n",
      " Test Loss 0.35955357551574707\n",
      "\n",
      "Epoch: 376 Train Loss: 0.3304\n",
      "\n",
      " Test Loss 0.3588636815547943\n",
      "\n",
      "Epoch: 377 Train Loss: 0.3298\n",
      "\n",
      " Test Loss 0.35818567872047424\n",
      "\n",
      "Epoch: 378 Train Loss: 0.3293\n",
      "\n",
      " Test Loss 0.35751909017562866\n",
      "\n",
      "Epoch: 379 Train Loss: 0.3287\n",
      "\n",
      " Test Loss 0.3568635582923889\n",
      "\n",
      "Epoch: 380 Train Loss: 0.3282\n",
      "\n",
      " Test Loss 0.3562186062335968\n",
      "\n",
      "Epoch: 381 Train Loss: 0.3276\n",
      "\n",
      " Test Loss 0.3555838167667389\n",
      "\n",
      "Epoch: 382 Train Loss: 0.3271\n",
      "\n",
      " Test Loss 0.3549588918685913\n",
      "\n",
      "Epoch: 383 Train Loss: 0.3266\n",
      "\n",
      " Test Loss 0.3543432950973511\n",
      "\n",
      "Epoch: 384 Train Loss: 0.3261\n",
      "\n",
      " Test Loss 0.353736937046051\n",
      "\n",
      "Epoch: 385 Train Loss: 0.3256\n",
      "\n",
      " Test Loss 0.3531392514705658\n",
      "\n",
      "Epoch: 386 Train Loss: 0.3251\n",
      "\n",
      " Test Loss 0.3525499403476715\n",
      "\n",
      "Epoch: 387 Train Loss: 0.3246\n",
      "\n",
      " Test Loss 0.35196876525878906\n",
      "\n",
      "Epoch: 388 Train Loss: 0.3241\n",
      "\n",
      " Test Loss 0.3513953983783722\n",
      "\n",
      "Epoch: 389 Train Loss: 0.3236\n",
      "\n",
      " Test Loss 0.35082948207855225\n",
      "\n",
      "Epoch: 390 Train Loss: 0.3231\n",
      "\n",
      " Test Loss 0.3502708077430725\n",
      "\n",
      "Epoch: 391 Train Loss: 0.3227\n",
      "\n",
      " Test Loss 0.3497191071510315\n",
      "\n",
      "Epoch: 392 Train Loss: 0.3222\n",
      "\n",
      " Test Loss 0.3491741120815277\n",
      "\n",
      "Epoch: 393 Train Loss: 0.3217\n",
      "\n",
      " Test Loss 0.34863555431365967\n",
      "\n",
      "Epoch: 394 Train Loss: 0.3213\n",
      "\n",
      " Test Loss 0.34810325503349304\n",
      "\n",
      "Epoch: 395 Train Loss: 0.3208\n",
      "\n",
      " Test Loss 0.3475768268108368\n",
      "\n",
      "Epoch: 396 Train Loss: 0.3204\n",
      "\n",
      " Test Loss 0.3470562696456909\n",
      "\n",
      "Epoch: 397 Train Loss: 0.3199\n",
      "\n",
      " Test Loss 0.346541166305542\n",
      "\n",
      "Epoch: 398 Train Loss: 0.3195\n",
      "\n",
      " Test Loss 0.3460314869880676\n",
      "\n",
      "Epoch: 399 Train Loss: 0.3191\n",
      "\n",
      " Test Loss 0.34552690386772156\n",
      "\n",
      "Epoch: 400 Train Loss: 0.3186\n",
      "\n",
      " Test Loss 0.34502726793289185\n",
      "\n",
      "Epoch: 401 Train Loss: 0.3182\n",
      "\n",
      " Test Loss 0.34453248977661133\n",
      "\n",
      "Epoch: 402 Train Loss: 0.3178\n",
      "\n",
      " Test Loss 0.34404227137565613\n",
      "\n",
      "Epoch: 403 Train Loss: 0.3174\n",
      "\n",
      " Test Loss 0.3435564637184143\n",
      "\n",
      "Epoch: 404 Train Loss: 0.3170\n",
      "\n",
      " Test Loss 0.3430749773979187\n",
      "\n",
      "Epoch: 405 Train Loss: 0.3165\n",
      "\n",
      " Test Loss 0.3425976037979126\n",
      "\n",
      "Epoch: 406 Train Loss: 0.3161\n",
      "\n",
      " Test Loss 0.34212422370910645\n",
      "\n",
      "Epoch: 407 Train Loss: 0.3157\n",
      "\n",
      " Test Loss 0.34165462851524353\n",
      "\n",
      "Epoch: 408 Train Loss: 0.3153\n",
      "\n",
      " Test Loss 0.34118884801864624\n",
      "\n",
      "Epoch: 409 Train Loss: 0.3149\n",
      "\n",
      " Test Loss 0.3407265543937683\n",
      "\n",
      "Epoch: 410 Train Loss: 0.3145\n",
      "\n",
      " Test Loss 0.34026771783828735\n",
      "\n",
      "Epoch: 411 Train Loss: 0.3141\n",
      "\n",
      " Test Loss 0.3398122191429138\n",
      "\n",
      "Epoch: 412 Train Loss: 0.3137\n",
      "\n",
      " Test Loss 0.33935993909835815\n",
      "\n",
      "Epoch: 413 Train Loss: 0.3133\n",
      "\n",
      " Test Loss 0.3389107584953308\n",
      "\n",
      "Epoch: 414 Train Loss: 0.3129\n",
      "\n",
      " Test Loss 0.3384645879268646\n",
      "\n",
      "Epoch: 415 Train Loss: 0.3126\n",
      "\n",
      " Test Loss 0.33802127838134766\n",
      "\n",
      "Epoch: 416 Train Loss: 0.3122\n",
      "\n",
      " Test Loss 0.3375808298587799\n",
      "\n",
      "Epoch: 417 Train Loss: 0.3118\n",
      "\n",
      " Test Loss 0.3371429741382599\n",
      "\n",
      "Epoch: 418 Train Loss: 0.3114\n",
      "\n",
      " Test Loss 0.33670780062675476\n",
      "\n",
      "Epoch: 419 Train Loss: 0.3110\n",
      "\n",
      " Test Loss 0.3362751305103302\n",
      "\n",
      "Epoch: 420 Train Loss: 0.3106\n",
      "\n",
      " Test Loss 0.3358449339866638\n",
      "\n",
      "Epoch: 421 Train Loss: 0.3102\n",
      "\n",
      " Test Loss 0.3354170620441437\n",
      "\n",
      "Epoch: 422 Train Loss: 0.3099\n",
      "\n",
      " Test Loss 0.3349914848804474\n",
      "\n",
      "Epoch: 423 Train Loss: 0.3095\n",
      "\n",
      " Test Loss 0.3345681130886078\n",
      "\n",
      "Epoch: 424 Train Loss: 0.3091\n",
      "\n",
      " Test Loss 0.3341468274593353\n",
      "\n",
      "Epoch: 425 Train Loss: 0.3087\n",
      "\n",
      " Test Loss 0.3337276875972748\n",
      "\n",
      "Epoch: 426 Train Loss: 0.3084\n",
      "\n",
      " Test Loss 0.3333105146884918\n",
      "\n",
      "Epoch: 427 Train Loss: 0.3080\n",
      "\n",
      " Test Loss 0.33289530873298645\n",
      "\n",
      "Epoch: 428 Train Loss: 0.3076\n",
      "\n",
      " Test Loss 0.3324819505214691\n",
      "\n",
      "Epoch: 429 Train Loss: 0.3072\n",
      "\n",
      " Test Loss 0.33207041025161743\n",
      "\n",
      "Epoch: 430 Train Loss: 0.3069\n",
      "\n",
      " Test Loss 0.3316606283187866\n",
      "\n",
      "Epoch: 431 Train Loss: 0.3065\n",
      "\n",
      " Test Loss 0.3312526345252991\n",
      "\n",
      "Epoch: 432 Train Loss: 0.3061\n",
      "\n",
      " Test Loss 0.33084625005722046\n",
      "\n",
      "Epoch: 433 Train Loss: 0.3058\n",
      "\n",
      " Test Loss 0.33044153451919556\n",
      "\n",
      "Epoch: 434 Train Loss: 0.3054\n",
      "\n",
      " Test Loss 0.33003827929496765\n",
      "\n",
      "Epoch: 435 Train Loss: 0.3050\n",
      "\n",
      " Test Loss 0.3296366035938263\n",
      "\n",
      "Epoch: 436 Train Loss: 0.3047\n",
      "\n",
      " Test Loss 0.32923638820648193\n",
      "\n",
      "Epoch: 437 Train Loss: 0.3043\n",
      "\n",
      " Test Loss 0.32883763313293457\n",
      "\n",
      "Epoch: 438 Train Loss: 0.3040\n",
      "\n",
      " Test Loss 0.32844024896621704\n",
      "\n",
      "Epoch: 439 Train Loss: 0.3036\n",
      "\n",
      " Test Loss 0.32804417610168457\n",
      "\n",
      "Epoch: 440 Train Loss: 0.3032\n",
      "\n",
      " Test Loss 0.32764947414398193\n",
      "\n",
      "Epoch: 441 Train Loss: 0.3029\n",
      "\n",
      " Test Loss 0.3272559940814972\n",
      "\n",
      "Epoch: 442 Train Loss: 0.3025\n",
      "\n",
      " Test Loss 0.3268638551235199\n",
      "\n",
      "Epoch: 443 Train Loss: 0.3021\n",
      "\n",
      " Test Loss 0.32647284865379333\n",
      "\n",
      "Epoch: 444 Train Loss: 0.3018\n",
      "\n",
      " Test Loss 0.3260830342769623\n",
      "\n",
      "Epoch: 445 Train Loss: 0.3014\n",
      "\n",
      " Test Loss 0.32569435238838196\n",
      "\n",
      "Epoch: 446 Train Loss: 0.3011\n",
      "\n",
      " Test Loss 0.32530683279037476\n",
      "\n",
      "Epoch: 447 Train Loss: 0.3007\n",
      "\n",
      " Test Loss 0.32492032647132874\n",
      "\n",
      "Epoch: 448 Train Loss: 0.3004\n",
      "\n",
      " Test Loss 0.32453495264053345\n",
      "\n",
      "Epoch: 449 Train Loss: 0.3000\n",
      "\n",
      " Test Loss 0.32415059208869934\n",
      "\n",
      "Epoch: 450 Train Loss: 0.2996\n",
      "\n",
      " Test Loss 0.3237672448158264\n",
      "\n",
      "Epoch: 451 Train Loss: 0.2993\n",
      "\n",
      " Test Loss 0.3233849108219147\n",
      "\n",
      "Epoch: 452 Train Loss: 0.2989\n",
      "\n",
      " Test Loss 0.32300347089767456\n",
      "\n",
      "Epoch: 453 Train Loss: 0.2986\n",
      "\n",
      " Test Loss 0.32262298464775085\n",
      "\n",
      "Epoch: 454 Train Loss: 0.2982\n",
      "\n",
      " Test Loss 0.32224345207214355\n",
      "\n",
      "Epoch: 455 Train Loss: 0.2979\n",
      "\n",
      " Test Loss 0.3218647837638855\n",
      "\n",
      "Epoch: 456 Train Loss: 0.2975\n",
      "\n",
      " Test Loss 0.3214870095252991\n",
      "\n",
      "Epoch: 457 Train Loss: 0.2972\n",
      "\n",
      " Test Loss 0.3211100995540619\n",
      "\n",
      "Epoch: 458 Train Loss: 0.2968\n",
      "\n",
      " Test Loss 0.32073408365249634\n",
      "\n",
      "Epoch: 459 Train Loss: 0.2965\n",
      "\n",
      " Test Loss 0.3203587830066681\n",
      "\n",
      "Epoch: 460 Train Loss: 0.2961\n",
      "\n",
      " Test Loss 0.3199843466281891\n",
      "\n",
      "Epoch: 461 Train Loss: 0.2958\n",
      "\n",
      " Test Loss 0.31961068511009216\n",
      "\n",
      "Epoch: 462 Train Loss: 0.2954\n",
      "\n",
      " Test Loss 0.31923776865005493\n",
      "\n",
      "Epoch: 463 Train Loss: 0.2951\n",
      "\n",
      " Test Loss 0.31886568665504456\n",
      "\n",
      "Epoch: 464 Train Loss: 0.2947\n",
      "\n",
      " Test Loss 0.3184942603111267\n",
      "\n",
      "Epoch: 465 Train Loss: 0.2944\n",
      "\n",
      " Test Loss 0.31812360882759094\n",
      "\n",
      "Epoch: 466 Train Loss: 0.2940\n",
      "\n",
      " Test Loss 0.3177535831928253\n",
      "\n",
      "Epoch: 467 Train Loss: 0.2937\n",
      "\n",
      " Test Loss 0.31738439202308655\n",
      "\n",
      "Epoch: 468 Train Loss: 0.2933\n",
      "\n",
      " Test Loss 0.31701576709747314\n",
      "\n",
      "Epoch: 469 Train Loss: 0.2930\n",
      "\n",
      " Test Loss 0.31664788722991943\n",
      "\n",
      "Epoch: 470 Train Loss: 0.2926\n",
      "\n",
      " Test Loss 0.31628063321113586\n",
      "\n",
      "Epoch: 471 Train Loss: 0.2923\n",
      "\n",
      " Test Loss 0.31591400504112244\n",
      "\n",
      "Epoch: 472 Train Loss: 0.2919\n",
      "\n",
      " Test Loss 0.31554800271987915\n",
      "\n",
      "Epoch: 473 Train Loss: 0.2916\n",
      "\n",
      " Test Loss 0.31518274545669556\n",
      "\n",
      "Epoch: 474 Train Loss: 0.2912\n",
      "\n",
      " Test Loss 0.31481799483299255\n",
      "\n",
      "Epoch: 475 Train Loss: 0.2909\n",
      "\n",
      " Test Loss 0.3144538998603821\n",
      "\n",
      "Epoch: 476 Train Loss: 0.2905\n",
      "\n",
      " Test Loss 0.31409040093421936\n",
      "\n",
      "Epoch: 477 Train Loss: 0.2902\n",
      "\n",
      " Test Loss 0.3137274980545044\n",
      "\n",
      "Epoch: 478 Train Loss: 0.2898\n",
      "\n",
      " Test Loss 0.31336510181427\n",
      "\n",
      "Epoch: 479 Train Loss: 0.2895\n",
      "\n",
      " Test Loss 0.31300339102745056\n",
      "\n",
      "Epoch: 480 Train Loss: 0.2892\n",
      "\n",
      " Test Loss 0.3126421570777893\n",
      "\n",
      "Epoch: 481 Train Loss: 0.2888\n",
      "\n",
      " Test Loss 0.3122815191745758\n",
      "\n",
      "Epoch: 482 Train Loss: 0.2885\n",
      "\n",
      " Test Loss 0.3119214177131653\n",
      "\n",
      "Epoch: 483 Train Loss: 0.2881\n",
      "\n",
      " Test Loss 0.3115618824958801\n",
      "\n",
      "Epoch: 484 Train Loss: 0.2878\n",
      "\n",
      " Test Loss 0.31120285391807556\n",
      "\n",
      "Epoch: 485 Train Loss: 0.2874\n",
      "\n",
      " Test Loss 0.3108443319797516\n",
      "\n",
      "Epoch: 486 Train Loss: 0.2871\n",
      "\n",
      " Test Loss 0.31048640608787537\n",
      "\n",
      "Epoch: 487 Train Loss: 0.2868\n",
      "\n",
      " Test Loss 0.31012895703315735\n",
      "\n",
      "Epoch: 488 Train Loss: 0.2864\n",
      "\n",
      " Test Loss 0.3097720146179199\n",
      "\n",
      "Epoch: 489 Train Loss: 0.2861\n",
      "\n",
      " Test Loss 0.30941563844680786\n",
      "\n",
      "Epoch: 490 Train Loss: 0.2857\n",
      "\n",
      " Test Loss 0.30905964970588684\n",
      "\n",
      "Epoch: 491 Train Loss: 0.2854\n",
      "\n",
      " Test Loss 0.30870428681373596\n",
      "\n",
      "Epoch: 492 Train Loss: 0.2850\n",
      "\n",
      " Test Loss 0.3083493113517761\n",
      "\n",
      "Epoch: 493 Train Loss: 0.2847\n",
      "\n",
      " Test Loss 0.30799487233161926\n",
      "\n",
      "Epoch: 494 Train Loss: 0.2844\n",
      "\n",
      " Test Loss 0.3076408803462982\n",
      "\n",
      "Epoch: 495 Train Loss: 0.2840\n",
      "\n",
      " Test Loss 0.30728739500045776\n",
      "\n",
      "Epoch: 496 Train Loss: 0.2837\n",
      "\n",
      " Test Loss 0.3069343566894531\n",
      "\n",
      "Epoch: 497 Train Loss: 0.2833\n",
      "\n",
      " Test Loss 0.3065818250179291\n",
      "\n",
      "Epoch: 498 Train Loss: 0.2830\n",
      "\n",
      " Test Loss 0.30622971057891846\n",
      "\n",
      "Epoch: 499 Train Loss: 0.2827\n",
      "\n",
      " Test Loss 0.30587807297706604\n",
      "\n",
      "Epoch: 500 Train Loss: 0.2823\n",
      "\n",
      " Test Loss 0.3055269122123718\n",
      "\n",
      "Epoch: 501 Train Loss: 0.2820\n",
      "\n",
      " Test Loss 0.3051762282848358\n",
      "\n",
      "Epoch: 502 Train Loss: 0.2817\n",
      "\n",
      " Test Loss 0.30482596158981323\n",
      "\n",
      "Epoch: 503 Train Loss: 0.2813\n",
      "\n",
      " Test Loss 0.30447614192962646\n",
      "\n",
      "Epoch: 504 Train Loss: 0.2810\n",
      "\n",
      " Test Loss 0.3041267991065979\n",
      "\n",
      "Epoch: 505 Train Loss: 0.2806\n",
      "\n",
      " Test Loss 0.3037778437137604\n",
      "\n",
      "Epoch: 506 Train Loss: 0.2803\n",
      "\n",
      " Test Loss 0.30342936515808105\n",
      "\n",
      "Epoch: 507 Train Loss: 0.2800\n",
      "\n",
      " Test Loss 0.30308133363723755\n",
      "\n",
      "Epoch: 508 Train Loss: 0.2796\n",
      "\n",
      " Test Loss 0.30273377895355225\n",
      "\n",
      "Epoch: 509 Train Loss: 0.2793\n",
      "\n",
      " Test Loss 0.3023865818977356\n",
      "\n",
      "Epoch: 510 Train Loss: 0.2790\n",
      "\n",
      " Test Loss 0.30203986167907715\n",
      "\n",
      "Epoch: 511 Train Loss: 0.2786\n",
      "\n",
      " Test Loss 0.30169355869293213\n",
      "\n",
      "Epoch: 512 Train Loss: 0.2783\n",
      "\n",
      " Test Loss 0.30134764313697815\n",
      "\n",
      "Epoch: 513 Train Loss: 0.2779\n",
      "\n",
      " Test Loss 0.3010022044181824\n",
      "\n",
      "Epoch: 514 Train Loss: 0.2776\n",
      "\n",
      " Test Loss 0.3006571829319\n",
      "\n",
      "Epoch: 515 Train Loss: 0.2773\n",
      "\n",
      " Test Loss 0.3003126084804535\n",
      "\n",
      "Epoch: 516 Train Loss: 0.2769\n",
      "\n",
      " Test Loss 0.2999683916568756\n",
      "\n",
      "Epoch: 517 Train Loss: 0.2766\n",
      "\n",
      " Test Loss 0.2996246814727783\n",
      "\n",
      "Epoch: 518 Train Loss: 0.2763\n",
      "\n",
      " Test Loss 0.2992812693119049\n",
      "\n",
      "Epoch: 519 Train Loss: 0.2759\n",
      "\n",
      " Test Loss 0.2989383935928345\n",
      "\n",
      "Epoch: 520 Train Loss: 0.2756\n",
      "\n",
      " Test Loss 0.2985958755016327\n",
      "\n",
      "Epoch: 521 Train Loss: 0.2753\n",
      "\n",
      " Test Loss 0.2982538342475891\n",
      "\n",
      "Epoch: 522 Train Loss: 0.2749\n",
      "\n",
      " Test Loss 0.2979121506214142\n",
      "\n",
      "Epoch: 523 Train Loss: 0.2746\n",
      "\n",
      " Test Loss 0.2975708544254303\n",
      "\n",
      "Epoch: 524 Train Loss: 0.2743\n",
      "\n",
      " Test Loss 0.2972300350666046\n",
      "\n",
      "Epoch: 525 Train Loss: 0.2740\n",
      "\n",
      " Test Loss 0.2968895435333252\n",
      "\n",
      "Epoch: 526 Train Loss: 0.2736\n",
      "\n",
      " Test Loss 0.29654961824417114\n",
      "\n",
      "Epoch: 527 Train Loss: 0.2733\n",
      "\n",
      " Test Loss 0.2962099313735962\n",
      "\n",
      "Epoch: 528 Train Loss: 0.2730\n",
      "\n",
      " Test Loss 0.29587075114250183\n",
      "\n",
      "Epoch: 529 Train Loss: 0.2726\n",
      "\n",
      " Test Loss 0.2955319881439209\n",
      "\n",
      "Epoch: 530 Train Loss: 0.2723\n",
      "\n",
      " Test Loss 0.2951935827732086\n",
      "\n",
      "Epoch: 531 Train Loss: 0.2720\n",
      "\n",
      " Test Loss 0.29485565423965454\n",
      "\n",
      "Epoch: 532 Train Loss: 0.2716\n",
      "\n",
      " Test Loss 0.29451805353164673\n",
      "\n",
      "Epoch: 533 Train Loss: 0.2713\n",
      "\n",
      " Test Loss 0.2941809296607971\n",
      "\n",
      "Epoch: 534 Train Loss: 0.2710\n",
      "\n",
      " Test Loss 0.29384422302246094\n",
      "\n",
      "Epoch: 535 Train Loss: 0.2707\n",
      "\n",
      " Test Loss 0.2935079038143158\n",
      "\n",
      "Epoch: 536 Train Loss: 0.2703\n",
      "\n",
      " Test Loss 0.2931720018386841\n",
      "\n",
      "Epoch: 537 Train Loss: 0.2700\n",
      "\n",
      " Test Loss 0.29283642768859863\n",
      "\n",
      "Epoch: 538 Train Loss: 0.2697\n",
      "\n",
      " Test Loss 0.2925013601779938\n",
      "\n",
      "Epoch: 539 Train Loss: 0.2693\n",
      "\n",
      " Test Loss 0.29216670989990234\n",
      "\n",
      "Epoch: 540 Train Loss: 0.2690\n",
      "\n",
      " Test Loss 0.29183247685432434\n",
      "\n",
      "Epoch: 541 Train Loss: 0.2687\n",
      "\n",
      " Test Loss 0.291498601436615\n",
      "\n",
      "Epoch: 542 Train Loss: 0.2684\n",
      "\n",
      " Test Loss 0.29116517305374146\n",
      "\n",
      "Epoch: 543 Train Loss: 0.2680\n",
      "\n",
      " Test Loss 0.29083213210105896\n",
      "\n",
      "Epoch: 544 Train Loss: 0.2677\n",
      "\n",
      " Test Loss 0.2904995083808899\n",
      "\n",
      "Epoch: 545 Train Loss: 0.2674\n",
      "\n",
      " Test Loss 0.29016733169555664\n",
      "\n",
      "Epoch: 546 Train Loss: 0.2671\n",
      "\n",
      " Test Loss 0.28983554244041443\n",
      "\n",
      "Epoch: 547 Train Loss: 0.2667\n",
      "\n",
      " Test Loss 0.28950417041778564\n",
      "\n",
      "Epoch: 548 Train Loss: 0.2664\n",
      "\n",
      " Test Loss 0.2891732454299927\n",
      "\n",
      "Epoch: 549 Train Loss: 0.2661\n",
      "\n",
      " Test Loss 0.28884270787239075\n",
      "\n",
      "Epoch: 550 Train Loss: 0.2658\n",
      "\n",
      " Test Loss 0.28851261734962463\n",
      "\n",
      "Epoch: 551 Train Loss: 0.2654\n",
      "\n",
      " Test Loss 0.28818294405937195\n",
      "\n",
      "Epoch: 552 Train Loss: 0.2651\n",
      "\n",
      " Test Loss 0.2878536880016327\n",
      "\n",
      "Epoch: 553 Train Loss: 0.2648\n",
      "\n",
      " Test Loss 0.2875248193740845\n",
      "\n",
      "Epoch: 554 Train Loss: 0.2645\n",
      "\n",
      " Test Loss 0.28719639778137207\n",
      "\n",
      "Epoch: 555 Train Loss: 0.2642\n",
      "\n",
      " Test Loss 0.2868683934211731\n",
      "\n",
      "Epoch: 556 Train Loss: 0.2638\n",
      "\n",
      " Test Loss 0.28654083609580994\n",
      "\n",
      "Epoch: 557 Train Loss: 0.2635\n",
      "\n",
      " Test Loss 0.286213755607605\n",
      "\n",
      "Epoch: 558 Train Loss: 0.2632\n",
      "\n",
      " Test Loss 0.2858870029449463\n",
      "\n",
      "Epoch: 559 Train Loss: 0.2629\n",
      "\n",
      " Test Loss 0.2855607271194458\n",
      "\n",
      "Epoch: 560 Train Loss: 0.2626\n",
      "\n",
      " Test Loss 0.28523483872413635\n",
      "\n",
      "Epoch: 561 Train Loss: 0.2622\n",
      "\n",
      " Test Loss 0.2849093973636627\n",
      "\n",
      "Epoch: 562 Train Loss: 0.2619\n",
      "\n",
      " Test Loss 0.2845844030380249\n",
      "\n",
      "Epoch: 563 Train Loss: 0.2616\n",
      "\n",
      " Test Loss 0.2842598259449005\n",
      "\n",
      "Epoch: 564 Train Loss: 0.2613\n",
      "\n",
      " Test Loss 0.28393569588661194\n",
      "\n",
      "Epoch: 565 Train Loss: 0.2610\n",
      "\n",
      " Test Loss 0.2836120128631592\n",
      "\n",
      "Epoch: 566 Train Loss: 0.2606\n",
      "\n",
      " Test Loss 0.28328877687454224\n",
      "\n",
      "Epoch: 567 Train Loss: 0.2603\n",
      "\n",
      " Test Loss 0.28296592831611633\n",
      "\n",
      "Epoch: 568 Train Loss: 0.2600\n",
      "\n",
      " Test Loss 0.28264355659484863\n",
      "\n",
      "Epoch: 569 Train Loss: 0.2597\n",
      "\n",
      " Test Loss 0.282321572303772\n",
      "\n",
      "Epoch: 570 Train Loss: 0.2594\n",
      "\n",
      " Test Loss 0.2820000946521759\n",
      "\n",
      "Epoch: 571 Train Loss: 0.2591\n",
      "\n",
      " Test Loss 0.2816790044307709\n",
      "\n",
      "Epoch: 572 Train Loss: 0.2588\n",
      "\n",
      " Test Loss 0.2813584506511688\n",
      "\n",
      "Epoch: 573 Train Loss: 0.2584\n",
      "\n",
      " Test Loss 0.2810382843017578\n",
      "\n",
      "Epoch: 574 Train Loss: 0.2581\n",
      "\n",
      " Test Loss 0.2807185649871826\n",
      "\n",
      "Epoch: 575 Train Loss: 0.2578\n",
      "\n",
      " Test Loss 0.2803993225097656\n",
      "\n",
      "Epoch: 576 Train Loss: 0.2575\n",
      "\n",
      " Test Loss 0.28008049726486206\n",
      "\n",
      "Epoch: 577 Train Loss: 0.2572\n",
      "\n",
      " Test Loss 0.2797622084617615\n",
      "\n",
      "Epoch: 578 Train Loss: 0.2569\n",
      "\n",
      " Test Loss 0.27944424748420715\n",
      "\n",
      "Epoch: 579 Train Loss: 0.2566\n",
      "\n",
      " Test Loss 0.2791268229484558\n",
      "\n",
      "Epoch: 580 Train Loss: 0.2562\n",
      "\n",
      " Test Loss 0.2788098454475403\n",
      "\n",
      "Epoch: 581 Train Loss: 0.2559\n",
      "\n",
      " Test Loss 0.27849337458610535\n",
      "\n",
      "Epoch: 582 Train Loss: 0.2556\n",
      "\n",
      " Test Loss 0.2781773507595062\n",
      "\n",
      "Epoch: 583 Train Loss: 0.2553\n",
      "\n",
      " Test Loss 0.2778617739677429\n",
      "\n",
      "Epoch: 584 Train Loss: 0.2550\n",
      "\n",
      " Test Loss 0.2775466740131378\n",
      "\n",
      "Epoch: 585 Train Loss: 0.2547\n",
      "\n",
      " Test Loss 0.2772320508956909\n",
      "\n",
      "Epoch: 586 Train Loss: 0.2544\n",
      "\n",
      " Test Loss 0.2769179046154022\n",
      "\n",
      "Epoch: 587 Train Loss: 0.2541\n",
      "\n",
      " Test Loss 0.27660423517227173\n",
      "\n",
      "Epoch: 588 Train Loss: 0.2538\n",
      "\n",
      " Test Loss 0.27629104256629944\n",
      "\n",
      "Epoch: 589 Train Loss: 0.2535\n",
      "\n",
      " Test Loss 0.27597832679748535\n",
      "\n",
      "Epoch: 590 Train Loss: 0.2532\n",
      "\n",
      " Test Loss 0.2756660282611847\n",
      "\n",
      "Epoch: 591 Train Loss: 0.2528\n",
      "\n",
      " Test Loss 0.275354266166687\n",
      "\n",
      "Epoch: 592 Train Loss: 0.2525\n",
      "\n",
      " Test Loss 0.2750430107116699\n",
      "\n",
      "Epoch: 593 Train Loss: 0.2522\n",
      "\n",
      " Test Loss 0.27473223209381104\n",
      "\n",
      "Epoch: 594 Train Loss: 0.2519\n",
      "\n",
      " Test Loss 0.27442190051078796\n",
      "\n",
      "Epoch: 595 Train Loss: 0.2516\n",
      "\n",
      " Test Loss 0.2741120755672455\n",
      "\n",
      "Epoch: 596 Train Loss: 0.2513\n",
      "\n",
      " Test Loss 0.273802787065506\n",
      "\n",
      "Epoch: 597 Train Loss: 0.2510\n",
      "\n",
      " Test Loss 0.2734939455986023\n",
      "\n",
      "Epoch: 598 Train Loss: 0.2507\n",
      "\n",
      " Test Loss 0.2731856405735016\n",
      "\n",
      "Epoch: 599 Train Loss: 0.2504\n",
      "\n",
      " Test Loss 0.2728778123855591\n",
      "\n",
      "Epoch: 600 Train Loss: 0.2501\n",
      "\n",
      " Test Loss 0.2725704610347748\n",
      "\n",
      "Epoch: 601 Train Loss: 0.2498\n",
      "\n",
      " Test Loss 0.27226367592811584\n",
      "\n",
      "Epoch: 602 Train Loss: 0.2495\n",
      "\n",
      " Test Loss 0.2719573676586151\n",
      "\n",
      "Epoch: 603 Train Loss: 0.2492\n",
      "\n",
      " Test Loss 0.27165156602859497\n",
      "\n",
      "Epoch: 604 Train Loss: 0.2489\n",
      "\n",
      " Test Loss 0.27134624123573303\n",
      "\n",
      "Epoch: 605 Train Loss: 0.2486\n",
      "\n",
      " Test Loss 0.27104148268699646\n",
      "\n",
      "Epoch: 606 Train Loss: 0.2483\n",
      "\n",
      " Test Loss 0.2707372307777405\n",
      "\n",
      "Epoch: 607 Train Loss: 0.2480\n",
      "\n",
      " Test Loss 0.2704335153102875\n",
      "\n",
      "Epoch: 608 Train Loss: 0.2477\n",
      "\n",
      " Test Loss 0.2701302468776703\n",
      "\n",
      "Epoch: 609 Train Loss: 0.2474\n",
      "\n",
      " Test Loss 0.26982754468917847\n",
      "\n",
      "Epoch: 610 Train Loss: 0.2471\n",
      "\n",
      " Test Loss 0.269525408744812\n",
      "\n",
      "Epoch: 611 Train Loss: 0.2468\n",
      "\n",
      " Test Loss 0.26922377943992615\n",
      "\n",
      "Epoch: 612 Train Loss: 0.2465\n",
      "\n",
      " Test Loss 0.2689226567745209\n",
      "\n",
      "Epoch: 613 Train Loss: 0.2462\n",
      "\n",
      " Test Loss 0.2686220705509186\n",
      "\n",
      "Epoch: 614 Train Loss: 0.2459\n",
      "\n",
      " Test Loss 0.26832202076911926\n",
      "\n",
      "Epoch: 615 Train Loss: 0.2456\n",
      "\n",
      " Test Loss 0.26802247762680054\n",
      "\n",
      "Epoch: 616 Train Loss: 0.2453\n",
      "\n",
      " Test Loss 0.26772353053092957\n",
      "\n",
      "Epoch: 617 Train Loss: 0.2450\n",
      "\n",
      " Test Loss 0.2674251198768616\n",
      "\n",
      "Epoch: 618 Train Loss: 0.2447\n",
      "\n",
      " Test Loss 0.26712724566459656\n",
      "\n",
      "Epoch: 619 Train Loss: 0.2445\n",
      "\n",
      " Test Loss 0.26682987809181213\n",
      "\n",
      "Epoch: 620 Train Loss: 0.2442\n",
      "\n",
      " Test Loss 0.26653310656547546\n",
      "\n",
      "Epoch: 621 Train Loss: 0.2439\n",
      "\n",
      " Test Loss 0.2662368714809418\n",
      "\n",
      "Epoch: 622 Train Loss: 0.2436\n",
      "\n",
      " Test Loss 0.26594117283821106\n",
      "\n",
      "Epoch: 623 Train Loss: 0.2433\n",
      "\n",
      " Test Loss 0.2656460106372833\n",
      "\n",
      "Epoch: 624 Train Loss: 0.2430\n",
      "\n",
      " Test Loss 0.26535147428512573\n",
      "\n",
      "Epoch: 625 Train Loss: 0.2427\n",
      "\n",
      " Test Loss 0.2650574743747711\n",
      "\n",
      "Epoch: 626 Train Loss: 0.2424\n",
      "\n",
      " Test Loss 0.2647639811038971\n",
      "\n",
      "Epoch: 627 Train Loss: 0.2421\n",
      "\n",
      " Test Loss 0.2644711136817932\n",
      "\n",
      "Epoch: 628 Train Loss: 0.2418\n",
      "\n",
      " Test Loss 0.2641788423061371\n",
      "\n",
      "Epoch: 629 Train Loss: 0.2416\n",
      "\n",
      " Test Loss 0.2638871371746063\n",
      "\n",
      "Epoch: 630 Train Loss: 0.2413\n",
      "\n",
      " Test Loss 0.26359593868255615\n",
      "\n",
      "Epoch: 631 Train Loss: 0.2410\n",
      "\n",
      " Test Loss 0.2633053958415985\n",
      "\n",
      "Epoch: 632 Train Loss: 0.2407\n",
      "\n",
      " Test Loss 0.26301538944244385\n",
      "\n",
      "Epoch: 633 Train Loss: 0.2404\n",
      "\n",
      " Test Loss 0.26272594928741455\n",
      "\n",
      "Epoch: 634 Train Loss: 0.2401\n",
      "\n",
      " Test Loss 0.2624371349811554\n",
      "\n",
      "Epoch: 635 Train Loss: 0.2398\n",
      "\n",
      " Test Loss 0.262148916721344\n",
      "\n",
      "Epoch: 636 Train Loss: 0.2396\n",
      "\n",
      " Test Loss 0.26186126470565796\n",
      "\n",
      "Epoch: 637 Train Loss: 0.2393\n",
      "\n",
      " Test Loss 0.26157423853874207\n",
      "\n",
      "Epoch: 638 Train Loss: 0.2390\n",
      "\n",
      " Test Loss 0.26128777861595154\n",
      "\n",
      "Epoch: 639 Train Loss: 0.2387\n",
      "\n",
      " Test Loss 0.26100194454193115\n",
      "\n",
      "Epoch: 640 Train Loss: 0.2384\n",
      "\n",
      " Test Loss 0.2607167065143585\n",
      "\n",
      "Epoch: 641 Train Loss: 0.2382\n",
      "\n",
      " Test Loss 0.26043206453323364\n",
      "\n",
      "Epoch: 642 Train Loss: 0.2379\n",
      "\n",
      " Test Loss 0.2601480484008789\n",
      "\n",
      "Epoch: 643 Train Loss: 0.2376\n",
      "\n",
      " Test Loss 0.2598646283149719\n",
      "\n",
      "Epoch: 644 Train Loss: 0.2373\n",
      "\n",
      " Test Loss 0.2595818042755127\n",
      "\n",
      "Epoch: 645 Train Loss: 0.2370\n",
      "\n",
      " Test Loss 0.259299635887146\n",
      "\n",
      "Epoch: 646 Train Loss: 0.2368\n",
      "\n",
      " Test Loss 0.25901806354522705\n",
      "\n",
      "Epoch: 647 Train Loss: 0.2365\n",
      "\n",
      " Test Loss 0.25873708724975586\n",
      "\n",
      "Epoch: 648 Train Loss: 0.2362\n",
      "\n",
      " Test Loss 0.2584567964076996\n",
      "\n",
      "Epoch: 649 Train Loss: 0.2359\n",
      "\n",
      " Test Loss 0.2581770718097687\n",
      "\n",
      "Epoch: 650 Train Loss: 0.2357\n",
      "\n",
      " Test Loss 0.2578980326652527\n",
      "\n",
      "Epoch: 651 Train Loss: 0.2354\n",
      "\n",
      " Test Loss 0.25761958956718445\n",
      "\n",
      "Epoch: 652 Train Loss: 0.2351\n",
      "\n",
      " Test Loss 0.25734183192253113\n",
      "\n",
      "Epoch: 653 Train Loss: 0.2348\n",
      "\n",
      " Test Loss 0.2570646405220032\n",
      "\n",
      "Epoch: 654 Train Loss: 0.2346\n",
      "\n",
      " Test Loss 0.25678810477256775\n",
      "\n",
      "Epoch: 655 Train Loss: 0.2343\n",
      "\n",
      " Test Loss 0.25651222467422485\n",
      "\n",
      "Epoch: 656 Train Loss: 0.2340\n",
      "\n",
      " Test Loss 0.2562370002269745\n",
      "\n",
      "Epoch: 657 Train Loss: 0.2338\n",
      "\n",
      " Test Loss 0.25596243143081665\n",
      "\n",
      "Epoch: 658 Train Loss: 0.2335\n",
      "\n",
      " Test Loss 0.25568845868110657\n",
      "\n",
      "Epoch: 659 Train Loss: 0.2332\n",
      "\n",
      " Test Loss 0.25541526079177856\n",
      "\n",
      "Epoch: 660 Train Loss: 0.2329\n",
      "\n",
      " Test Loss 0.25514256954193115\n",
      "\n",
      "Epoch: 661 Train Loss: 0.2327\n",
      "\n",
      " Test Loss 0.25487062335014343\n",
      "\n",
      "Epoch: 662 Train Loss: 0.2324\n",
      "\n",
      " Test Loss 0.25459936261177063\n",
      "\n",
      "Epoch: 663 Train Loss: 0.2321\n",
      "\n",
      " Test Loss 0.2543286979198456\n",
      "\n",
      "Epoch: 664 Train Loss: 0.2319\n",
      "\n",
      " Test Loss 0.25405871868133545\n",
      "\n",
      "Epoch: 665 Train Loss: 0.2316\n",
      "\n",
      " Test Loss 0.25378936529159546\n",
      "\n",
      "Epoch: 666 Train Loss: 0.2314\n",
      "\n",
      " Test Loss 0.2535206973552704\n",
      "\n",
      "Epoch: 667 Train Loss: 0.2311\n",
      "\n",
      " Test Loss 0.253252774477005\n",
      "\n",
      "Epoch: 668 Train Loss: 0.2308\n",
      "\n",
      " Test Loss 0.2529854476451874\n",
      "\n",
      "Epoch: 669 Train Loss: 0.2306\n",
      "\n",
      " Test Loss 0.25271886587142944\n",
      "\n",
      "Epoch: 670 Train Loss: 0.2303\n",
      "\n",
      " Test Loss 0.2524529695510864\n",
      "\n",
      "Epoch: 671 Train Loss: 0.2300\n",
      "\n",
      " Test Loss 0.25218769907951355\n",
      "\n",
      "Epoch: 672 Train Loss: 0.2298\n",
      "\n",
      " Test Loss 0.25192317366600037\n",
      "\n",
      "Epoch: 673 Train Loss: 0.2295\n",
      "\n",
      " Test Loss 0.2516593039035797\n",
      "\n",
      "Epoch: 674 Train Loss: 0.2293\n",
      "\n",
      " Test Loss 0.25139614939689636\n",
      "\n",
      "Epoch: 675 Train Loss: 0.2290\n",
      "\n",
      " Test Loss 0.25113368034362793\n",
      "\n",
      "Epoch: 676 Train Loss: 0.2287\n",
      "\n",
      " Test Loss 0.2508718967437744\n",
      "\n",
      "Epoch: 677 Train Loss: 0.2285\n",
      "\n",
      " Test Loss 0.2506107687950134\n",
      "\n",
      "Epoch: 678 Train Loss: 0.2282\n",
      "\n",
      " Test Loss 0.2503504455089569\n",
      "\n",
      "Epoch: 679 Train Loss: 0.2280\n",
      "\n",
      " Test Loss 0.25009074807167053\n",
      "\n",
      "Epoch: 680 Train Loss: 0.2277\n",
      "\n",
      " Test Loss 0.24983179569244385\n",
      "\n",
      "Epoch: 681 Train Loss: 0.2275\n",
      "\n",
      " Test Loss 0.24957354366779327\n",
      "\n",
      "Epoch: 682 Train Loss: 0.2272\n",
      "\n",
      " Test Loss 0.24931600689888\n",
      "\n",
      "Epoch: 683 Train Loss: 0.2270\n",
      "\n",
      " Test Loss 0.24905917048454285\n",
      "\n",
      "Epoch: 684 Train Loss: 0.2267\n",
      "\n",
      " Test Loss 0.24880309402942657\n",
      "\n",
      "Epoch: 685 Train Loss: 0.2265\n",
      "\n",
      " Test Loss 0.24854770302772522\n",
      "\n",
      "Epoch: 686 Train Loss: 0.2262\n",
      "\n",
      " Test Loss 0.24829307198524475\n",
      "\n",
      "Epoch: 687 Train Loss: 0.2260\n",
      "\n",
      " Test Loss 0.2480391412973404\n",
      "\n",
      "Epoch: 688 Train Loss: 0.2257\n",
      "\n",
      " Test Loss 0.24778598546981812\n",
      "\n",
      "Epoch: 689 Train Loss: 0.2255\n",
      "\n",
      " Test Loss 0.24753348529338837\n",
      "\n",
      "Epoch: 690 Train Loss: 0.2252\n",
      "\n",
      " Test Loss 0.24728178977966309\n",
      "\n",
      "Epoch: 691 Train Loss: 0.2250\n",
      "\n",
      " Test Loss 0.24703074991703033\n",
      "\n",
      "Epoch: 692 Train Loss: 0.2247\n",
      "\n",
      " Test Loss 0.24678049981594086\n",
      "\n",
      "Epoch: 693 Train Loss: 0.2245\n",
      "\n",
      " Test Loss 0.24653096497058868\n",
      "\n",
      "Epoch: 694 Train Loss: 0.2243\n",
      "\n",
      " Test Loss 0.2462821900844574\n",
      "\n",
      "Epoch: 695 Train Loss: 0.2240\n",
      "\n",
      " Test Loss 0.2460341900587082\n",
      "\n",
      "Epoch: 696 Train Loss: 0.2238\n",
      "\n",
      " Test Loss 0.2457869052886963\n",
      "\n",
      "Epoch: 697 Train Loss: 0.2235\n",
      "\n",
      " Test Loss 0.24554036557674408\n",
      "\n",
      "Epoch: 698 Train Loss: 0.2233\n",
      "\n",
      " Test Loss 0.24529458582401276\n",
      "\n",
      "Epoch: 699 Train Loss: 0.2230\n",
      "\n",
      " Test Loss 0.24504955112934113\n",
      "\n",
      "Epoch: 700 Train Loss: 0.2228\n",
      "\n",
      " Test Loss 0.24480529129505157\n",
      "\n",
      "Epoch: 701 Train Loss: 0.2226\n",
      "\n",
      " Test Loss 0.24456177651882172\n",
      "\n",
      "Epoch: 702 Train Loss: 0.2223\n",
      "\n",
      " Test Loss 0.24431903660297394\n",
      "\n",
      "Epoch: 703 Train Loss: 0.2221\n",
      "\n",
      " Test Loss 0.24407710134983063\n",
      "\n",
      "Epoch: 704 Train Loss: 0.2219\n",
      "\n",
      " Test Loss 0.24383589625358582\n",
      "\n",
      "Epoch: 705 Train Loss: 0.2216\n",
      "\n",
      " Test Loss 0.24359539151191711\n",
      "\n",
      "Epoch: 706 Train Loss: 0.2214\n",
      "\n",
      " Test Loss 0.24335575103759766\n",
      "\n",
      "Epoch: 707 Train Loss: 0.2212\n",
      "\n",
      " Test Loss 0.24311688542366028\n",
      "\n",
      "Epoch: 708 Train Loss: 0.2209\n",
      "\n",
      " Test Loss 0.2428787648677826\n",
      "\n",
      "Epoch: 709 Train Loss: 0.2207\n",
      "\n",
      " Test Loss 0.2426413893699646\n",
      "\n",
      "Epoch: 710 Train Loss: 0.2205\n",
      "\n",
      " Test Loss 0.24240489304065704\n",
      "\n",
      "Epoch: 711 Train Loss: 0.2202\n",
      "\n",
      " Test Loss 0.24216905236244202\n",
      "\n",
      "Epoch: 712 Train Loss: 0.2200\n",
      "\n",
      " Test Loss 0.24193406105041504\n",
      "\n",
      "Epoch: 713 Train Loss: 0.2198\n",
      "\n",
      " Test Loss 0.24169984459877014\n",
      "\n",
      "Epoch: 714 Train Loss: 0.2196\n",
      "\n",
      " Test Loss 0.24146640300750732\n",
      "\n",
      "Epoch: 715 Train Loss: 0.2193\n",
      "\n",
      " Test Loss 0.24123378098011017\n",
      "\n",
      "Epoch: 716 Train Loss: 0.2191\n",
      "\n",
      " Test Loss 0.2410019338130951\n",
      "\n",
      "Epoch: 717 Train Loss: 0.2189\n",
      "\n",
      " Test Loss 0.24077089130878448\n",
      "\n",
      "Epoch: 718 Train Loss: 0.2187\n",
      "\n",
      " Test Loss 0.24054059386253357\n",
      "\n",
      "Epoch: 719 Train Loss: 0.2184\n",
      "\n",
      " Test Loss 0.2403111457824707\n",
      "\n",
      "Epoch: 720 Train Loss: 0.2182\n",
      "\n",
      " Test Loss 0.24008241295814514\n",
      "\n",
      "Epoch: 721 Train Loss: 0.2180\n",
      "\n",
      " Test Loss 0.23985454440116882\n",
      "\n",
      "Epoch: 722 Train Loss: 0.2178\n",
      "\n",
      " Test Loss 0.23962745070457458\n",
      "\n",
      "Epoch: 723 Train Loss: 0.2175\n",
      "\n",
      " Test Loss 0.239401176571846\n",
      "\n",
      "Epoch: 724 Train Loss: 0.2173\n",
      "\n",
      " Test Loss 0.2391756772994995\n",
      "\n",
      "Epoch: 725 Train Loss: 0.2171\n",
      "\n",
      " Test Loss 0.23895099759101868\n",
      "\n",
      "Epoch: 726 Train Loss: 0.2169\n",
      "\n",
      " Test Loss 0.2387271374464035\n",
      "\n",
      "Epoch: 727 Train Loss: 0.2167\n",
      "\n",
      " Test Loss 0.23850403726100922\n",
      "\n",
      "Epoch: 728 Train Loss: 0.2165\n",
      "\n",
      " Test Loss 0.23828183114528656\n",
      "\n",
      "Epoch: 729 Train Loss: 0.2162\n",
      "\n",
      " Test Loss 0.23806039988994598\n",
      "\n",
      "Epoch: 730 Train Loss: 0.2160\n",
      "\n",
      " Test Loss 0.2378397285938263\n",
      "\n",
      "Epoch: 731 Train Loss: 0.2158\n",
      "\n",
      " Test Loss 0.23761995136737823\n",
      "\n",
      "Epoch: 732 Train Loss: 0.2156\n",
      "\n",
      " Test Loss 0.23740094900131226\n",
      "\n",
      "Epoch: 733 Train Loss: 0.2154\n",
      "\n",
      " Test Loss 0.23718275129795074\n",
      "\n",
      "Epoch: 734 Train Loss: 0.2152\n",
      "\n",
      " Test Loss 0.2369653582572937\n",
      "\n",
      "Epoch: 735 Train Loss: 0.2150\n",
      "\n",
      " Test Loss 0.2367488592863083\n",
      "\n",
      "Epoch: 736 Train Loss: 0.2148\n",
      "\n",
      " Test Loss 0.23653309047222137\n",
      "\n",
      "Epoch: 737 Train Loss: 0.2146\n",
      "\n",
      " Test Loss 0.2363182008266449\n",
      "\n",
      "Epoch: 738 Train Loss: 0.2144\n",
      "\n",
      " Test Loss 0.23610414564609528\n",
      "\n",
      "Epoch: 739 Train Loss: 0.2141\n",
      "\n",
      " Test Loss 0.23589085042476654\n",
      "\n",
      "Epoch: 740 Train Loss: 0.2139\n",
      "\n",
      " Test Loss 0.23567841947078705\n",
      "\n",
      "Epoch: 741 Train Loss: 0.2137\n",
      "\n",
      " Test Loss 0.2354668527841568\n",
      "\n",
      "Epoch: 742 Train Loss: 0.2135\n",
      "\n",
      " Test Loss 0.23525607585906982\n",
      "\n",
      "Epoch: 743 Train Loss: 0.2133\n",
      "\n",
      " Test Loss 0.2350461632013321\n",
      "\n",
      "Epoch: 744 Train Loss: 0.2131\n",
      "\n",
      " Test Loss 0.23483699560165405\n",
      "\n",
      "Epoch: 745 Train Loss: 0.2129\n",
      "\n",
      " Test Loss 0.23462872207164764\n",
      "\n",
      "Epoch: 746 Train Loss: 0.2127\n",
      "\n",
      " Test Loss 0.2344212532043457\n",
      "\n",
      "Epoch: 747 Train Loss: 0.2125\n",
      "\n",
      " Test Loss 0.2342146635055542\n",
      "\n",
      "Epoch: 748 Train Loss: 0.2123\n",
      "\n",
      " Test Loss 0.23400886356830597\n",
      "\n",
      "Epoch: 749 Train Loss: 0.2121\n",
      "\n",
      " Test Loss 0.23380394279956818\n",
      "\n",
      "Epoch: 750 Train Loss: 0.2119\n",
      "\n",
      " Test Loss 0.23359979689121246\n",
      "\n",
      "Epoch: 751 Train Loss: 0.2117\n",
      "\n",
      " Test Loss 0.2333964854478836\n",
      "\n",
      "Epoch: 752 Train Loss: 0.2116\n",
      "\n",
      " Test Loss 0.2331940233707428\n",
      "\n",
      "Epoch: 753 Train Loss: 0.2114\n",
      "\n",
      " Test Loss 0.23299239575862885\n",
      "\n",
      "Epoch: 754 Train Loss: 0.2112\n",
      "\n",
      " Test Loss 0.23279163241386414\n",
      "\n",
      "Epoch: 755 Train Loss: 0.2110\n",
      "\n",
      " Test Loss 0.2325916588306427\n",
      "\n",
      "Epoch: 756 Train Loss: 0.2108\n",
      "\n",
      " Test Loss 0.23239251971244812\n",
      "\n",
      "Epoch: 757 Train Loss: 0.2106\n",
      "\n",
      " Test Loss 0.23219427466392517\n",
      "\n",
      "Epoch: 758 Train Loss: 0.2104\n",
      "\n",
      " Test Loss 0.23199677467346191\n",
      "\n",
      "Epoch: 759 Train Loss: 0.2102\n",
      "\n",
      " Test Loss 0.23180021345615387\n",
      "\n",
      "Epoch: 760 Train Loss: 0.2100\n",
      "\n",
      " Test Loss 0.2316044718027115\n",
      "\n",
      "Epoch: 761 Train Loss: 0.2098\n",
      "\n",
      " Test Loss 0.23140959441661835\n",
      "\n",
      "Epoch: 762 Train Loss: 0.2097\n",
      "\n",
      " Test Loss 0.23121552169322968\n",
      "\n",
      "Epoch: 763 Train Loss: 0.2095\n",
      "\n",
      " Test Loss 0.23102231323719025\n",
      "\n",
      "Epoch: 764 Train Loss: 0.2093\n",
      "\n",
      " Test Loss 0.23082992434501648\n",
      "\n",
      "Epoch: 765 Train Loss: 0.2091\n",
      "\n",
      " Test Loss 0.23063839972019196\n",
      "\n",
      "Epoch: 766 Train Loss: 0.2089\n",
      "\n",
      " Test Loss 0.23044775426387787\n",
      "\n",
      "Epoch: 767 Train Loss: 0.2087\n",
      "\n",
      " Test Loss 0.2302578240633011\n",
      "\n",
      "Epoch: 768 Train Loss: 0.2086\n",
      "\n",
      " Test Loss 0.23006875813007355\n",
      "\n",
      "Epoch: 769 Train Loss: 0.2084\n",
      "\n",
      " Test Loss 0.22988055646419525\n",
      "\n",
      "Epoch: 770 Train Loss: 0.2082\n",
      "\n",
      " Test Loss 0.2296932488679886\n",
      "\n",
      "Epoch: 771 Train Loss: 0.2080\n",
      "\n",
      " Test Loss 0.22950679063796997\n",
      "\n",
      "Epoch: 772 Train Loss: 0.2079\n",
      "\n",
      " Test Loss 0.22932104766368866\n",
      "\n",
      "Epoch: 773 Train Loss: 0.2077\n",
      "\n",
      " Test Loss 0.22913622856140137\n",
      "\n",
      "Epoch: 774 Train Loss: 0.2075\n",
      "\n",
      " Test Loss 0.22895222902297974\n",
      "\n",
      "Epoch: 775 Train Loss: 0.2073\n",
      "\n",
      " Test Loss 0.22876906394958496\n",
      "\n",
      "Epoch: 776 Train Loss: 0.2072\n",
      "\n",
      " Test Loss 0.22858677804470062\n",
      "\n",
      "Epoch: 777 Train Loss: 0.2070\n",
      "\n",
      " Test Loss 0.22840529680252075\n",
      "\n",
      "Epoch: 778 Train Loss: 0.2068\n",
      "\n",
      " Test Loss 0.22822466492652893\n",
      "\n",
      "Epoch: 779 Train Loss: 0.2067\n",
      "\n",
      " Test Loss 0.22804488241672516\n",
      "\n",
      "Epoch: 780 Train Loss: 0.2065\n",
      "\n",
      " Test Loss 0.22786593437194824\n",
      "\n",
      "Epoch: 781 Train Loss: 0.2063\n",
      "\n",
      " Test Loss 0.22768786549568176\n",
      "\n",
      "Epoch: 782 Train Loss: 0.2061\n",
      "\n",
      " Test Loss 0.22751057147979736\n",
      "\n",
      "Epoch: 783 Train Loss: 0.2060\n",
      "\n",
      " Test Loss 0.2273341715335846\n",
      "\n",
      "Epoch: 784 Train Loss: 0.2058\n",
      "\n",
      " Test Loss 0.2271585315465927\n",
      "\n",
      "Epoch: 785 Train Loss: 0.2057\n",
      "\n",
      " Test Loss 0.22698374092578888\n",
      "\n",
      "Epoch: 786 Train Loss: 0.2055\n",
      "\n",
      " Test Loss 0.22680982947349548\n",
      "\n",
      "Epoch: 787 Train Loss: 0.2053\n",
      "\n",
      " Test Loss 0.22663672268390656\n",
      "\n",
      "Epoch: 788 Train Loss: 0.2052\n",
      "\n",
      " Test Loss 0.2264644205570221\n",
      "\n",
      "Epoch: 789 Train Loss: 0.2050\n",
      "\n",
      " Test Loss 0.22629301249980927\n",
      "\n",
      "Epoch: 790 Train Loss: 0.2048\n",
      "\n",
      " Test Loss 0.2261223942041397\n",
      "\n",
      "Epoch: 791 Train Loss: 0.2047\n",
      "\n",
      " Test Loss 0.225952610373497\n",
      "\n",
      "Epoch: 792 Train Loss: 0.2045\n",
      "\n",
      " Test Loss 0.22578364610671997\n",
      "\n",
      "Epoch: 793 Train Loss: 0.2044\n",
      "\n",
      " Test Loss 0.2256155163049698\n",
      "\n",
      "Epoch: 794 Train Loss: 0.2042\n",
      "\n",
      " Test Loss 0.22544819116592407\n",
      "\n",
      "Epoch: 795 Train Loss: 0.2041\n",
      "\n",
      " Test Loss 0.22528168559074402\n",
      "\n",
      "Epoch: 796 Train Loss: 0.2039\n",
      "\n",
      " Test Loss 0.22511596977710724\n",
      "\n",
      "Epoch: 797 Train Loss: 0.2037\n",
      "\n",
      " Test Loss 0.2249511331319809\n",
      "\n",
      "Epoch: 798 Train Loss: 0.2036\n",
      "\n",
      " Test Loss 0.22478702664375305\n",
      "\n",
      "Epoch: 799 Train Loss: 0.2034\n",
      "\n",
      " Test Loss 0.22462373971939087\n",
      "\n",
      "Epoch: 800 Train Loss: 0.2033\n",
      "\n",
      " Test Loss 0.22446130216121674\n",
      "\n",
      "Epoch: 801 Train Loss: 0.2031\n",
      "\n",
      " Test Loss 0.22429965436458588\n",
      "\n",
      "Epoch: 802 Train Loss: 0.2030\n",
      "\n",
      " Test Loss 0.22413884103298187\n",
      "\n",
      "Epoch: 803 Train Loss: 0.2028\n",
      "\n",
      " Test Loss 0.22397886216640472\n",
      "\n",
      "Epoch: 804 Train Loss: 0.2027\n",
      "\n",
      " Test Loss 0.22381968796253204\n",
      "\n",
      "Epoch: 805 Train Loss: 0.2026\n",
      "\n",
      " Test Loss 0.22366128861904144\n",
      "\n",
      "Epoch: 806 Train Loss: 0.2024\n",
      "\n",
      " Test Loss 0.2235036939382553\n",
      "\n",
      "Epoch: 807 Train Loss: 0.2023\n",
      "\n",
      " Test Loss 0.22334697842597961\n",
      "\n",
      "Epoch: 808 Train Loss: 0.2021\n",
      "\n",
      " Test Loss 0.22319093346595764\n",
      "\n",
      "Epoch: 809 Train Loss: 0.2020\n",
      "\n",
      " Test Loss 0.2230357676744461\n",
      "\n",
      "Epoch: 810 Train Loss: 0.2018\n",
      "\n",
      " Test Loss 0.22288139164447784\n",
      "\n",
      "Epoch: 811 Train Loss: 0.2017\n",
      "\n",
      " Test Loss 0.22272777557373047\n",
      "\n",
      "Epoch: 812 Train Loss: 0.2015\n",
      "\n",
      " Test Loss 0.22257502377033234\n",
      "\n",
      "Epoch: 813 Train Loss: 0.2014\n",
      "\n",
      " Test Loss 0.2224230319261551\n",
      "\n",
      "Epoch: 814 Train Loss: 0.2013\n",
      "\n",
      " Test Loss 0.2222718447446823\n",
      "\n",
      "Epoch: 815 Train Loss: 0.2011\n",
      "\n",
      " Test Loss 0.2221214473247528\n",
      "\n",
      "Epoch: 816 Train Loss: 0.2010\n",
      "\n",
      " Test Loss 0.22197182476520538\n",
      "\n",
      "Epoch: 817 Train Loss: 0.2009\n",
      "\n",
      " Test Loss 0.22182302176952362\n",
      "\n",
      "Epoch: 818 Train Loss: 0.2007\n",
      "\n",
      " Test Loss 0.22167503833770752\n",
      "\n",
      "Epoch: 819 Train Loss: 0.2006\n",
      "\n",
      " Test Loss 0.22152778506278992\n",
      "\n",
      "Epoch: 820 Train Loss: 0.2004\n",
      "\n",
      " Test Loss 0.2213813215494156\n",
      "\n",
      "Epoch: 821 Train Loss: 0.2003\n",
      "\n",
      " Test Loss 0.22123563289642334\n",
      "\n",
      "Epoch: 822 Train Loss: 0.2002\n",
      "\n",
      " Test Loss 0.22109073400497437\n",
      "\n",
      "Epoch: 823 Train Loss: 0.2001\n",
      "\n",
      " Test Loss 0.22094660997390747\n",
      "\n",
      "Epoch: 824 Train Loss: 0.1999\n",
      "\n",
      " Test Loss 0.22080326080322266\n",
      "\n",
      "Epoch: 825 Train Loss: 0.1998\n",
      "\n",
      " Test Loss 0.22066070139408112\n",
      "\n",
      "Epoch: 826 Train Loss: 0.1997\n",
      "\n",
      " Test Loss 0.22051884233951569\n",
      "\n",
      "Epoch: 827 Train Loss: 0.1995\n",
      "\n",
      " Test Loss 0.2203778177499771\n",
      "\n",
      "Epoch: 828 Train Loss: 0.1994\n",
      "\n",
      " Test Loss 0.22023747861385345\n",
      "\n",
      "Epoch: 829 Train Loss: 0.1993\n",
      "\n",
      " Test Loss 0.22009795904159546\n",
      "\n",
      "Epoch: 830 Train Loss: 0.1992\n",
      "\n",
      " Test Loss 0.21995916962623596\n",
      "\n",
      "Epoch: 831 Train Loss: 0.1990\n",
      "\n",
      " Test Loss 0.21982118487358093\n",
      "\n",
      "Epoch: 832 Train Loss: 0.1989\n",
      "\n",
      " Test Loss 0.21968387067317963\n",
      "\n",
      "Epoch: 833 Train Loss: 0.1988\n",
      "\n",
      " Test Loss 0.21954742074012756\n",
      "\n",
      "Epoch: 834 Train Loss: 0.1987\n",
      "\n",
      " Test Loss 0.21941162645816803\n",
      "\n",
      "Epoch: 835 Train Loss: 0.1985\n",
      "\n",
      " Test Loss 0.21927660703659058\n",
      "\n",
      "Epoch: 836 Train Loss: 0.1984\n",
      "\n",
      " Test Loss 0.21914231777191162\n",
      "\n",
      "Epoch: 837 Train Loss: 0.1983\n",
      "\n",
      " Test Loss 0.21900875866413116\n",
      "\n",
      "Epoch: 838 Train Loss: 0.1982\n",
      "\n",
      " Test Loss 0.2188759446144104\n",
      "\n",
      "Epoch: 839 Train Loss: 0.1981\n",
      "\n",
      " Test Loss 0.21874387562274933\n",
      "\n",
      "Epoch: 840 Train Loss: 0.1979\n",
      "\n",
      " Test Loss 0.21861256659030914\n",
      "\n",
      "Epoch: 841 Train Loss: 0.1978\n",
      "\n",
      " Test Loss 0.2184819132089615\n",
      "\n",
      "Epoch: 842 Train Loss: 0.1977\n",
      "\n",
      " Test Loss 0.21835200488567352\n",
      "\n",
      "Epoch: 843 Train Loss: 0.1976\n",
      "\n",
      " Test Loss 0.21822278201580048\n",
      "\n",
      "Epoch: 844 Train Loss: 0.1975\n",
      "\n",
      " Test Loss 0.21809417009353638\n",
      "\n",
      "Epoch: 845 Train Loss: 0.1974\n",
      "\n",
      " Test Loss 0.21796636283397675\n",
      "\n",
      "Epoch: 846 Train Loss: 0.1972\n",
      "\n",
      " Test Loss 0.21783927083015442\n",
      "\n",
      "Epoch: 847 Train Loss: 0.1971\n",
      "\n",
      " Test Loss 0.2177128791809082\n",
      "\n",
      "Epoch: 848 Train Loss: 0.1970\n",
      "\n",
      " Test Loss 0.2175871729850769\n",
      "\n",
      "Epoch: 849 Train Loss: 0.1969\n",
      "\n",
      " Test Loss 0.2174621969461441\n",
      "\n",
      "Epoch: 850 Train Loss: 0.1968\n",
      "\n",
      " Test Loss 0.2173379361629486\n",
      "\n",
      "Epoch: 851 Train Loss: 0.1967\n",
      "\n",
      " Test Loss 0.21721433103084564\n",
      "\n",
      "Epoch: 852 Train Loss: 0.1966\n",
      "\n",
      " Test Loss 0.21709144115447998\n",
      "\n",
      "Epoch: 853 Train Loss: 0.1965\n",
      "\n",
      " Test Loss 0.21696928143501282\n",
      "\n",
      "Epoch: 854 Train Loss: 0.1963\n",
      "\n",
      " Test Loss 0.2168477177619934\n",
      "\n",
      "Epoch: 855 Train Loss: 0.1962\n",
      "\n",
      " Test Loss 0.2167268842458725\n",
      "\n",
      "Epoch: 856 Train Loss: 0.1961\n",
      "\n",
      " Test Loss 0.21660670638084412\n",
      "\n",
      "Epoch: 857 Train Loss: 0.1960\n",
      "\n",
      " Test Loss 0.21648725867271423\n",
      "\n",
      "Epoch: 858 Train Loss: 0.1959\n",
      "\n",
      " Test Loss 0.21636828780174255\n",
      "\n",
      "Epoch: 859 Train Loss: 0.1958\n",
      "\n",
      " Test Loss 0.2162499576807022\n",
      "\n",
      "Epoch: 860 Train Loss: 0.1957\n",
      "\n",
      " Test Loss 0.2161322981119156\n",
      "\n",
      "Epoch: 861 Train Loss: 0.1956\n",
      "\n",
      " Test Loss 0.21601538360118866\n",
      "\n",
      "Epoch: 862 Train Loss: 0.1955\n",
      "\n",
      " Test Loss 0.21589908003807068\n",
      "\n",
      "Epoch: 863 Train Loss: 0.1954\n",
      "\n",
      " Test Loss 0.21578344702720642\n",
      "\n",
      "Epoch: 864 Train Loss: 0.1953\n",
      "\n",
      " Test Loss 0.2156684398651123\n",
      "\n",
      "Epoch: 865 Train Loss: 0.1952\n",
      "\n",
      " Test Loss 0.2155541181564331\n",
      "\n",
      "Epoch: 866 Train Loss: 0.1951\n",
      "\n",
      " Test Loss 0.21544042229652405\n",
      "\n",
      "Epoch: 867 Train Loss: 0.1950\n",
      "\n",
      " Test Loss 0.21532736718654633\n",
      "\n",
      "Epoch: 868 Train Loss: 0.1949\n",
      "\n",
      " Test Loss 0.21521490812301636\n",
      "\n",
      "Epoch: 869 Train Loss: 0.1948\n",
      "\n",
      " Test Loss 0.2151031345129013\n",
      "\n",
      "Epoch: 870 Train Loss: 0.1947\n",
      "\n",
      " Test Loss 0.2149919867515564\n",
      "\n",
      "Epoch: 871 Train Loss: 0.1946\n",
      "\n",
      " Test Loss 0.21488121151924133\n",
      "\n",
      "Epoch: 872 Train Loss: 0.1945\n",
      "\n",
      " Test Loss 0.21477110683918\n",
      "\n",
      "Epoch: 873 Train Loss: 0.1944\n",
      "\n",
      " Test Loss 0.21466165781021118\n",
      "\n",
      "Epoch: 874 Train Loss: 0.1943\n",
      "\n",
      " Test Loss 0.21455277502536774\n",
      "\n",
      "Epoch: 875 Train Loss: 0.1942\n",
      "\n",
      " Test Loss 0.21444450318813324\n",
      "\n",
      "Epoch: 876 Train Loss: 0.1941\n",
      "\n",
      " Test Loss 0.2143368422985077\n",
      "\n",
      "Epoch: 877 Train Loss: 0.1940\n",
      "\n",
      " Test Loss 0.2142297625541687\n",
      "\n",
      "Epoch: 878 Train Loss: 0.1939\n",
      "\n",
      " Test Loss 0.21412335336208344\n",
      "\n",
      "Epoch: 879 Train Loss: 0.1938\n",
      "\n",
      " Test Loss 0.2140173465013504\n",
      "\n",
      "Epoch: 880 Train Loss: 0.1938\n",
      "\n",
      " Test Loss 0.21391189098358154\n",
      "\n",
      "Epoch: 881 Train Loss: 0.1937\n",
      "\n",
      " Test Loss 0.21380706131458282\n",
      "\n",
      "Epoch: 882 Train Loss: 0.1936\n",
      "\n",
      " Test Loss 0.21370282769203186\n",
      "\n",
      "Epoch: 883 Train Loss: 0.1935\n",
      "\n",
      " Test Loss 0.21359916031360626\n",
      "\n",
      "Epoch: 884 Train Loss: 0.1934\n",
      "\n",
      " Test Loss 0.21349604427814484\n",
      "\n",
      "Epoch: 885 Train Loss: 0.1933\n",
      "\n",
      " Test Loss 0.21339352428913116\n",
      "\n",
      "Epoch: 886 Train Loss: 0.1932\n",
      "\n",
      " Test Loss 0.21329161524772644\n",
      "\n",
      "Epoch: 887 Train Loss: 0.1931\n",
      "\n",
      " Test Loss 0.2131902426481247\n",
      "\n",
      "Epoch: 888 Train Loss: 0.1930\n",
      "\n",
      " Test Loss 0.21308943629264832\n",
      "\n",
      "Epoch: 889 Train Loss: 0.1929\n",
      "\n",
      " Test Loss 0.21298916637897491\n",
      "\n",
      "Epoch: 890 Train Loss: 0.1929\n",
      "\n",
      " Test Loss 0.2128894329071045\n",
      "\n",
      "Epoch: 891 Train Loss: 0.1928\n",
      "\n",
      " Test Loss 0.21279031038284302\n",
      "\n",
      "Epoch: 892 Train Loss: 0.1927\n",
      "\n",
      " Test Loss 0.21269169449806213\n",
      "\n",
      "Epoch: 893 Train Loss: 0.1926\n",
      "\n",
      " Test Loss 0.21259364485740662\n",
      "\n",
      "Epoch: 894 Train Loss: 0.1925\n",
      "\n",
      " Test Loss 0.21249616146087646\n",
      "\n",
      "Epoch: 895 Train Loss: 0.1924\n",
      "\n",
      " Test Loss 0.2123991996049881\n",
      "\n",
      "Epoch: 896 Train Loss: 0.1923\n",
      "\n",
      " Test Loss 0.21230272948741913\n",
      "\n",
      "Epoch: 897 Train Loss: 0.1923\n",
      "\n",
      " Test Loss 0.21220678091049194\n",
      "\n",
      "Epoch: 898 Train Loss: 0.1922\n",
      "\n",
      " Test Loss 0.21211138367652893\n",
      "\n",
      "Epoch: 899 Train Loss: 0.1921\n",
      "\n",
      " Test Loss 0.2120165079832077\n",
      "\n",
      "Epoch: 900 Train Loss: 0.1920\n",
      "\n",
      " Test Loss 0.21192215383052826\n",
      "\n",
      "Epoch: 901 Train Loss: 0.1919\n",
      "\n",
      " Test Loss 0.2118282914161682\n",
      "\n",
      "Epoch: 902 Train Loss: 0.1918\n",
      "\n",
      " Test Loss 0.21173496544361115\n",
      "\n",
      "Epoch: 903 Train Loss: 0.1918\n",
      "\n",
      " Test Loss 0.21164217591285706\n",
      "\n",
      "Epoch: 904 Train Loss: 0.1917\n",
      "\n",
      " Test Loss 0.21154983341693878\n",
      "\n",
      "Epoch: 905 Train Loss: 0.1916\n",
      "\n",
      " Test Loss 0.2114579975605011\n",
      "\n",
      "Epoch: 906 Train Loss: 0.1915\n",
      "\n",
      " Test Loss 0.2113666832447052\n",
      "\n",
      "Epoch: 907 Train Loss: 0.1914\n",
      "\n",
      " Test Loss 0.21127577126026154\n",
      "\n",
      "Epoch: 908 Train Loss: 0.1914\n",
      "\n",
      " Test Loss 0.21118541061878204\n",
      "\n",
      "Epoch: 909 Train Loss: 0.1913\n",
      "\n",
      " Test Loss 0.21109552681446075\n",
      "\n",
      "Epoch: 910 Train Loss: 0.1912\n",
      "\n",
      " Test Loss 0.2110060751438141\n",
      "\n",
      "Epoch: 911 Train Loss: 0.1911\n",
      "\n",
      " Test Loss 0.21091704070568085\n",
      "\n",
      "Epoch: 912 Train Loss: 0.1911\n",
      "\n",
      " Test Loss 0.21082857251167297\n",
      "\n",
      "Epoch: 913 Train Loss: 0.1910\n",
      "\n",
      " Test Loss 0.21074050664901733\n",
      "\n",
      "Epoch: 914 Train Loss: 0.1909\n",
      "\n",
      " Test Loss 0.21065299212932587\n",
      "\n",
      "Epoch: 915 Train Loss: 0.1908\n",
      "\n",
      " Test Loss 0.2105659395456314\n",
      "\n",
      "Epoch: 916 Train Loss: 0.1907\n",
      "\n",
      " Test Loss 0.21047930419445038\n",
      "\n",
      "Epoch: 917 Train Loss: 0.1907\n",
      "\n",
      " Test Loss 0.21039316058158875\n",
      "\n",
      "Epoch: 918 Train Loss: 0.1906\n",
      "\n",
      " Test Loss 0.21030741930007935\n",
      "\n",
      "Epoch: 919 Train Loss: 0.1905\n",
      "\n",
      " Test Loss 0.21022213995456696\n",
      "\n",
      "Epoch: 920 Train Loss: 0.1904\n",
      "\n",
      " Test Loss 0.21013732254505157\n",
      "\n",
      "Epoch: 921 Train Loss: 0.1904\n",
      "\n",
      " Test Loss 0.21005293726921082\n",
      "\n",
      "Epoch: 922 Train Loss: 0.1903\n",
      "\n",
      " Test Loss 0.20996901392936707\n",
      "\n",
      "Epoch: 923 Train Loss: 0.1902\n",
      "\n",
      " Test Loss 0.20988549292087555\n",
      "\n",
      "Epoch: 924 Train Loss: 0.1902\n",
      "\n",
      " Test Loss 0.20980244874954224\n",
      "\n",
      "Epoch: 925 Train Loss: 0.1901\n",
      "\n",
      " Test Loss 0.20971977710723877\n",
      "\n",
      "Epoch: 926 Train Loss: 0.1900\n",
      "\n",
      " Test Loss 0.20963755249977112\n",
      "\n",
      "Epoch: 927 Train Loss: 0.1899\n",
      "\n",
      " Test Loss 0.20955567061901093\n",
      "\n",
      "Epoch: 928 Train Loss: 0.1899\n",
      "\n",
      " Test Loss 0.20947429537773132\n",
      "\n",
      "Epoch: 929 Train Loss: 0.1898\n",
      "\n",
      " Test Loss 0.20939335227012634\n",
      "\n",
      "Epoch: 930 Train Loss: 0.1897\n",
      "\n",
      " Test Loss 0.2093127816915512\n",
      "\n",
      "Epoch: 931 Train Loss: 0.1897\n",
      "\n",
      " Test Loss 0.20923256874084473\n",
      "\n",
      "Epoch: 932 Train Loss: 0.1896\n",
      "\n",
      " Test Loss 0.20915286242961884\n",
      "\n",
      "Epoch: 933 Train Loss: 0.1895\n",
      "\n",
      " Test Loss 0.2090735137462616\n",
      "\n",
      "Epoch: 934 Train Loss: 0.1895\n",
      "\n",
      " Test Loss 0.20899461209774017\n",
      "\n",
      "Epoch: 935 Train Loss: 0.1894\n",
      "\n",
      " Test Loss 0.20891602337360382\n",
      "\n",
      "Epoch: 936 Train Loss: 0.1893\n",
      "\n",
      " Test Loss 0.20883792638778687\n",
      "\n",
      "Epoch: 937 Train Loss: 0.1892\n",
      "\n",
      " Test Loss 0.20876017212867737\n",
      "\n",
      "Epoch: 938 Train Loss: 0.1892\n",
      "\n",
      " Test Loss 0.20868276059627533\n",
      "\n",
      "Epoch: 939 Train Loss: 0.1891\n",
      "\n",
      " Test Loss 0.2086057811975479\n",
      "\n",
      "Epoch: 940 Train Loss: 0.1890\n",
      "\n",
      " Test Loss 0.20852915942668915\n",
      "\n",
      "Epoch: 941 Train Loss: 0.1890\n",
      "\n",
      " Test Loss 0.2084529548883438\n",
      "\n",
      "Epoch: 942 Train Loss: 0.1889\n",
      "\n",
      " Test Loss 0.20837712287902832\n",
      "\n",
      "Epoch: 943 Train Loss: 0.1888\n",
      "\n",
      " Test Loss 0.2083016186952591\n",
      "\n",
      "Epoch: 944 Train Loss: 0.1888\n",
      "\n",
      " Test Loss 0.2082265019416809\n",
      "\n",
      "Epoch: 945 Train Loss: 0.1887\n",
      "\n",
      " Test Loss 0.20815177261829376\n",
      "\n",
      "Epoch: 946 Train Loss: 0.1887\n",
      "\n",
      " Test Loss 0.20807738602161407\n",
      "\n",
      "Epoch: 947 Train Loss: 0.1886\n",
      "\n",
      " Test Loss 0.20800331234931946\n",
      "\n",
      "Epoch: 948 Train Loss: 0.1885\n",
      "\n",
      " Test Loss 0.2079296112060547\n",
      "\n",
      "Epoch: 949 Train Loss: 0.1885\n",
      "\n",
      " Test Loss 0.2078561931848526\n",
      "\n",
      "Epoch: 950 Train Loss: 0.1884\n",
      "\n",
      " Test Loss 0.20778314769268036\n",
      "\n",
      "Epoch: 951 Train Loss: 0.1883\n",
      "\n",
      " Test Loss 0.20771019160747528\n",
      "\n",
      "Epoch: 952 Train Loss: 0.1883\n",
      "\n",
      " Test Loss 0.2076375037431717\n",
      "\n",
      "Epoch: 953 Train Loss: 0.1882\n",
      "\n",
      " Test Loss 0.20756518840789795\n",
      "\n",
      "Epoch: 954 Train Loss: 0.1881\n",
      "\n",
      " Test Loss 0.20749317109584808\n",
      "\n",
      "Epoch: 955 Train Loss: 0.1881\n",
      "\n",
      " Test Loss 0.20742155611515045\n",
      "\n",
      "Epoch: 956 Train Loss: 0.1880\n",
      "\n",
      " Test Loss 0.20735016465187073\n",
      "\n",
      "Epoch: 957 Train Loss: 0.1879\n",
      "\n",
      " Test Loss 0.207279235124588\n",
      "\n",
      "Epoch: 958 Train Loss: 0.1879\n",
      "\n",
      " Test Loss 0.2072085440158844\n",
      "\n",
      "Epoch: 959 Train Loss: 0.1878\n",
      "\n",
      " Test Loss 0.20713822543621063\n",
      "\n",
      "Epoch: 960 Train Loss: 0.1878\n",
      "\n",
      " Test Loss 0.20706817507743835\n",
      "\n",
      "Epoch: 961 Train Loss: 0.1877\n",
      "\n",
      " Test Loss 0.20699840784072876\n",
      "\n",
      "Epoch: 962 Train Loss: 0.1876\n",
      "\n",
      " Test Loss 0.206929013133049\n",
      "\n",
      "Epoch: 963 Train Loss: 0.1876\n",
      "\n",
      " Test Loss 0.20685985684394836\n",
      "\n",
      "Epoch: 964 Train Loss: 0.1875\n",
      "\n",
      " Test Loss 0.20679108798503876\n",
      "\n",
      "Epoch: 965 Train Loss: 0.1875\n",
      "\n",
      " Test Loss 0.20672261714935303\n",
      "\n",
      "Epoch: 966 Train Loss: 0.1874\n",
      "\n",
      " Test Loss 0.2066543847322464\n",
      "\n",
      "Epoch: 967 Train Loss: 0.1873\n",
      "\n",
      " Test Loss 0.20658646523952484\n",
      "\n",
      "Epoch: 968 Train Loss: 0.1873\n",
      "\n",
      " Test Loss 0.20651884377002716\n",
      "\n",
      "Epoch: 969 Train Loss: 0.1872\n",
      "\n",
      " Test Loss 0.20645146071910858\n",
      "\n",
      "Epoch: 970 Train Loss: 0.1872\n",
      "\n",
      " Test Loss 0.20638443529605865\n",
      "\n",
      "Epoch: 971 Train Loss: 0.1871\n",
      "\n",
      " Test Loss 0.20631766319274902\n",
      "\n",
      "Epoch: 972 Train Loss: 0.1870\n",
      "\n",
      " Test Loss 0.20625121891498566\n",
      "\n",
      "Epoch: 973 Train Loss: 0.1870\n",
      "\n",
      " Test Loss 0.206184983253479\n",
      "\n",
      "Epoch: 974 Train Loss: 0.1869\n",
      "\n",
      " Test Loss 0.2061191350221634\n",
      "\n",
      "Epoch: 975 Train Loss: 0.1869\n",
      "\n",
      " Test Loss 0.2060534954071045\n",
      "\n",
      "Epoch: 976 Train Loss: 0.1868\n",
      "\n",
      " Test Loss 0.20598818361759186\n",
      "\n",
      "Epoch: 977 Train Loss: 0.1868\n",
      "\n",
      " Test Loss 0.2059231549501419\n",
      "\n",
      "Epoch: 978 Train Loss: 0.1867\n",
      "\n",
      " Test Loss 0.20585836470127106\n",
      "\n",
      "Epoch: 979 Train Loss: 0.1866\n",
      "\n",
      " Test Loss 0.20579388737678528\n",
      "\n",
      "Epoch: 980 Train Loss: 0.1866\n",
      "\n",
      " Test Loss 0.2057296186685562\n",
      "\n",
      "Epoch: 981 Train Loss: 0.1865\n",
      "\n",
      " Test Loss 0.2056656777858734\n",
      "\n",
      "Epoch: 982 Train Loss: 0.1865\n",
      "\n",
      " Test Loss 0.20560196042060852\n",
      "\n",
      "Epoch: 983 Train Loss: 0.1864\n",
      "\n",
      " Test Loss 0.20553851127624512\n",
      "\n",
      "Epoch: 984 Train Loss: 0.1864\n",
      "\n",
      " Test Loss 0.20547524094581604\n",
      "\n",
      "Epoch: 985 Train Loss: 0.1863\n",
      "\n",
      " Test Loss 0.20541219413280487\n",
      "\n",
      "Epoch: 986 Train Loss: 0.1862\n",
      "\n",
      " Test Loss 0.2053493708372116\n",
      "\n",
      "Epoch: 987 Train Loss: 0.1862\n",
      "\n",
      " Test Loss 0.20528686046600342\n",
      "\n",
      "Epoch: 988 Train Loss: 0.1861\n",
      "\n",
      " Test Loss 0.20522452890872955\n",
      "\n",
      "Epoch: 989 Train Loss: 0.1861\n",
      "\n",
      " Test Loss 0.20516251027584076\n",
      "\n",
      "Epoch: 990 Train Loss: 0.1860\n",
      "\n",
      " Test Loss 0.20510070025920868\n",
      "\n",
      "Epoch: 991 Train Loss: 0.1860\n",
      "\n",
      " Test Loss 0.20503917336463928\n",
      "\n",
      "Epoch: 992 Train Loss: 0.1859\n",
      "\n",
      " Test Loss 0.20497781038284302\n",
      "\n",
      "Epoch: 993 Train Loss: 0.1859\n",
      "\n",
      " Test Loss 0.20491676032543182\n",
      "\n",
      "Epoch: 994 Train Loss: 0.1858\n",
      "\n",
      " Test Loss 0.20485590398311615\n",
      "\n",
      "Epoch: 995 Train Loss: 0.1857\n",
      "\n",
      " Test Loss 0.20479530096054077\n",
      "\n",
      "Epoch: 996 Train Loss: 0.1857\n",
      "\n",
      " Test Loss 0.2047348916530609\n",
      "\n",
      "Epoch: 997 Train Loss: 0.1856\n",
      "\n",
      " Test Loss 0.20467470586299896\n",
      "\n",
      "Epoch: 998 Train Loss: 0.1856\n",
      "\n",
      " Test Loss 0.20461474359035492\n",
      "\n",
      "Epoch: 999 Train Loss: 0.1855\n",
      "\n",
      " Test Loss 0.20455504953861237\n",
      "\n",
      "Epoch: 1000 Train Loss: 0.1855\n",
      "\n",
      " Test Loss 0.20449548959732056\n",
      "\n",
      "Epoch: 1001 Train Loss: 0.1854\n",
      "\n",
      " Test Loss 0.204436257481575\n",
      "\n",
      "Epoch: 1002 Train Loss: 0.1854\n",
      "\n",
      " Test Loss 0.2043771594762802\n",
      "\n",
      "Epoch: 1003 Train Loss: 0.1853\n",
      "\n",
      " Test Loss 0.2043183296918869\n",
      "\n",
      "Epoch: 1004 Train Loss: 0.1853\n",
      "\n",
      " Test Loss 0.2042597085237503\n",
      "\n",
      "Epoch: 1005 Train Loss: 0.1852\n",
      "\n",
      " Test Loss 0.20420129597187042\n",
      "\n",
      "Epoch: 1006 Train Loss: 0.1852\n",
      "\n",
      " Test Loss 0.20414310693740845\n",
      "\n",
      "Epoch: 1007 Train Loss: 0.1851\n",
      "\n",
      " Test Loss 0.204085111618042\n",
      "\n",
      "Epoch: 1008 Train Loss: 0.1850\n",
      "\n",
      " Test Loss 0.20402733981609344\n",
      "\n",
      "Epoch: 1009 Train Loss: 0.1850\n",
      "\n",
      " Test Loss 0.203969806432724\n",
      "\n",
      "Epoch: 1010 Train Loss: 0.1849\n",
      "\n",
      " Test Loss 0.20391243696212769\n",
      "\n",
      "Epoch: 1011 Train Loss: 0.1849\n",
      "\n",
      " Test Loss 0.20385527610778809\n",
      "\n",
      "Epoch: 1012 Train Loss: 0.1848\n",
      "\n",
      " Test Loss 0.2037983536720276\n",
      "\n",
      "Epoch: 1013 Train Loss: 0.1848\n",
      "\n",
      " Test Loss 0.20374161005020142\n",
      "\n",
      "Epoch: 1014 Train Loss: 0.1847\n",
      "\n",
      " Test Loss 0.20368504524230957\n",
      "\n",
      "Epoch: 1015 Train Loss: 0.1847\n",
      "\n",
      " Test Loss 0.20362873375415802\n",
      "\n",
      "Epoch: 1016 Train Loss: 0.1846\n",
      "\n",
      " Test Loss 0.2035725712776184\n",
      "\n",
      "Epoch: 1017 Train Loss: 0.1846\n",
      "\n",
      " Test Loss 0.2035166174173355\n",
      "\n",
      "Epoch: 1018 Train Loss: 0.1845\n",
      "\n",
      " Test Loss 0.20346084237098694\n",
      "\n",
      "Epoch: 1019 Train Loss: 0.1845\n",
      "\n",
      " Test Loss 0.20340532064437866\n",
      "\n",
      "Epoch: 1020 Train Loss: 0.1844\n",
      "\n",
      " Test Loss 0.20334994792938232\n",
      "\n",
      "Epoch: 1021 Train Loss: 0.1844\n",
      "\n",
      " Test Loss 0.2032947987318039\n",
      "\n",
      "Epoch: 1022 Train Loss: 0.1843\n",
      "\n",
      " Test Loss 0.2032397985458374\n",
      "\n",
      "Epoch: 1023 Train Loss: 0.1843\n",
      "\n",
      " Test Loss 0.20318496227264404\n",
      "\n",
      "Epoch: 1024 Train Loss: 0.1842\n",
      "\n",
      " Test Loss 0.2031303495168686\n",
      "\n",
      "Epoch: 1025 Train Loss: 0.1842\n",
      "\n",
      " Test Loss 0.20307593047618866\n",
      "\n",
      "Epoch: 1026 Train Loss: 0.1841\n",
      "\n",
      " Test Loss 0.20302164554595947\n",
      "\n",
      "Epoch: 1027 Train Loss: 0.1841\n",
      "\n",
      " Test Loss 0.202967569231987\n",
      "\n",
      "Epoch: 1028 Train Loss: 0.1840\n",
      "\n",
      " Test Loss 0.20291365683078766\n",
      "\n",
      "Epoch: 1029 Train Loss: 0.1840\n",
      "\n",
      " Test Loss 0.20285992324352264\n",
      "\n",
      "Epoch: 1030 Train Loss: 0.1839\n",
      "\n",
      " Test Loss 0.20280636847019196\n",
      "\n",
      "Epoch: 1031 Train Loss: 0.1839\n",
      "\n",
      " Test Loss 0.2027529627084732\n",
      "\n",
      "Epoch: 1032 Train Loss: 0.1838\n",
      "\n",
      " Test Loss 0.20269973576068878\n",
      "\n",
      "Epoch: 1033 Train Loss: 0.1838\n",
      "\n",
      " Test Loss 0.20264668762683868\n",
      "\n",
      "Epoch: 1034 Train Loss: 0.1837\n",
      "\n",
      " Test Loss 0.2025936096906662\n",
      "\n",
      "Epoch: 1035 Train Loss: 0.1837\n",
      "\n",
      " Test Loss 0.20254084467887878\n",
      "\n",
      "Epoch: 1036 Train Loss: 0.1836\n",
      "\n",
      " Test Loss 0.20248813927173615\n",
      "\n",
      "Epoch: 1037 Train Loss: 0.1836\n",
      "\n",
      " Test Loss 0.2024356871843338\n",
      "\n",
      "Epoch: 1038 Train Loss: 0.1835\n",
      "\n",
      " Test Loss 0.20238327980041504\n",
      "\n",
      "Epoch: 1039 Train Loss: 0.1835\n",
      "\n",
      " Test Loss 0.20233112573623657\n",
      "\n",
      "Epoch: 1040 Train Loss: 0.1834\n",
      "\n",
      " Test Loss 0.20227910578250885\n",
      "\n",
      "Epoch: 1041 Train Loss: 0.1834\n",
      "\n",
      " Test Loss 0.20222729444503784\n",
      "\n",
      "Epoch: 1042 Train Loss: 0.1834\n",
      "\n",
      " Test Loss 0.2021755874156952\n",
      "\n",
      "Epoch: 1043 Train Loss: 0.1833\n",
      "\n",
      " Test Loss 0.20212401449680328\n",
      "\n",
      "Epoch: 1044 Train Loss: 0.1833\n",
      "\n",
      " Test Loss 0.2020726501941681\n",
      "\n",
      "Epoch: 1045 Train Loss: 0.1832\n",
      "\n",
      " Test Loss 0.20202144980430603\n",
      "\n",
      "Epoch: 1046 Train Loss: 0.1832\n",
      "\n",
      " Test Loss 0.20197035372257233\n",
      "\n",
      "Epoch: 1047 Train Loss: 0.1831\n",
      "\n",
      " Test Loss 0.20191943645477295\n",
      "\n",
      "Epoch: 1048 Train Loss: 0.1831\n",
      "\n",
      " Test Loss 0.2018686681985855\n",
      "\n",
      "Epoch: 1049 Train Loss: 0.1830\n",
      "\n",
      " Test Loss 0.2018180638551712\n",
      "\n",
      "Epoch: 1050 Train Loss: 0.1830\n",
      "\n",
      " Test Loss 0.20176756381988525\n",
      "\n",
      "Epoch: 1051 Train Loss: 0.1829\n",
      "\n",
      " Test Loss 0.20171722769737244\n",
      "\n",
      "Epoch: 1052 Train Loss: 0.1829\n",
      "\n",
      " Test Loss 0.20166707038879395\n",
      "\n",
      "Epoch: 1053 Train Loss: 0.1828\n",
      "\n",
      " Test Loss 0.20161698758602142\n",
      "\n",
      "Epoch: 1054 Train Loss: 0.1828\n",
      "\n",
      " Test Loss 0.20156708359718323\n",
      "\n",
      "Epoch: 1055 Train Loss: 0.1827\n",
      "\n",
      " Test Loss 0.20151734352111816\n",
      "\n",
      "Epoch: 1056 Train Loss: 0.1827\n",
      "\n",
      " Test Loss 0.20146773755550385\n",
      "\n",
      "Epoch: 1057 Train Loss: 0.1827\n",
      "\n",
      " Test Loss 0.20141825079917908\n",
      "\n",
      "Epoch: 1058 Train Loss: 0.1826\n",
      "\n",
      " Test Loss 0.20136895775794983\n",
      "\n",
      "Epoch: 1059 Train Loss: 0.1826\n",
      "\n",
      " Test Loss 0.20131975412368774\n",
      "\n",
      "Epoch: 1060 Train Loss: 0.1825\n",
      "\n",
      " Test Loss 0.2012707144021988\n",
      "\n",
      "Epoch: 1061 Train Loss: 0.1825\n",
      "\n",
      " Test Loss 0.2012217789888382\n",
      "\n",
      "Epoch: 1062 Train Loss: 0.1824\n",
      "\n",
      " Test Loss 0.20117299258708954\n",
      "\n",
      "Epoch: 1063 Train Loss: 0.1824\n",
      "\n",
      " Test Loss 0.20112435519695282\n",
      "\n",
      "Epoch: 1064 Train Loss: 0.1823\n",
      "\n",
      " Test Loss 0.20107588171958923\n",
      "\n",
      "Epoch: 1065 Train Loss: 0.1823\n",
      "\n",
      " Test Loss 0.20102748274803162\n",
      "\n",
      "Epoch: 1066 Train Loss: 0.1822\n",
      "\n",
      " Test Loss 0.20097914338111877\n",
      "\n",
      "Epoch: 1067 Train Loss: 0.1822\n",
      "\n",
      " Test Loss 0.2009309083223343\n",
      "\n",
      "Epoch: 1068 Train Loss: 0.1821\n",
      "\n",
      " Test Loss 0.20088279247283936\n",
      "\n",
      "Epoch: 1069 Train Loss: 0.1821\n",
      "\n",
      " Test Loss 0.20083484053611755\n",
      "\n",
      "Epoch: 1070 Train Loss: 0.1821\n",
      "\n",
      " Test Loss 0.20078694820404053\n",
      "\n",
      "Epoch: 1071 Train Loss: 0.1820\n",
      "\n",
      " Test Loss 0.20073913037776947\n",
      "\n",
      "Epoch: 1072 Train Loss: 0.1820\n",
      "\n",
      " Test Loss 0.20069152116775513\n",
      "\n",
      "Epoch: 1073 Train Loss: 0.1819\n",
      "\n",
      " Test Loss 0.20064400136470795\n",
      "\n",
      "Epoch: 1074 Train Loss: 0.1819\n",
      "\n",
      " Test Loss 0.2005966305732727\n",
      "\n",
      "Epoch: 1075 Train Loss: 0.1818\n",
      "\n",
      " Test Loss 0.20054928958415985\n",
      "\n",
      "Epoch: 1076 Train Loss: 0.1818\n",
      "\n",
      " Test Loss 0.20050214231014252\n",
      "\n",
      "Epoch: 1077 Train Loss: 0.1817\n",
      "\n",
      " Test Loss 0.20045511424541473\n",
      "\n",
      "Epoch: 1078 Train Loss: 0.1817\n",
      "\n",
      " Test Loss 0.2004082202911377\n",
      "\n",
      "Epoch: 1079 Train Loss: 0.1817\n",
      "\n",
      " Test Loss 0.20036137104034424\n",
      "\n",
      "Epoch: 1080 Train Loss: 0.1816\n",
      "\n",
      " Test Loss 0.20031467080116272\n",
      "\n",
      "Epoch: 1081 Train Loss: 0.1816\n",
      "\n",
      " Test Loss 0.20026811957359314\n",
      "\n",
      "Epoch: 1082 Train Loss: 0.1815\n",
      "\n",
      " Test Loss 0.20022164285182953\n",
      "\n",
      "Epoch: 1083 Train Loss: 0.1815\n",
      "\n",
      " Test Loss 0.20017525553703308\n",
      "\n",
      "Epoch: 1084 Train Loss: 0.1814\n",
      "\n",
      " Test Loss 0.20012900233268738\n",
      "\n",
      "Epoch: 1085 Train Loss: 0.1814\n",
      "\n",
      " Test Loss 0.20008283853530884\n",
      "\n",
      "Epoch: 1086 Train Loss: 0.1814\n",
      "\n",
      " Test Loss 0.20003680884838104\n",
      "\n",
      "Epoch: 1087 Train Loss: 0.1813\n",
      "\n",
      " Test Loss 0.1999908685684204\n",
      "\n",
      "Epoch: 1088 Train Loss: 0.1813\n",
      "\n",
      " Test Loss 0.19994504749774933\n",
      "\n",
      "Epoch: 1089 Train Loss: 0.1812\n",
      "\n",
      " Test Loss 0.19989937543869019\n",
      "\n",
      "Epoch: 1090 Train Loss: 0.1812\n",
      "\n",
      " Test Loss 0.19985374808311462\n",
      "\n",
      "Epoch: 1091 Train Loss: 0.1811\n",
      "\n",
      " Test Loss 0.19980819523334503\n",
      "\n",
      "Epoch: 1092 Train Loss: 0.1811\n",
      "\n",
      " Test Loss 0.19976283609867096\n",
      "\n",
      "Epoch: 1093 Train Loss: 0.1810\n",
      "\n",
      " Test Loss 0.19971758127212524\n",
      "\n",
      "Epoch: 1094 Train Loss: 0.1810\n",
      "\n",
      " Test Loss 0.19967232644557953\n",
      "\n",
      "Epoch: 1095 Train Loss: 0.1810\n",
      "\n",
      " Test Loss 0.19962722063064575\n",
      "\n",
      "Epoch: 1096 Train Loss: 0.1809\n",
      "\n",
      " Test Loss 0.19958221912384033\n",
      "\n",
      "Epoch: 1097 Train Loss: 0.1809\n",
      "\n",
      " Test Loss 0.19953732192516327\n",
      "\n",
      "Epoch: 1098 Train Loss: 0.1808\n",
      "\n",
      " Test Loss 0.19949254393577576\n",
      "\n",
      "Epoch: 1099 Train Loss: 0.1808\n",
      "\n",
      " Test Loss 0.1994478702545166\n",
      "\n",
      "Epoch: 1100 Train Loss: 0.1808\n",
      "\n",
      " Test Loss 0.19940324127674103\n",
      "\n",
      "Epoch: 1101 Train Loss: 0.1807\n",
      "\n",
      " Test Loss 0.1993587166070938\n",
      "\n",
      "Epoch: 1102 Train Loss: 0.1807\n",
      "\n",
      " Test Loss 0.19931432604789734\n",
      "\n",
      "Epoch: 1103 Train Loss: 0.1806\n",
      "\n",
      " Test Loss 0.19927000999450684\n",
      "\n",
      "Epoch: 1104 Train Loss: 0.1806\n",
      "\n",
      " Test Loss 0.1992257982492447\n",
      "\n",
      "Epoch: 1105 Train Loss: 0.1805\n",
      "\n",
      " Test Loss 0.19918173551559448\n",
      "\n",
      "Epoch: 1106 Train Loss: 0.1805\n",
      "\n",
      " Test Loss 0.19913770258426666\n",
      "\n",
      "Epoch: 1107 Train Loss: 0.1805\n",
      "\n",
      " Test Loss 0.1990938037633896\n",
      "\n",
      "Epoch: 1108 Train Loss: 0.1804\n",
      "\n",
      " Test Loss 0.19904999434947968\n",
      "\n",
      "Epoch: 1109 Train Loss: 0.1804\n",
      "\n",
      " Test Loss 0.19900625944137573\n",
      "\n",
      "Epoch: 1110 Train Loss: 0.1803\n",
      "\n",
      " Test Loss 0.19896256923675537\n",
      "\n",
      "Epoch: 1111 Train Loss: 0.1803\n",
      "\n",
      " Test Loss 0.19891899824142456\n",
      "\n",
      "Epoch: 1112 Train Loss: 0.1802\n",
      "\n",
      " Test Loss 0.19887544214725494\n",
      "\n",
      "Epoch: 1113 Train Loss: 0.1802\n",
      "\n",
      " Test Loss 0.1988319456577301\n",
      "\n",
      "Epoch: 1114 Train Loss: 0.1802\n",
      "\n",
      " Test Loss 0.19878855347633362\n",
      "\n",
      "Epoch: 1115 Train Loss: 0.1801\n",
      "\n",
      " Test Loss 0.1987452358007431\n",
      "\n",
      "Epoch: 1116 Train Loss: 0.1801\n",
      "\n",
      " Test Loss 0.19870199263095856\n",
      "\n",
      "Epoch: 1117 Train Loss: 0.1800\n",
      "\n",
      " Test Loss 0.19865882396697998\n",
      "\n",
      "Epoch: 1118 Train Loss: 0.1800\n",
      "\n",
      " Test Loss 0.19861580431461334\n",
      "\n",
      "Epoch: 1119 Train Loss: 0.1800\n",
      "\n",
      " Test Loss 0.19857282936573029\n",
      "\n",
      "Epoch: 1120 Train Loss: 0.1799\n",
      "\n",
      " Test Loss 0.1985299289226532\n",
      "\n",
      "Epoch: 1121 Train Loss: 0.1799\n",
      "\n",
      " Test Loss 0.19848714768886566\n",
      "\n",
      "Epoch: 1122 Train Loss: 0.1798\n",
      "\n",
      " Test Loss 0.1984444260597229\n",
      "\n",
      "Epoch: 1123 Train Loss: 0.1798\n",
      "\n",
      " Test Loss 0.1984017789363861\n",
      "\n",
      "Epoch: 1124 Train Loss: 0.1798\n",
      "\n",
      " Test Loss 0.19835926592350006\n",
      "\n",
      "Epoch: 1125 Train Loss: 0.1797\n",
      "\n",
      " Test Loss 0.1983168125152588\n",
      "\n",
      "Epoch: 1126 Train Loss: 0.1797\n",
      "\n",
      " Test Loss 0.19827446341514587\n",
      "\n",
      "Epoch: 1127 Train Loss: 0.1796\n",
      "\n",
      " Test Loss 0.19823217391967773\n",
      "\n",
      "Epoch: 1128 Train Loss: 0.1796\n",
      "\n",
      " Test Loss 0.19818997383117676\n",
      "\n",
      "Epoch: 1129 Train Loss: 0.1796\n",
      "\n",
      " Test Loss 0.19814789295196533\n",
      "\n",
      "Epoch: 1130 Train Loss: 0.1795\n",
      "\n",
      " Test Loss 0.1981058418750763\n",
      "\n",
      "Epoch: 1131 Train Loss: 0.1795\n",
      "\n",
      " Test Loss 0.1980638951063156\n",
      "\n",
      "Epoch: 1132 Train Loss: 0.1794\n",
      "\n",
      " Test Loss 0.1980220228433609\n",
      "\n",
      "Epoch: 1133 Train Loss: 0.1794\n",
      "\n",
      " Test Loss 0.19798026978969574\n",
      "\n",
      "Epoch: 1134 Train Loss: 0.1794\n",
      "\n",
      " Test Loss 0.19793856143951416\n",
      "\n",
      "Epoch: 1135 Train Loss: 0.1793\n",
      "\n",
      " Test Loss 0.19789694249629974\n",
      "\n",
      "Epoch: 1136 Train Loss: 0.1793\n",
      "\n",
      " Test Loss 0.1978554129600525\n",
      "\n",
      "Epoch: 1137 Train Loss: 0.1792\n",
      "\n",
      " Test Loss 0.1978139579296112\n",
      "\n",
      "Epoch: 1138 Train Loss: 0.1792\n",
      "\n",
      " Test Loss 0.1977725774049759\n",
      "\n",
      "Epoch: 1139 Train Loss: 0.1792\n",
      "\n",
      " Test Loss 0.19773125648498535\n",
      "\n",
      "Epoch: 1140 Train Loss: 0.1791\n",
      "\n",
      " Test Loss 0.19769003987312317\n",
      "\n",
      "Epoch: 1141 Train Loss: 0.1791\n",
      "\n",
      " Test Loss 0.19764892756938934\n",
      "\n",
      "Epoch: 1142 Train Loss: 0.1790\n",
      "\n",
      " Test Loss 0.1976078599691391\n",
      "\n",
      "Epoch: 1143 Train Loss: 0.1790\n",
      "\n",
      " Test Loss 0.19756683707237244\n",
      "\n",
      "Epoch: 1144 Train Loss: 0.1790\n",
      "\n",
      " Test Loss 0.19752594828605652\n",
      "\n",
      "Epoch: 1145 Train Loss: 0.1789\n",
      "\n",
      " Test Loss 0.19748511910438538\n",
      "\n",
      "Epoch: 1146 Train Loss: 0.1789\n",
      "\n",
      " Test Loss 0.197444349527359\n",
      "\n",
      "Epoch: 1147 Train Loss: 0.1788\n",
      "\n",
      " Test Loss 0.1974037140607834\n",
      "\n",
      "Epoch: 1148 Train Loss: 0.1788\n",
      "\n",
      " Test Loss 0.19736307859420776\n",
      "\n",
      "Epoch: 1149 Train Loss: 0.1788\n",
      "\n",
      " Test Loss 0.1973225474357605\n",
      "\n",
      "Epoch: 1150 Train Loss: 0.1787\n",
      "\n",
      " Test Loss 0.1972821056842804\n",
      "\n",
      "Epoch: 1151 Train Loss: 0.1787\n",
      "\n",
      " Test Loss 0.19724173843860626\n",
      "\n",
      "Epoch: 1152 Train Loss: 0.1786\n",
      "\n",
      " Test Loss 0.19720140099525452\n",
      "\n",
      "Epoch: 1153 Train Loss: 0.1786\n",
      "\n",
      " Test Loss 0.19716119766235352\n",
      "\n",
      "Epoch: 1154 Train Loss: 0.1786\n",
      "\n",
      " Test Loss 0.19712099432945251\n",
      "\n",
      "Epoch: 1155 Train Loss: 0.1785\n",
      "\n",
      " Test Loss 0.19708091020584106\n",
      "\n",
      "Epoch: 1156 Train Loss: 0.1785\n",
      "\n",
      " Test Loss 0.1970408856868744\n",
      "\n",
      "Epoch: 1157 Train Loss: 0.1785\n",
      "\n",
      " Test Loss 0.1970009207725525\n",
      "\n",
      "Epoch: 1158 Train Loss: 0.1784\n",
      "\n",
      " Test Loss 0.19696108996868134\n",
      "\n",
      "Epoch: 1159 Train Loss: 0.1784\n",
      "\n",
      " Test Loss 0.1969212293624878\n",
      "\n",
      "Epoch: 1160 Train Loss: 0.1783\n",
      "\n",
      " Test Loss 0.1968815177679062\n",
      "\n",
      "Epoch: 1161 Train Loss: 0.1783\n",
      "\n",
      " Test Loss 0.19684180617332458\n",
      "\n",
      "Epoch: 1162 Train Loss: 0.1783\n",
      "\n",
      " Test Loss 0.19680218398571014\n",
      "\n",
      "Epoch: 1163 Train Loss: 0.1782\n",
      "\n",
      " Test Loss 0.19676263630390167\n",
      "\n",
      "Epoch: 1164 Train Loss: 0.1782\n",
      "\n",
      " Test Loss 0.19672313332557678\n",
      "\n",
      "Epoch: 1165 Train Loss: 0.1781\n",
      "\n",
      " Test Loss 0.19668374955654144\n",
      "\n",
      "Epoch: 1166 Train Loss: 0.1781\n",
      "\n",
      " Test Loss 0.19664441049098969\n",
      "\n",
      "Epoch: 1167 Train Loss: 0.1781\n",
      "\n",
      " Test Loss 0.19660517573356628\n",
      "\n",
      "Epoch: 1168 Train Loss: 0.1780\n",
      "\n",
      " Test Loss 0.1965659111738205\n",
      "\n",
      "Epoch: 1169 Train Loss: 0.1780\n",
      "\n",
      " Test Loss 0.19652676582336426\n",
      "\n",
      "Epoch: 1170 Train Loss: 0.1780\n",
      "\n",
      " Test Loss 0.1964876800775528\n",
      "\n",
      "Epoch: 1171 Train Loss: 0.1779\n",
      "\n",
      " Test Loss 0.1964486837387085\n",
      "\n",
      "Epoch: 1172 Train Loss: 0.1779\n",
      "\n",
      " Test Loss 0.19640973210334778\n",
      "\n",
      "Epoch: 1173 Train Loss: 0.1778\n",
      "\n",
      " Test Loss 0.19637082517147064\n",
      "\n",
      "Epoch: 1174 Train Loss: 0.1778\n",
      "\n",
      " Test Loss 0.19633199274539948\n",
      "\n",
      "Epoch: 1175 Train Loss: 0.1778\n",
      "\n",
      " Test Loss 0.19629323482513428\n",
      "\n",
      "Epoch: 1176 Train Loss: 0.1777\n",
      "\n",
      " Test Loss 0.19625453650951385\n",
      "\n",
      "Epoch: 1177 Train Loss: 0.1777\n",
      "\n",
      " Test Loss 0.19621586799621582\n",
      "\n",
      "Epoch: 1178 Train Loss: 0.1777\n",
      "\n",
      " Test Loss 0.19617730379104614\n",
      "\n",
      "Epoch: 1179 Train Loss: 0.1776\n",
      "\n",
      " Test Loss 0.19613878428936005\n",
      "\n",
      "Epoch: 1180 Train Loss: 0.1776\n",
      "\n",
      " Test Loss 0.19610030949115753\n",
      "\n",
      "Epoch: 1181 Train Loss: 0.1775\n",
      "\n",
      " Test Loss 0.19606192409992218\n",
      "\n",
      "Epoch: 1182 Train Loss: 0.1775\n",
      "\n",
      " Test Loss 0.19602356851100922\n",
      "\n",
      "Epoch: 1183 Train Loss: 0.1775\n",
      "\n",
      " Test Loss 0.19598528742790222\n",
      "\n",
      "Epoch: 1184 Train Loss: 0.1774\n",
      "\n",
      " Test Loss 0.1959470957517624\n",
      "\n",
      "Epoch: 1185 Train Loss: 0.1774\n",
      "\n",
      " Test Loss 0.19590888917446136\n",
      "\n",
      "Epoch: 1186 Train Loss: 0.1774\n",
      "\n",
      " Test Loss 0.19587081670761108\n",
      "\n",
      "Epoch: 1187 Train Loss: 0.1773\n",
      "\n",
      " Test Loss 0.1958327740430832\n",
      "\n",
      "Epoch: 1188 Train Loss: 0.1773\n",
      "\n",
      " Test Loss 0.19579477608203888\n",
      "\n",
      "Epoch: 1189 Train Loss: 0.1773\n",
      "\n",
      " Test Loss 0.19575680792331696\n",
      "\n",
      "Epoch: 1190 Train Loss: 0.1772\n",
      "\n",
      " Test Loss 0.19571895897388458\n",
      "\n",
      "Epoch: 1191 Train Loss: 0.1772\n",
      "\n",
      " Test Loss 0.1956811249256134\n",
      "\n",
      "Epoch: 1192 Train Loss: 0.1771\n",
      "\n",
      " Test Loss 0.195643350481987\n",
      "\n",
      "Epoch: 1193 Train Loss: 0.1771\n",
      "\n",
      " Test Loss 0.19560565054416656\n",
      "\n",
      "Epoch: 1194 Train Loss: 0.1771\n",
      "\n",
      " Test Loss 0.1955680400133133\n",
      "\n",
      "Epoch: 1195 Train Loss: 0.1770\n",
      "\n",
      " Test Loss 0.19553042948246002\n",
      "\n",
      "Epoch: 1196 Train Loss: 0.1770\n",
      "\n",
      " Test Loss 0.19549289345741272\n",
      "\n",
      "Epoch: 1197 Train Loss: 0.1770\n",
      "\n",
      " Test Loss 0.1954554170370102\n",
      "\n",
      "Epoch: 1198 Train Loss: 0.1769\n",
      "\n",
      " Test Loss 0.19541803002357483\n",
      "\n",
      "Epoch: 1199 Train Loss: 0.1769\n",
      "\n",
      " Test Loss 0.19538061320781708\n",
      "\n",
      "Epoch: 1200 Train Loss: 0.1769\n",
      "\n",
      " Test Loss 0.19534331560134888\n",
      "\n",
      "Epoch: 1201 Train Loss: 0.1768\n",
      "\n",
      " Test Loss 0.19530606269836426\n",
      "\n",
      "Epoch: 1202 Train Loss: 0.1768\n",
      "\n",
      " Test Loss 0.19526886940002441\n",
      "\n",
      "Epoch: 1203 Train Loss: 0.1767\n",
      "\n",
      " Test Loss 0.19523173570632935\n",
      "\n",
      "Epoch: 1204 Train Loss: 0.1767\n",
      "\n",
      " Test Loss 0.19519461691379547\n",
      "\n",
      "Epoch: 1205 Train Loss: 0.1767\n",
      "\n",
      " Test Loss 0.19515755772590637\n",
      "\n",
      "Epoch: 1206 Train Loss: 0.1766\n",
      "\n",
      " Test Loss 0.19512060284614563\n",
      "\n",
      "Epoch: 1207 Train Loss: 0.1766\n",
      "\n",
      " Test Loss 0.19508370757102966\n",
      "\n",
      "Epoch: 1208 Train Loss: 0.1766\n",
      "\n",
      " Test Loss 0.1950467973947525\n",
      "\n",
      "Epoch: 1209 Train Loss: 0.1765\n",
      "\n",
      " Test Loss 0.1950099617242813\n",
      "\n",
      "Epoch: 1210 Train Loss: 0.1765\n",
      "\n",
      " Test Loss 0.1949731409549713\n",
      "\n",
      "Epoch: 1211 Train Loss: 0.1765\n",
      "\n",
      " Test Loss 0.19493642449378967\n",
      "\n",
      "Epoch: 1212 Train Loss: 0.1764\n",
      "\n",
      " Test Loss 0.19489970803260803\n",
      "\n",
      "Epoch: 1213 Train Loss: 0.1764\n",
      "\n",
      " Test Loss 0.19486309587955475\n",
      "\n",
      "Epoch: 1214 Train Loss: 0.1764\n",
      "\n",
      " Test Loss 0.19482649862766266\n",
      "\n",
      "Epoch: 1215 Train Loss: 0.1763\n",
      "\n",
      " Test Loss 0.19478996098041534\n",
      "\n",
      "Epoch: 1216 Train Loss: 0.1763\n",
      "\n",
      " Test Loss 0.1947534829378128\n",
      "\n",
      "Epoch: 1217 Train Loss: 0.1763\n",
      "\n",
      " Test Loss 0.19471704959869385\n",
      "\n",
      "Epoch: 1218 Train Loss: 0.1762\n",
      "\n",
      " Test Loss 0.19468067586421967\n",
      "\n",
      "Epoch: 1219 Train Loss: 0.1762\n",
      "\n",
      " Test Loss 0.19464433193206787\n",
      "\n",
      "Epoch: 1220 Train Loss: 0.1761\n",
      "\n",
      " Test Loss 0.19460806250572205\n",
      "\n",
      "Epoch: 1221 Train Loss: 0.1761\n",
      "\n",
      " Test Loss 0.19457180798053741\n",
      "\n",
      "Epoch: 1222 Train Loss: 0.1761\n",
      "\n",
      " Test Loss 0.19453558325767517\n",
      "\n",
      "Epoch: 1223 Train Loss: 0.1760\n",
      "\n",
      " Test Loss 0.1944994181394577\n",
      "\n",
      "Epoch: 1224 Train Loss: 0.1760\n",
      "\n",
      " Test Loss 0.19446329772472382\n",
      "\n",
      "Epoch: 1225 Train Loss: 0.1760\n",
      "\n",
      " Test Loss 0.1944272518157959\n",
      "\n",
      "Epoch: 1226 Train Loss: 0.1759\n",
      "\n",
      " Test Loss 0.19439129531383514\n",
      "\n",
      "Epoch: 1227 Train Loss: 0.1759\n",
      "\n",
      " Test Loss 0.1943552941083908\n",
      "\n",
      "Epoch: 1228 Train Loss: 0.1759\n",
      "\n",
      " Test Loss 0.19431936740875244\n",
      "\n",
      "Epoch: 1229 Train Loss: 0.1758\n",
      "\n",
      " Test Loss 0.19428351521492004\n",
      "\n",
      "Epoch: 1230 Train Loss: 0.1758\n",
      "\n",
      " Test Loss 0.19424769282341003\n",
      "\n",
      "Epoch: 1231 Train Loss: 0.1758\n",
      "\n",
      " Test Loss 0.194211944937706\n",
      "\n",
      "Epoch: 1232 Train Loss: 0.1757\n",
      "\n",
      " Test Loss 0.19417624175548553\n",
      "\n",
      "Epoch: 1233 Train Loss: 0.1757\n",
      "\n",
      " Test Loss 0.19414061307907104\n",
      "\n",
      "Epoch: 1234 Train Loss: 0.1757\n",
      "\n",
      " Test Loss 0.19410493969917297\n",
      "\n",
      "Epoch: 1235 Train Loss: 0.1756\n",
      "\n",
      " Test Loss 0.19406932592391968\n",
      "\n",
      "Epoch: 1236 Train Loss: 0.1756\n",
      "\n",
      " Test Loss 0.19403374195098877\n",
      "\n",
      "Epoch: 1237 Train Loss: 0.1756\n",
      "\n",
      " Test Loss 0.19399824738502502\n",
      "\n",
      "Epoch: 1238 Train Loss: 0.1755\n",
      "\n",
      " Test Loss 0.1939626783132553\n",
      "\n",
      "Epoch: 1239 Train Loss: 0.1755\n",
      "\n",
      " Test Loss 0.19392725825309753\n",
      "\n",
      "Epoch: 1240 Train Loss: 0.1755\n",
      "\n",
      " Test Loss 0.19389183819293976\n",
      "\n",
      "Epoch: 1241 Train Loss: 0.1754\n",
      "\n",
      " Test Loss 0.19385652244091034\n",
      "\n",
      "Epoch: 1242 Train Loss: 0.1754\n",
      "\n",
      " Test Loss 0.19382120668888092\n",
      "\n",
      "Epoch: 1243 Train Loss: 0.1754\n",
      "\n",
      " Test Loss 0.1937858909368515\n",
      "\n",
      "Epoch: 1244 Train Loss: 0.1753\n",
      "\n",
      " Test Loss 0.19375070929527283\n",
      "\n",
      "Epoch: 1245 Train Loss: 0.1753\n",
      "\n",
      " Test Loss 0.19371554255485535\n",
      "\n",
      "Epoch: 1246 Train Loss: 0.1753\n",
      "\n",
      " Test Loss 0.19368043541908264\n",
      "\n",
      "Epoch: 1247 Train Loss: 0.1752\n",
      "\n",
      " Test Loss 0.19364532828330994\n",
      "\n",
      "Epoch: 1248 Train Loss: 0.1752\n",
      "\n",
      " Test Loss 0.1936102658510208\n",
      "\n",
      "Epoch: 1249 Train Loss: 0.1752\n",
      "\n",
      " Test Loss 0.19357529282569885\n",
      "\n",
      "Epoch: 1250 Train Loss: 0.1751\n",
      "\n",
      " Test Loss 0.19354034960269928\n",
      "\n",
      "Epoch: 1251 Train Loss: 0.1751\n",
      "\n",
      " Test Loss 0.1935054212808609\n",
      "\n",
      "Epoch: 1252 Train Loss: 0.1751\n",
      "\n",
      " Test Loss 0.1934705674648285\n",
      "\n",
      "Epoch: 1253 Train Loss: 0.1750\n",
      "\n",
      " Test Loss 0.19343577325344086\n",
      "\n",
      "Epoch: 1254 Train Loss: 0.1750\n",
      "\n",
      " Test Loss 0.19340097904205322\n",
      "\n",
      "Epoch: 1255 Train Loss: 0.1750\n",
      "\n",
      " Test Loss 0.19336628913879395\n",
      "\n",
      "Epoch: 1256 Train Loss: 0.1749\n",
      "\n",
      " Test Loss 0.19333156943321228\n",
      "\n",
      "Epoch: 1257 Train Loss: 0.1749\n",
      "\n",
      " Test Loss 0.1932969093322754\n",
      "\n",
      "Epoch: 1258 Train Loss: 0.1749\n",
      "\n",
      " Test Loss 0.19326229393482208\n",
      "\n",
      "Epoch: 1259 Train Loss: 0.1748\n",
      "\n",
      " Test Loss 0.19322773814201355\n",
      "\n",
      "Epoch: 1260 Train Loss: 0.1748\n",
      "\n",
      " Test Loss 0.193193256855011\n",
      "\n",
      "Epoch: 1261 Train Loss: 0.1748\n",
      "\n",
      " Test Loss 0.19315876066684723\n",
      "\n",
      "Epoch: 1262 Train Loss: 0.1747\n",
      "\n",
      " Test Loss 0.19312432408332825\n",
      "\n",
      "Epoch: 1263 Train Loss: 0.1747\n",
      "\n",
      " Test Loss 0.19308994710445404\n",
      "\n",
      "Epoch: 1264 Train Loss: 0.1747\n",
      "\n",
      " Test Loss 0.19305557012557983\n",
      "\n",
      "Epoch: 1265 Train Loss: 0.1746\n",
      "\n",
      " Test Loss 0.1930212527513504\n",
      "\n",
      "Epoch: 1266 Train Loss: 0.1746\n",
      "\n",
      " Test Loss 0.19298695027828217\n",
      "\n",
      "Epoch: 1267 Train Loss: 0.1746\n",
      "\n",
      " Test Loss 0.1929527223110199\n",
      "\n",
      "Epoch: 1268 Train Loss: 0.1745\n",
      "\n",
      " Test Loss 0.1929185390472412\n",
      "\n",
      "Epoch: 1269 Train Loss: 0.1745\n",
      "\n",
      " Test Loss 0.19288435578346252\n",
      "\n",
      "Epoch: 1270 Train Loss: 0.1745\n",
      "\n",
      " Test Loss 0.192850261926651\n",
      "\n",
      "Epoch: 1271 Train Loss: 0.1744\n",
      "\n",
      " Test Loss 0.1928161382675171\n",
      "\n",
      "Epoch: 1272 Train Loss: 0.1744\n",
      "\n",
      " Test Loss 0.19278211891651154\n",
      "\n",
      "Epoch: 1273 Train Loss: 0.1744\n",
      "\n",
      " Test Loss 0.19274809956550598\n",
      "\n",
      "Epoch: 1274 Train Loss: 0.1743\n",
      "\n",
      " Test Loss 0.1927141547203064\n",
      "\n",
      "Epoch: 1275 Train Loss: 0.1743\n",
      "\n",
      " Test Loss 0.19268019497394562\n",
      "\n",
      "Epoch: 1276 Train Loss: 0.1743\n",
      "\n",
      " Test Loss 0.192646324634552\n",
      "\n",
      "Epoch: 1277 Train Loss: 0.1742\n",
      "\n",
      " Test Loss 0.19261249899864197\n",
      "\n",
      "Epoch: 1278 Train Loss: 0.1742\n",
      "\n",
      " Test Loss 0.19257871806621552\n",
      "\n",
      "Epoch: 1279 Train Loss: 0.1742\n",
      "\n",
      " Test Loss 0.19254493713378906\n",
      "\n",
      "Epoch: 1280 Train Loss: 0.1741\n",
      "\n",
      " Test Loss 0.19251127541065216\n",
      "\n",
      "Epoch: 1281 Train Loss: 0.1741\n",
      "\n",
      " Test Loss 0.19247758388519287\n",
      "\n",
      "Epoch: 1282 Train Loss: 0.1741\n",
      "\n",
      " Test Loss 0.19244392216205597\n",
      "\n",
      "Epoch: 1283 Train Loss: 0.1740\n",
      "\n",
      " Test Loss 0.19241027534008026\n",
      "\n",
      "Epoch: 1284 Train Loss: 0.1740\n",
      "\n",
      " Test Loss 0.19237668812274933\n",
      "\n",
      "Epoch: 1285 Train Loss: 0.1740\n",
      "\n",
      " Test Loss 0.19234313070774078\n",
      "\n",
      "Epoch: 1286 Train Loss: 0.1739\n",
      "\n",
      " Test Loss 0.1923096626996994\n",
      "\n",
      "Epoch: 1287 Train Loss: 0.1739\n",
      "\n",
      " Test Loss 0.1922762095928192\n",
      "\n",
      "Epoch: 1288 Train Loss: 0.1739\n",
      "\n",
      " Test Loss 0.1922428011894226\n",
      "\n",
      "Epoch: 1289 Train Loss: 0.1738\n",
      "\n",
      " Test Loss 0.19220933318138123\n",
      "\n",
      "Epoch: 1290 Train Loss: 0.1738\n",
      "\n",
      " Test Loss 0.19217604398727417\n",
      "\n",
      "Epoch: 1291 Train Loss: 0.1738\n",
      "\n",
      " Test Loss 0.19214271008968353\n",
      "\n",
      "Epoch: 1292 Train Loss: 0.1737\n",
      "\n",
      " Test Loss 0.19210949540138245\n",
      "\n",
      "Epoch: 1293 Train Loss: 0.1737\n",
      "\n",
      " Test Loss 0.19207626581192017\n",
      "\n",
      "Epoch: 1294 Train Loss: 0.1737\n",
      "\n",
      " Test Loss 0.19204306602478027\n",
      "\n",
      "Epoch: 1295 Train Loss: 0.1737\n",
      "\n",
      " Test Loss 0.19200995564460754\n",
      "\n",
      "Epoch: 1296 Train Loss: 0.1736\n",
      "\n",
      " Test Loss 0.1919768899679184\n",
      "\n",
      "Epoch: 1297 Train Loss: 0.1736\n",
      "\n",
      " Test Loss 0.19194380939006805\n",
      "\n",
      "Epoch: 1298 Train Loss: 0.1736\n",
      "\n",
      " Test Loss 0.1919107735157013\n",
      "\n",
      "Epoch: 1299 Train Loss: 0.1735\n",
      "\n",
      " Test Loss 0.1918778121471405\n",
      "\n",
      "Epoch: 1300 Train Loss: 0.1735\n",
      "\n",
      " Test Loss 0.1918448507785797\n",
      "\n",
      "Epoch: 1301 Train Loss: 0.1735\n",
      "\n",
      " Test Loss 0.1918119341135025\n",
      "\n",
      "Epoch: 1302 Train Loss: 0.1734\n",
      "\n",
      " Test Loss 0.19177906215190887\n",
      "\n",
      "Epoch: 1303 Train Loss: 0.1734\n",
      "\n",
      " Test Loss 0.19174621999263763\n",
      "\n",
      "Epoch: 1304 Train Loss: 0.1734\n",
      "\n",
      " Test Loss 0.191713348031044\n",
      "\n",
      "Epoch: 1305 Train Loss: 0.1733\n",
      "\n",
      " Test Loss 0.1916806399822235\n",
      "\n",
      "Epoch: 1306 Train Loss: 0.1733\n",
      "\n",
      " Test Loss 0.19164788722991943\n",
      "\n",
      "Epoch: 1307 Train Loss: 0.1733\n",
      "\n",
      " Test Loss 0.19161511957645416\n",
      "\n",
      "Epoch: 1308 Train Loss: 0.1732\n",
      "\n",
      " Test Loss 0.19158245623111725\n",
      "\n",
      "Epoch: 1309 Train Loss: 0.1732\n",
      "\n",
      " Test Loss 0.19154968857765198\n",
      "\n",
      "Epoch: 1310 Train Loss: 0.1732\n",
      "\n",
      " Test Loss 0.19151704013347626\n",
      "\n",
      "Epoch: 1311 Train Loss: 0.1731\n",
      "\n",
      " Test Loss 0.19148437678813934\n",
      "\n",
      "Epoch: 1312 Train Loss: 0.1731\n",
      "\n",
      " Test Loss 0.1914518028497696\n",
      "\n",
      "Epoch: 1313 Train Loss: 0.1731\n",
      "\n",
      " Test Loss 0.19141922891139984\n",
      "\n",
      "Epoch: 1314 Train Loss: 0.1731\n",
      "\n",
      " Test Loss 0.19138669967651367\n",
      "\n",
      "Epoch: 1315 Train Loss: 0.1730\n",
      "\n",
      " Test Loss 0.19135424494743347\n",
      "\n",
      "Epoch: 1316 Train Loss: 0.1730\n",
      "\n",
      " Test Loss 0.19132177531719208\n",
      "\n",
      "Epoch: 1317 Train Loss: 0.1730\n",
      "\n",
      " Test Loss 0.19128935039043427\n",
      "\n",
      "Epoch: 1318 Train Loss: 0.1729\n",
      "\n",
      " Test Loss 0.19125698506832123\n",
      "\n",
      "Epoch: 1319 Train Loss: 0.1729\n",
      "\n",
      " Test Loss 0.1912246197462082\n",
      "\n",
      "Epoch: 1320 Train Loss: 0.1729\n",
      "\n",
      " Test Loss 0.19119231402873993\n",
      "\n",
      "Epoch: 1321 Train Loss: 0.1728\n",
      "\n",
      " Test Loss 0.19116003811359406\n",
      "\n",
      "Epoch: 1322 Train Loss: 0.1728\n",
      "\n",
      " Test Loss 0.19112779200077057\n",
      "\n",
      "Epoch: 1323 Train Loss: 0.1728\n",
      "\n",
      " Test Loss 0.19109557569026947\n",
      "\n",
      "Epoch: 1324 Train Loss: 0.1727\n",
      "\n",
      " Test Loss 0.19106341898441315\n",
      "\n",
      "Epoch: 1325 Train Loss: 0.1727\n",
      "\n",
      " Test Loss 0.19103124737739563\n",
      "\n",
      "Epoch: 1326 Train Loss: 0.1727\n",
      "\n",
      " Test Loss 0.19099915027618408\n",
      "\n",
      "Epoch: 1327 Train Loss: 0.1727\n",
      "\n",
      " Test Loss 0.19096708297729492\n",
      "\n",
      "Epoch: 1328 Train Loss: 0.1726\n",
      "\n",
      " Test Loss 0.19093504548072815\n",
      "\n",
      "Epoch: 1329 Train Loss: 0.1726\n",
      "\n",
      " Test Loss 0.19090303778648376\n",
      "\n",
      "Epoch: 1330 Train Loss: 0.1726\n",
      "\n",
      " Test Loss 0.19087107479572296\n",
      "\n",
      "Epoch: 1331 Train Loss: 0.1725\n",
      "\n",
      " Test Loss 0.19083915650844574\n",
      "\n",
      "Epoch: 1332 Train Loss: 0.1725\n",
      "\n",
      " Test Loss 0.19080723822116852\n",
      "\n",
      "Epoch: 1333 Train Loss: 0.1725\n",
      "\n",
      " Test Loss 0.19077536463737488\n",
      "\n",
      "Epoch: 1334 Train Loss: 0.1724\n",
      "\n",
      " Test Loss 0.19074350595474243\n",
      "\n",
      "Epoch: 1335 Train Loss: 0.1724\n",
      "\n",
      " Test Loss 0.19071170687675476\n",
      "\n",
      "Epoch: 1336 Train Loss: 0.1724\n",
      "\n",
      " Test Loss 0.19067992269992828\n",
      "\n",
      "Epoch: 1337 Train Loss: 0.1724\n",
      "\n",
      " Test Loss 0.1906481832265854\n",
      "\n",
      "Epoch: 1338 Train Loss: 0.1723\n",
      "\n",
      " Test Loss 0.1906164288520813\n",
      "\n",
      "Epoch: 1339 Train Loss: 0.1723\n",
      "\n",
      " Test Loss 0.19058480858802795\n",
      "\n",
      "Epoch: 1340 Train Loss: 0.1723\n",
      "\n",
      " Test Loss 0.19055312871932983\n",
      "\n",
      "Epoch: 1341 Train Loss: 0.1722\n",
      "\n",
      " Test Loss 0.1905215084552765\n",
      "\n",
      "Epoch: 1342 Train Loss: 0.1722\n",
      "\n",
      " Test Loss 0.19048991799354553\n",
      "\n",
      "Epoch: 1343 Train Loss: 0.1722\n",
      "\n",
      " Test Loss 0.19045838713645935\n",
      "\n",
      "Epoch: 1344 Train Loss: 0.1721\n",
      "\n",
      " Test Loss 0.19042682647705078\n",
      "\n",
      "Epoch: 1345 Train Loss: 0.1721\n",
      "\n",
      " Test Loss 0.190395325422287\n",
      "\n",
      "Epoch: 1346 Train Loss: 0.1721\n",
      "\n",
      " Test Loss 0.1903638243675232\n",
      "\n",
      "Epoch: 1347 Train Loss: 0.1721\n",
      "\n",
      " Test Loss 0.19033242762088776\n",
      "\n",
      "Epoch: 1348 Train Loss: 0.1720\n",
      "\n",
      " Test Loss 0.19030098617076874\n",
      "\n",
      "Epoch: 1349 Train Loss: 0.1720\n",
      "\n",
      " Test Loss 0.19026963412761688\n",
      "\n",
      "Epoch: 1350 Train Loss: 0.1720\n",
      "\n",
      " Test Loss 0.19023831188678741\n",
      "\n",
      "Epoch: 1351 Train Loss: 0.1719\n",
      "\n",
      " Test Loss 0.19020697474479675\n",
      "\n",
      "Epoch: 1352 Train Loss: 0.1719\n",
      "\n",
      " Test Loss 0.19017568230628967\n",
      "\n",
      "Epoch: 1353 Train Loss: 0.1719\n",
      "\n",
      " Test Loss 0.19014446437358856\n",
      "\n",
      "Epoch: 1354 Train Loss: 0.1718\n",
      "\n",
      " Test Loss 0.19011323153972626\n",
      "\n",
      "Epoch: 1355 Train Loss: 0.1718\n",
      "\n",
      " Test Loss 0.19008201360702515\n",
      "\n",
      "Epoch: 1356 Train Loss: 0.1718\n",
      "\n",
      " Test Loss 0.19005082547664642\n",
      "\n",
      "Epoch: 1357 Train Loss: 0.1718\n",
      "\n",
      " Test Loss 0.19001972675323486\n",
      "\n",
      "Epoch: 1358 Train Loss: 0.1717\n",
      "\n",
      " Test Loss 0.1899886131286621\n",
      "\n",
      "Epoch: 1359 Train Loss: 0.1717\n",
      "\n",
      " Test Loss 0.18995751440525055\n",
      "\n",
      "Epoch: 1360 Train Loss: 0.1717\n",
      "\n",
      " Test Loss 0.18992644548416138\n",
      "\n",
      "Epoch: 1361 Train Loss: 0.1716\n",
      "\n",
      " Test Loss 0.18989546597003937\n",
      "\n",
      "Epoch: 1362 Train Loss: 0.1716\n",
      "\n",
      " Test Loss 0.18986450135707855\n",
      "\n",
      "Epoch: 1363 Train Loss: 0.1716\n",
      "\n",
      " Test Loss 0.18983352184295654\n",
      "\n",
      "Epoch: 1364 Train Loss: 0.1716\n",
      "\n",
      " Test Loss 0.1898026317358017\n",
      "\n",
      "Epoch: 1365 Train Loss: 0.1715\n",
      "\n",
      " Test Loss 0.18977171182632446\n",
      "\n",
      "Epoch: 1366 Train Loss: 0.1715\n",
      "\n",
      " Test Loss 0.18974080681800842\n",
      "\n",
      "Epoch: 1367 Train Loss: 0.1715\n",
      "\n",
      " Test Loss 0.18971000611782074\n",
      "\n",
      "Epoch: 1368 Train Loss: 0.1714\n",
      "\n",
      " Test Loss 0.18967917561531067\n",
      "\n",
      "Epoch: 1369 Train Loss: 0.1714\n",
      "\n",
      " Test Loss 0.18964840471744537\n",
      "\n",
      "Epoch: 1370 Train Loss: 0.1714\n",
      "\n",
      " Test Loss 0.1896176040172577\n",
      "\n",
      "Epoch: 1371 Train Loss: 0.1713\n",
      "\n",
      " Test Loss 0.18958692252635956\n",
      "\n",
      "Epoch: 1372 Train Loss: 0.1713\n",
      "\n",
      " Test Loss 0.18955624103546143\n",
      "\n",
      "Epoch: 1373 Train Loss: 0.1713\n",
      "\n",
      " Test Loss 0.1895255744457245\n",
      "\n",
      "Epoch: 1374 Train Loss: 0.1713\n",
      "\n",
      " Test Loss 0.18949489295482635\n",
      "\n",
      "Epoch: 1375 Train Loss: 0.1712\n",
      "\n",
      " Test Loss 0.189464271068573\n",
      "\n",
      "Epoch: 1376 Train Loss: 0.1712\n",
      "\n",
      " Test Loss 0.1894337236881256\n",
      "\n",
      "Epoch: 1377 Train Loss: 0.1712\n",
      "\n",
      " Test Loss 0.18940317630767822\n",
      "\n",
      "Epoch: 1378 Train Loss: 0.1711\n",
      "\n",
      " Test Loss 0.18937265872955322\n",
      "\n",
      "Epoch: 1379 Train Loss: 0.1711\n",
      "\n",
      " Test Loss 0.1893421858549118\n",
      "\n",
      "Epoch: 1380 Train Loss: 0.1711\n",
      "\n",
      " Test Loss 0.18931174278259277\n",
      "\n",
      "Epoch: 1381 Train Loss: 0.1711\n",
      "\n",
      " Test Loss 0.18928128480911255\n",
      "\n",
      "Epoch: 1382 Train Loss: 0.1710\n",
      "\n",
      " Test Loss 0.18925093114376068\n",
      "\n",
      "Epoch: 1383 Train Loss: 0.1710\n",
      "\n",
      " Test Loss 0.18922054767608643\n",
      "\n",
      "Epoch: 1384 Train Loss: 0.1710\n",
      "\n",
      " Test Loss 0.18919019401073456\n",
      "\n",
      "Epoch: 1385 Train Loss: 0.1709\n",
      "\n",
      " Test Loss 0.18915987014770508\n",
      "\n",
      "Epoch: 1386 Train Loss: 0.1709\n",
      "\n",
      " Test Loss 0.1891295611858368\n",
      "\n",
      "Epoch: 1387 Train Loss: 0.1709\n",
      "\n",
      " Test Loss 0.18909934163093567\n",
      "\n",
      "Epoch: 1388 Train Loss: 0.1709\n",
      "\n",
      " Test Loss 0.18906909227371216\n",
      "\n",
      "Epoch: 1389 Train Loss: 0.1708\n",
      "\n",
      " Test Loss 0.18903891742229462\n",
      "\n",
      "Epoch: 1390 Train Loss: 0.1708\n",
      "\n",
      " Test Loss 0.18900875747203827\n",
      "\n",
      "Epoch: 1391 Train Loss: 0.1708\n",
      "\n",
      " Test Loss 0.1889786720275879\n",
      "\n",
      "Epoch: 1392 Train Loss: 0.1707\n",
      "\n",
      " Test Loss 0.1889486014842987\n",
      "\n",
      "Epoch: 1393 Train Loss: 0.1707\n",
      "\n",
      " Test Loss 0.18891854584217072\n",
      "\n",
      "Epoch: 1394 Train Loss: 0.1707\n",
      "\n",
      " Test Loss 0.1888885200023651\n",
      "\n",
      "Epoch: 1395 Train Loss: 0.1707\n",
      "\n",
      " Test Loss 0.1888585090637207\n",
      "\n",
      "Epoch: 1396 Train Loss: 0.1706\n",
      "\n",
      " Test Loss 0.18882857263088226\n",
      "\n",
      "Epoch: 1397 Train Loss: 0.1706\n",
      "\n",
      " Test Loss 0.18879859149456024\n",
      "\n",
      "Epoch: 1398 Train Loss: 0.1706\n",
      "\n",
      " Test Loss 0.1887686848640442\n",
      "\n",
      "Epoch: 1399 Train Loss: 0.1706\n",
      "\n",
      " Test Loss 0.18873873353004456\n",
      "\n",
      "Epoch: 1400 Train Loss: 0.1705\n",
      "\n",
      " Test Loss 0.1887088268995285\n",
      "\n",
      "Epoch: 1401 Train Loss: 0.1705\n",
      "\n",
      " Test Loss 0.18867895007133484\n",
      "\n",
      "Epoch: 1402 Train Loss: 0.1705\n",
      "\n",
      " Test Loss 0.18864905834197998\n",
      "\n",
      "Epoch: 1403 Train Loss: 0.1704\n",
      "\n",
      " Test Loss 0.1886192262172699\n",
      "\n",
      "Epoch: 1404 Train Loss: 0.1704\n",
      "\n",
      " Test Loss 0.1885894536972046\n",
      "\n",
      "Epoch: 1405 Train Loss: 0.1704\n",
      "\n",
      " Test Loss 0.18855968117713928\n",
      "\n",
      "Epoch: 1406 Train Loss: 0.1704\n",
      "\n",
      " Test Loss 0.18852993845939636\n",
      "\n",
      "Epoch: 1407 Train Loss: 0.1703\n",
      "\n",
      " Test Loss 0.18850021064281464\n",
      "\n",
      "Epoch: 1408 Train Loss: 0.1703\n",
      "\n",
      " Test Loss 0.1884705126285553\n",
      "\n",
      "Epoch: 1409 Train Loss: 0.1703\n",
      "\n",
      " Test Loss 0.18844085931777954\n",
      "\n",
      "Epoch: 1410 Train Loss: 0.1702\n",
      "\n",
      " Test Loss 0.1884111911058426\n",
      "\n",
      "Epoch: 1411 Train Loss: 0.1702\n",
      "\n",
      " Test Loss 0.1883815973997116\n",
      "\n",
      "Epoch: 1412 Train Loss: 0.1702\n",
      "\n",
      " Test Loss 0.18835198879241943\n",
      "\n",
      "Epoch: 1413 Train Loss: 0.1702\n",
      "\n",
      " Test Loss 0.18832242488861084\n",
      "\n",
      "Epoch: 1414 Train Loss: 0.1701\n",
      "\n",
      " Test Loss 0.18829290568828583\n",
      "\n",
      "Epoch: 1415 Train Loss: 0.1701\n",
      "\n",
      " Test Loss 0.18826338648796082\n",
      "\n",
      "Epoch: 1416 Train Loss: 0.1701\n",
      "\n",
      " Test Loss 0.18823394179344177\n",
      "\n",
      "Epoch: 1417 Train Loss: 0.1701\n",
      "\n",
      " Test Loss 0.18820443749427795\n",
      "\n",
      "Epoch: 1418 Train Loss: 0.1700\n",
      "\n",
      " Test Loss 0.1881750077009201\n",
      "\n",
      "Epoch: 1419 Train Loss: 0.1700\n",
      "\n",
      " Test Loss 0.18814557790756226\n",
      "\n",
      "Epoch: 1420 Train Loss: 0.1700\n",
      "\n",
      " Test Loss 0.1881161779165268\n",
      "\n",
      "Epoch: 1421 Train Loss: 0.1699\n",
      "\n",
      " Test Loss 0.1880868524312973\n",
      "\n",
      "Epoch: 1422 Train Loss: 0.1699\n",
      "\n",
      " Test Loss 0.18805746734142303\n",
      "\n",
      "Epoch: 1423 Train Loss: 0.1699\n",
      "\n",
      " Test Loss 0.18802817165851593\n",
      "\n",
      "Epoch: 1424 Train Loss: 0.1699\n",
      "\n",
      " Test Loss 0.1879989057779312\n",
      "\n",
      "Epoch: 1425 Train Loss: 0.1698\n",
      "\n",
      " Test Loss 0.1879696398973465\n",
      "\n",
      "Epoch: 1426 Train Loss: 0.1698\n",
      "\n",
      " Test Loss 0.18794043362140656\n",
      "\n",
      "Epoch: 1427 Train Loss: 0.1698\n",
      "\n",
      " Test Loss 0.1879112422466278\n",
      "\n",
      "Epoch: 1428 Train Loss: 0.1698\n",
      "\n",
      " Test Loss 0.18788205087184906\n",
      "\n",
      "Epoch: 1429 Train Loss: 0.1697\n",
      "\n",
      " Test Loss 0.1878528594970703\n",
      "\n",
      "Epoch: 1430 Train Loss: 0.1697\n",
      "\n",
      " Test Loss 0.18782366812229156\n",
      "\n",
      "Epoch: 1431 Train Loss: 0.1697\n",
      "\n",
      " Test Loss 0.1877945363521576\n",
      "\n",
      "Epoch: 1432 Train Loss: 0.1696\n",
      "\n",
      " Test Loss 0.1877654641866684\n",
      "\n",
      "Epoch: 1433 Train Loss: 0.1696\n",
      "\n",
      " Test Loss 0.18773634731769562\n",
      "\n",
      "Epoch: 1434 Train Loss: 0.1696\n",
      "\n",
      " Test Loss 0.1877073347568512\n",
      "\n",
      "Epoch: 1435 Train Loss: 0.1696\n",
      "\n",
      " Test Loss 0.18767832219600677\n",
      "\n",
      "Epoch: 1436 Train Loss: 0.1695\n",
      "\n",
      " Test Loss 0.18764929473400116\n",
      "\n",
      "Epoch: 1437 Train Loss: 0.1695\n",
      "\n",
      " Test Loss 0.18762029707431793\n",
      "\n",
      "Epoch: 1438 Train Loss: 0.1695\n",
      "\n",
      " Test Loss 0.18759135901927948\n",
      "\n",
      "Epoch: 1439 Train Loss: 0.1695\n",
      "\n",
      " Test Loss 0.18756240606307983\n",
      "\n",
      "Epoch: 1440 Train Loss: 0.1694\n",
      "\n",
      " Test Loss 0.18753349781036377\n",
      "\n",
      "Epoch: 1441 Train Loss: 0.1694\n",
      "\n",
      " Test Loss 0.1875045746564865\n",
      "\n",
      "Epoch: 1442 Train Loss: 0.1694\n",
      "\n",
      " Test Loss 0.18747572600841522\n",
      "\n",
      "Epoch: 1443 Train Loss: 0.1693\n",
      "\n",
      " Test Loss 0.18744687736034393\n",
      "\n",
      "Epoch: 1444 Train Loss: 0.1693\n",
      "\n",
      " Test Loss 0.18741801381111145\n",
      "\n",
      "Epoch: 1445 Train Loss: 0.1693\n",
      "\n",
      " Test Loss 0.18738925457000732\n",
      "\n",
      "Epoch: 1446 Train Loss: 0.1693\n",
      "\n",
      " Test Loss 0.18736052513122559\n",
      "\n",
      "Epoch: 1447 Train Loss: 0.1692\n",
      "\n",
      " Test Loss 0.18733175098896027\n",
      "\n",
      "Epoch: 1448 Train Loss: 0.1692\n",
      "\n",
      " Test Loss 0.18730305135250092\n",
      "\n",
      "Epoch: 1449 Train Loss: 0.1692\n",
      "\n",
      " Test Loss 0.18727438151836395\n",
      "\n",
      "Epoch: 1450 Train Loss: 0.1692\n",
      "\n",
      " Test Loss 0.18724565207958221\n",
      "\n",
      "Epoch: 1451 Train Loss: 0.1691\n",
      "\n",
      " Test Loss 0.18721702694892883\n",
      "\n",
      "Epoch: 1452 Train Loss: 0.1691\n",
      "\n",
      " Test Loss 0.18718840181827545\n",
      "\n",
      "Epoch: 1453 Train Loss: 0.1691\n",
      "\n",
      " Test Loss 0.18715979158878326\n",
      "\n",
      "Epoch: 1454 Train Loss: 0.1691\n",
      "\n",
      " Test Loss 0.18713125586509705\n",
      "\n",
      "Epoch: 1455 Train Loss: 0.1690\n",
      "\n",
      " Test Loss 0.18710269033908844\n",
      "\n",
      "Epoch: 1456 Train Loss: 0.1690\n",
      "\n",
      " Test Loss 0.1870741844177246\n",
      "\n",
      "Epoch: 1457 Train Loss: 0.1690\n",
      "\n",
      " Test Loss 0.18704567849636078\n",
      "\n",
      "Epoch: 1458 Train Loss: 0.1690\n",
      "\n",
      " Test Loss 0.18701715767383575\n",
      "\n",
      "Epoch: 1459 Train Loss: 0.1689\n",
      "\n",
      " Test Loss 0.1869887262582779\n",
      "\n",
      "Epoch: 1460 Train Loss: 0.1689\n",
      "\n",
      " Test Loss 0.18696030974388123\n",
      "\n",
      "Epoch: 1461 Train Loss: 0.1689\n",
      "\n",
      " Test Loss 0.18693196773529053\n",
      "\n",
      "Epoch: 1462 Train Loss: 0.1688\n",
      "\n",
      " Test Loss 0.18690353631973267\n",
      "\n",
      "Epoch: 1463 Train Loss: 0.1688\n",
      "\n",
      " Test Loss 0.18687522411346436\n",
      "\n",
      "Epoch: 1464 Train Loss: 0.1688\n",
      "\n",
      " Test Loss 0.18684688210487366\n",
      "\n",
      "Epoch: 1465 Train Loss: 0.1688\n",
      "\n",
      " Test Loss 0.18681855499744415\n",
      "\n",
      "Epoch: 1466 Train Loss: 0.1687\n",
      "\n",
      " Test Loss 0.18679030239582062\n",
      "\n",
      "Epoch: 1467 Train Loss: 0.1687\n",
      "\n",
      " Test Loss 0.1867620199918747\n",
      "\n",
      "Epoch: 1468 Train Loss: 0.1687\n",
      "\n",
      " Test Loss 0.18673382699489594\n",
      "\n",
      "Epoch: 1469 Train Loss: 0.1687\n",
      "\n",
      " Test Loss 0.1867055892944336\n",
      "\n",
      "Epoch: 1470 Train Loss: 0.1686\n",
      "\n",
      " Test Loss 0.18667736649513245\n",
      "\n",
      "Epoch: 1471 Train Loss: 0.1686\n",
      "\n",
      " Test Loss 0.1866491734981537\n",
      "\n",
      "Epoch: 1472 Train Loss: 0.1686\n",
      "\n",
      " Test Loss 0.1866210252046585\n",
      "\n",
      "Epoch: 1473 Train Loss: 0.1686\n",
      "\n",
      " Test Loss 0.18659289181232452\n",
      "\n",
      "Epoch: 1474 Train Loss: 0.1685\n",
      "\n",
      " Test Loss 0.18656477332115173\n",
      "\n",
      "Epoch: 1475 Train Loss: 0.1685\n",
      "\n",
      " Test Loss 0.18653668463230133\n",
      "\n",
      "Epoch: 1476 Train Loss: 0.1685\n",
      "\n",
      " Test Loss 0.18650862574577332\n",
      "\n",
      "Epoch: 1477 Train Loss: 0.1685\n",
      "\n",
      " Test Loss 0.1864805668592453\n",
      "\n",
      "Epoch: 1478 Train Loss: 0.1684\n",
      "\n",
      " Test Loss 0.18645253777503967\n",
      "\n",
      "Epoch: 1479 Train Loss: 0.1684\n",
      "\n",
      " Test Loss 0.18642458319664001\n",
      "\n",
      "Epoch: 1480 Train Loss: 0.1684\n",
      "\n",
      " Test Loss 0.18639658391475677\n",
      "\n",
      "Epoch: 1481 Train Loss: 0.1684\n",
      "\n",
      " Test Loss 0.18636862933635712\n",
      "\n",
      "Epoch: 1482 Train Loss: 0.1683\n",
      "\n",
      " Test Loss 0.18634070456027985\n",
      "\n",
      "Epoch: 1483 Train Loss: 0.1683\n",
      "\n",
      " Test Loss 0.18631279468536377\n",
      "\n",
      "Epoch: 1484 Train Loss: 0.1683\n",
      "\n",
      " Test Loss 0.18628491461277008\n",
      "\n",
      "Epoch: 1485 Train Loss: 0.1683\n",
      "\n",
      " Test Loss 0.18625706434249878\n",
      "\n",
      "Epoch: 1486 Train Loss: 0.1682\n",
      "\n",
      " Test Loss 0.18622922897338867\n",
      "\n",
      "Epoch: 1487 Train Loss: 0.1682\n",
      "\n",
      " Test Loss 0.18620142340660095\n",
      "\n",
      "Epoch: 1488 Train Loss: 0.1682\n",
      "\n",
      " Test Loss 0.18617363274097443\n",
      "\n",
      "Epoch: 1489 Train Loss: 0.1681\n",
      "\n",
      " Test Loss 0.1861458271741867\n",
      "\n",
      "Epoch: 1490 Train Loss: 0.1681\n",
      "\n",
      " Test Loss 0.18611814081668854\n",
      "\n",
      "Epoch: 1491 Train Loss: 0.1681\n",
      "\n",
      " Test Loss 0.1860903799533844\n",
      "\n",
      "Epoch: 1492 Train Loss: 0.1681\n",
      "\n",
      " Test Loss 0.18606270849704742\n",
      "\n",
      "Epoch: 1493 Train Loss: 0.1680\n",
      "\n",
      " Test Loss 0.18603503704071045\n",
      "\n",
      "Epoch: 1494 Train Loss: 0.1680\n",
      "\n",
      " Test Loss 0.18600739538669586\n",
      "\n",
      "Epoch: 1495 Train Loss: 0.1680\n",
      "\n",
      " Test Loss 0.18597975373268127\n",
      "\n",
      "Epoch: 1496 Train Loss: 0.1680\n",
      "\n",
      " Test Loss 0.18595212697982788\n",
      "\n",
      "Epoch: 1497 Train Loss: 0.1679\n",
      "\n",
      " Test Loss 0.18592458963394165\n",
      "\n",
      "Epoch: 1498 Train Loss: 0.1679\n",
      "\n",
      " Test Loss 0.18589702248573303\n",
      "\n",
      "Epoch: 1499 Train Loss: 0.1679\n",
      "\n",
      " Test Loss 0.1858694851398468\n",
      "\n",
      "Epoch: 1500 Train Loss: 0.1679\n",
      "\n",
      " Test Loss 0.18584196269512177\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(4321)\n",
    "model = predicter()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "trainloss = []\n",
    "testloss = []\n",
    "accuracy = []\n",
    "for epoch in range(0,1500):\n",
    "    train(model, df_train_tens, target_train_tens, optimizer, epoch+1)\n",
    "    test(model, df_test_tens, target_test_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8ff598d-9719-4203-8bc9-8726d56aeea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainloss, label='train_loss')#lr 0.1, rmse\n",
    "plt.plot(testloss, label='test_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb16ed0-8d89-4870-881f-7d0be79c656b",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727abe3-7949-425f-9fc9-8a49c10ab7b2",
   "metadata": {},
   "source": [
    "First, let's take a look at what we have learnt. The model basically learnt the following number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c6a95d-7155-48b1-bf0b-dd51a0f8a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 330])\n",
      "torch.Size([500])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000])\n",
      "torch.Size([200, 2000])\n",
      "torch.Size([200])\n",
      "torch.Size([32, 200])\n",
      "torch.Size([32])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for params in model.parameters():\n",
    "    print(params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b144c-d1e4-42c6-9179-06dbd738c0be",
   "metadata": {},
   "source": [
    "Let's calculate error and compare our predictions with the actual results of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "623c72d3-89ed-4db0-870c-108478f473b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1712)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model(df_tens).detach().reshape(1460)\n",
    "y = torch.tensor(target_raw)\n",
    "rmse(y,y_hat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5715fd90-472c-469e-b7b4-c5cb9cbd568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>208500</td>\n",
       "      <td>224641.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181500</td>\n",
       "      <td>182967.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223500</td>\n",
       "      <td>229908.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140000</td>\n",
       "      <td>168062.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250000</td>\n",
       "      <td>269913.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>175000</td>\n",
       "      <td>205520.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>210000</td>\n",
       "      <td>209720.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>266500</td>\n",
       "      <td>197443.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>142125</td>\n",
       "      <td>137613.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>147500</td>\n",
       "      <td>168145.265625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SalePrice    predictions\n",
       "0        208500  224641.687500\n",
       "1        181500  182967.093750\n",
       "2        223500  229908.890625\n",
       "3        140000  168062.656250\n",
       "4        250000  269913.875000\n",
       "...         ...            ...\n",
       "1455     175000  205520.687500\n",
       "1456     210000  209720.765625\n",
       "1457     266500  197443.140625\n",
       "1458     142125  137613.890625\n",
       "1459     147500  168145.265625\n",
       "\n",
       "[1460 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res = pd.DataFrame(y.numpy(), columns=['SalePrice'])\n",
    "df_res['predictions'] = pd.DataFrame(y_hat.numpy(),columns=['predictions'])\n",
    "df_res #doesn't look too bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6350b48-b4a6-4ce9-a267-101369abfba1",
   "metadata": {},
   "source": [
    "Next, we calculate predictions for the test data and save it as a .csv file for submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e741541d-56e6-4ac7-807e-bde5324b1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_raw = model(df_pred_tens).detach().reshape(1459)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "983602f1-464f-42bf-a71b-4b8bd8ee0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions_raw.numpy(), columns=['SalePrice'])\n",
    "predictions_df['Id'] = pred_id\n",
    "predictions_df = pd.DataFrame(predictions_df, columns=['Id', 'SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "311cf694-4f7c-470c-bdfe-b7c180aaba6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>120461.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>163342.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>200931.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>220835.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>182689.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>84996.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>98403.828125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>198683.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>112219.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>227360.343750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  120461.359375\n",
       "1     1462  163342.218750\n",
       "2     1463  200931.531250\n",
       "3     1464  220835.203125\n",
       "4     1465  182689.343750\n",
       "...    ...            ...\n",
       "1454  2915   84996.109375\n",
       "1455  2916   98403.828125\n",
       "1456  2917  198683.656250\n",
       "1457  2918  112219.421875\n",
       "1458  2919  227360.343750\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4c6221a-b081-4c26-9e20-319737803cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv('./submission.csv', columns=['Id', 'SalePrice'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25910048-f859-4e6a-bb7a-127e1957d52a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
